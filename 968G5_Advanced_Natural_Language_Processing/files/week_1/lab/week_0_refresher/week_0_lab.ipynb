{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0618e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1783805",
   "metadata": {},
   "source": [
    "1. Write a python program to take a text file and output the number of lines, words and characters in the file.\n",
    "\n",
    "2. What assumptions do you make about the text?\n",
    "\n",
    "3. Can you write the program so that it can be run from the comman line? And so that you can give thename of the text file to be processed on the command line?\n",
    "\n",
    "4. Can you extend your code so that you can find the average length of a word?\n",
    "\n",
    "5. Can you extend your code so that you can find the most frequently occuring letter?\n",
    "\n",
    "6. Can you extend your code so that you can display a bar graph showing the frequency distribution of letters in the text file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b3b5a",
   "metadata": {},
   "source": [
    "### 1. Write a python program to take a text file and output the number of lines, words and characters in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b1f6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines: 9\n",
      "number of words: 155\n",
      "number of characters: 623\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lines=0\n",
    "words=0\n",
    "chars=0\n",
    "\n",
    "with open('text.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        lines+=1\n",
    "        line = word_tokenize(line)\n",
    "        for word in line:\n",
    "            words+=1\n",
    "            for char in word:\n",
    "                chars+=1\n",
    "\n",
    "\n",
    "print(f\"number of lines: {lines}\")\n",
    "print(f\"number of words: {words}\")\n",
    "print(f\"number of characters: {chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02550d",
   "metadata": {},
   "source": [
    "### 2. What assumptions do you make about the text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376bc02",
   "metadata": {},
   "source": [
    "* Empty lines (newlines) are counted as lines\n",
    "* A word is just any chunk of characters, i.e. Â£100 is a word. \n",
    "* The lines are tokenized and therefore the spaces between words are lost and not counted in the char count. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89184cd2",
   "metadata": {},
   "source": [
    "### 3. Can you write the program so that it can be run from the comman line? And so that you can give thename of the text file to be processed on the command line?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8695c86",
   "metadata": {},
   "source": [
    "The following code is held in a file called `counter.py`, it can be executed from the terminal using `python counter.py text.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5522cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_stats(filepath):\n",
    "    lines = 0\n",
    "    words = 0\n",
    "    chars = 0\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                lines+=1\n",
    "                line = word_tokenize(line)\n",
    "                for word in line:\n",
    "                    words+=1\n",
    "                    for char in word:\n",
    "                        chars+=1\n",
    "                    \n",
    "        print(f\"Number of lines: {lines}\")\n",
    "        print(f\"Number of words: {words}\")\n",
    "        print(f\"Number of characters: {chars}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        count_stats(sys.argv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359eabe3",
   "metadata": {},
   "source": [
    "### 4. Can you extend your code so that you can find the average length of a word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d29349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length: 4.384615384615385\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words=0\n",
    "chars=0\n",
    "\n",
    "with open('text.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = word_tokenize(line)\n",
    "        line = [t.lower() for t in line if t.isalpha()]\n",
    "        for word in line:\n",
    "            words+=1\n",
    "            chars+= len(word)\n",
    "\n",
    "print(f\"Average Word Length: {chars/words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb740d9e",
   "metadata": {},
   "source": [
    "### 5. Can you extend your code so that you can find the most frequently occuring letter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3162c0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common character is \"e\" with a count of 70\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "chars_dict = {}\n",
    "\n",
    "with open('text.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = word_tokenize(line)\n",
    "        line = [t.lower() for t in line if t.isalpha()]\n",
    "        for word in line:\n",
    "            for char in word:\n",
    "                chars_dict[char] = chars_dict.get(char, 0) + 1\n",
    "\n",
    "most_frequent = max(chars_dict, key=chars_dict.get)\n",
    "print(f'The most common character is \"{most_frequent}\" with a count of {chars_dict[most_frequent]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
