{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121a3d52-940e-453c-bd84-9e5259cbd2fd",
   "metadata": {},
   "source": [
    "# Lab 1: Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b01bc6",
   "metadata": {},
   "source": [
    "In this lab, you will be investigating measures of semantic similarity based on WordNet and distributional similarity. In particular, you will be considering how closely they correlate with human judgements of synonymy. Students who have recently done Natural Language Engineering or Applied Natural Language Processing should be able to get through this relatively quickly and have time to move on to the extension material looking at statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ebbde",
   "metadata": {},
   "source": [
    "* [Getting Started](#getting-started)\n",
    "* []()\n",
    "* []()\n",
    "* []()\n",
    "* []()\n",
    "* []()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393ed10",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc840474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "import operator\n",
    "from nltk.corpus import wordnet as wn, wordnet_ic as wn_ic, lin_thesaurus as lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f11175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bcc4c",
   "metadata": {},
   "source": [
    "## Using WordNet (WN) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627ea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7098990245459575"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"book\") # returns the all the senses of a word\n",
    "wn.synsets(\"book\",wn.NOUN) # retuns the senses that are nouns\n",
    "synsetA=wn.synsets(\"book\",wn.NOUN)[0] # extract the first sense as a variable\n",
    "synsetA.definition() # get the definition of that sense\n",
    "synsetA.hyponyms() # get hyponyms (lower/children) of the sense\n",
    "synsetA.hypernyms() # get the hypernym(s) of the sense\n",
    "synsetB=wn.synsets(\"book\",wn.NOUN)[1] # grab another sense\n",
    "synsetB\n",
    "synsetA.path_similarity(synsetB) # compuate the wordnet path length betwen the two\n",
    "brown_ic=wn_ic.ic(\"ic-brown.dat\") # read in the brown corpus\n",
    "brown_ic\n",
    "synsetA.res_similarity(synsetB,brown_ic)\n",
    "synsetA.lin_similarity(synsetB,brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d23c7",
   "metadata": {},
   "source": [
    "### `ic-brown` Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c12ce",
   "metadata": {},
   "source": [
    "The particular brown data imported is in dictionary form and it is a frequency dictionary mappted to WordNet synset IDs. The IDs are mapped to IC scores based on the Brown Corpus. \n",
    "\n",
    "The structure of `ic-brown` is given as:\n",
    "`{ 'part_of_speech': defaultdict(float, {synset_id: frequency_count}) }`\n",
    "\n",
    "`part_of_speech` might be noun which is under the key `n`. Nested within the POS will be a `dict` which is structured as `synset_id: frequency_count` (key,value) pairs. \n",
    "\n",
    "The `synset_id` will be in \"offset\" form which is an 8 digit number. WordNet is usually accessed using this form `syn = wn.synset('bank.n.01')` but it's offset can be gathered using `syn.offset()`. WordNet can be directly searching using the offset with `wn.synset_from_pos_and_offset(pos, offset)`\n",
    "\n",
    "Functions that take the `ic-brown` dictionary will have methods to derive and compare items using the offset: `lion.lin_similarity(cat, brown_ic)`. In this example it is obtaining the offset values for `lion` and `cat` and then using both IC scores to compute lin_similarity. It will also use `brown_ic` as a lookup for the LCS's IC as well. \n",
    "\n",
    "$$Sim_{Lin}(s_1, s_2) = \\frac{2 \\times IC(LCS)}{IC(s_1) + IC(s_2)}$$\n",
    "\n",
    "The brown dataset is essentially just a lookup table of IC scores that WordNet and NLTK can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1dfea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "363c281a",
   "metadata": {},
   "source": [
    "## 2.1 Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38ffed",
   "metadata": {},
   "source": [
    "#### Write a function to return the path similarity of two nouns. Remember this is the maximum similarity of all of the possible pairings of the two nouns. Make sure you test it. For (chicken,car) the correct answer is 0.0909 (3sf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe91ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_1 is: Synset('chicken.n.01')\n",
      "noun_2 is: Synset('car.n.01')\n",
      "The LCS is Synset('physical_entity.n.01')\n",
      "===\n",
      "starting sense Synset('chicken.n.01')\n",
      "Synset('poultry.n.02')\n",
      "Synset('bird.n.02')\n",
      "Synset('meat.n.01')\n",
      "Synset('food.n.02')\n",
      "Synset('solid.n.01')\n",
      "Synset('matter.n.03')\n",
      "Synset('physical_entity.n.01')\n",
      "distance: 7\n",
      "starting sense Synset('car.n.01')\n",
      "Synset('motor_vehicle.n.01')\n",
      "Synset('self-propelled_vehicle.n.01')\n",
      "Synset('wheeled_vehicle.n.01')\n",
      "Synset('container.n.01')\n",
      "Synset('instrumentality.n.03')\n",
      "Synset('artifact.n.01')\n",
      "Synset('whole.n.02')\n",
      "Synset('object.n.01')\n",
      "Synset('physical_entity.n.01')\n",
      "distance: 9\n",
      "0.058823529411764705\n"
     ]
    }
   ],
   "source": [
    "# TODO: use word directly to grab all senses and offset loop though them both. \n",
    "# take the max sim (probably overwrite keeping the max)\n",
    "\n",
    "syn1 = wn.synset('chicken.n.01')\n",
    "syn2 = wn.synset('car.n.01')\n",
    "\n",
    "def distance_from_hyper(entry,ancestor):\n",
    "    distance = 0\n",
    "    print(f\"starting sense {entry}\")\n",
    "    while entry != ancestor:\n",
    "        entry = entry.hypernyms()[0]\n",
    "        print(entry)\n",
    "        distance += 1\n",
    "    return distance\n",
    "\n",
    "\n",
    "def path_similarity(noun1, noun2):\n",
    "    print(f\"noun_1 is: {noun1}\")\n",
    "    print(f\"noun_2 is: {noun2}\")\n",
    "\n",
    "    anc = noun1.lowest_common_hypernyms(noun2)[0]\n",
    "    print(f\"The LCS is {anc}\")\n",
    "\n",
    "    print(\"===\")\n",
    "    \n",
    "    noun1_distance = distance_from_hyper(noun1, anc)\n",
    "    print(f\"distance: {noun1_distance}\")\n",
    "\n",
    "    noun2_distance = distance_from_hyper(noun2, anc)\n",
    "    print(f\"distance: {noun2_distance}\")\n",
    "\n",
    "    total_distance = noun1_distance + noun2_distance\n",
    "\n",
    "    path_sim = 1 / (total_distance+1)\n",
    "\n",
    "    print(path_sim)\n",
    "\n",
    "path_similarity(syn1, syn2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda4123f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d903d523",
   "metadata": {},
   "source": [
    "#### Generalise it so that you have an extra (optional) parameter which you use to select the WordNet similarity measure e.g., res similarity and lin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0bf65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_nlp",
   "language": "python",
   "name": "adv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
