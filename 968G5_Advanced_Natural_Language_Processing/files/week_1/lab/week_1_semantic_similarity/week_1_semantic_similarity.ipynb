{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121a3d52-940e-453c-bd84-9e5259cbd2fd",
   "metadata": {},
   "source": [
    "# Lab 1: Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b01bc6",
   "metadata": {},
   "source": [
    "In this lab, you will be investigating measures of semantic similarity based on WordNet and distributional similarity. In particular, you will be considering how closely they correlate with human judgements of synonymy. Students who have recently done Natural Language Engineering or Applied Natural Language Processing should be able to get through this relatively quickly and have time to move on to the extension material looking at statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ebbde",
   "metadata": {},
   "source": [
    "1. [Getting Started](#getting-started)\n",
    "    * `ic-brown` explained\n",
    "2. [Useful WordNet Functions](#using-wordnet-wn-functions)\n",
    "    * Write a function to return the path similarity of two nouns\n",
    "    * Generalise the function to use IC measures\n",
    "3. [Human Synonymy Judgements](#3-human-synonymy-judgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393ed10",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc840474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "import operator\n",
    "from nltk.corpus import wordnet as wn, wordnet_ic as wn_ic, lin_thesaurus as lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f11175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bcc4c",
   "metadata": {},
   "source": [
    "## Using WordNet (WN) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f627ea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7098990245459575"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"book\") # returns the all the senses of a word\n",
    "wn.synsets(\"book\",wn.NOUN) # retuns the senses that are nouns\n",
    "synsetA=wn.synsets(\"book\",wn.NOUN)[0] # extract the first sense as a variable\n",
    "synsetA.definition() # get the definition of that sense\n",
    "synsetA.hyponyms() # get hyponyms (lower/children) of the sense\n",
    "synsetA.hypernyms() # get the hypernym(s) of the sense\n",
    "synsetB=wn.synsets(\"book\",wn.NOUN)[1] # grab another sense\n",
    "synsetB\n",
    "synsetA.path_similarity(synsetB) # compuate the wordnet path length betwen the two\n",
    "brown_ic=wn_ic.ic(\"ic-brown.dat\") # read in the brown corpus\n",
    "brown_ic\n",
    "synsetA.res_similarity(synsetB,brown_ic)\n",
    "synsetA.lin_similarity(synsetB,brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d23c7",
   "metadata": {},
   "source": [
    "### `ic-brown` Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c12ce",
   "metadata": {},
   "source": [
    "The particular brown data imported is in dictionary form and it is a frequency dictionary mappted to WordNet synset IDs. The IDs are mapped to IC scores based on the Brown Corpus. \n",
    "\n",
    "The structure of `ic-brown` is given as:\n",
    "`{ 'part_of_speech': defaultdict(float, {synset_id: frequency_count}) }`\n",
    "\n",
    "`part_of_speech` might be noun which is under the key `n`. Nested within the POS will be a `dict` which is structured as `synset_id: frequency_count` (key,value) pairs. \n",
    "\n",
    "The `synset_id` will be in \"offset\" form which is an 8 digit number. WordNet is usually accessed using this form `syn = wn.synset('bank.n.01')` but it's offset can be gathered using `syn.offset()`. WordNet can be directly searching using the offset with `wn.synset_from_pos_and_offset(pos, offset)`\n",
    "\n",
    "Functions that take the `ic-brown` dictionary will have methods to derive and compare items using the offset: `lion.lin_similarity(cat, brown_ic)`. In this example it is obtaining the offset values for `lion` and `cat` and then using both IC scores to compute lin_similarity. It will also use `brown_ic` as a lookup for the LCS's IC as well. \n",
    "\n",
    "$$Sim_{Lin}(s_1, s_2) = \\frac{2 \\times IC(LCS)}{IC(s_1) + IC(s_2)}$$\n",
    "\n",
    "The brown dataset is essentially just a lookup table of IC scores that WordNet and NLTK can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c281a",
   "metadata": {},
   "source": [
    "## 2.1 Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38ffed",
   "metadata": {},
   "source": [
    "#### Write a function to return the path similarity of two nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de7f10",
   "metadata": {},
   "source": [
    "Remember this is the maximum similarity of all of the possible pairings of the two nouns. Make sure you test it. For (chicken,car) the correct answer is 0.0909 (3sf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903d523",
   "metadata": {},
   "source": [
    "#### Generalise it so that you have an extra (optional) parameter which you use to select the WordNet similarity measure e.g., res similarity and lin similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0bf65",
   "metadata": {},
   "source": [
    "**Reminder on Res and Lin Similarity:** Both are metrics that rely on Information Score (IC) which represents how \"specific\" or \"informative\" a concept is based on its frequency in a large body of text (a corpus).\n",
    "\n",
    "Resnikâ€™s measure is based entirely on the Information Content (IC) of the Lowest Common Subsumer (LCS). This means that is only needs that one peice of informaiton to be calculated. The intuition is that is the common word between the two is rare, then the two words are highly similar. These are a common subset of the meaning of the ancestor, i.e. breed names for the LCS \"dog\". However, if the LCS is a common word then the two words are very distinct, there is no commmon ancestor that specificies their category. \n",
    "\n",
    "$$Sim_{resnik}(s_1, s_2) = IC(LCS(s_1, s_2))$$\n",
    "\n",
    "Lin's measure is an extension of Resnik through normalization. The $IC(LCS)$ is taken as a ratio of the two words ICs. It can be summarization as the ratio of the shared information. The downside to Lin is that is now needs 3 IC values to calculate. If the two words are very rare, they made not show up in a corpus as would therefore fail the caclulation.\n",
    "\n",
    "$$Sim_{lin}(s_1, s_2) = \\frac{2 \\times IC(LCS(s_1, s_2))}{IC(s_1) + IC(s_2)}$$\n",
    "\n",
    "Additionally, where as Path Length can be calculated using WordNet alone, IC measures require the application of a corpus to the methodology. WordNet provides the hierachy and the corpus provides the statistics/data. The more frequently a concept appears, the less information it carries, this is because IC is the inverse of probability. \n",
    "\n",
    "$$IC(s) = -\\log P(s)$$\n",
    "\n",
    "Where the probability $P(s)$ is estimated by:Counting how many times the word (and all its more specific \"children\" in the hierarchy) appears in the corpus.Dividing by the total number of words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "08bd8b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair': (Synset('wimp.n.01'), Synset('car.n.02')),\n",
       " 'anc': Synset('whole.n.02'),\n",
       " 'noun1_distance': 5,\n",
       " 'noun2_distance': 5,\n",
       " 'total_distance': 10,\n",
       " 'path_similarity': 0.09090909090909091,\n",
       " 'res_similarity': 1.5318337432196856,\n",
       " 'lin_similarity': 0.16297501193902675}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "import pprint\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def distance_from_ancestor(entry,ancestor):\n",
    "    distance = 0\n",
    "    current = entry\n",
    "\n",
    "    while current != ancestor:\n",
    "        hypers = current.hypernyms()\n",
    "        if not hypers:\n",
    "            break\n",
    "        current = hypers[0]\n",
    "        distance += 1\n",
    "\n",
    "    return distance\n",
    "\n",
    "def similarity(noun1, noun2):\n",
    "    payload = {}\n",
    "    payload[\"pair\"] = (noun1,noun2)\n",
    "    payload[\"anc\"] = noun1.lowest_common_hypernyms(noun2)[0]\n",
    "    payload[\"noun1_distance\"] = distance_from_ancestor(noun1, payload[\"anc\"])\n",
    "    payload[\"noun2_distance\"] = distance_from_ancestor(noun2, payload[\"anc\"])\n",
    "    payload[\"total_distance\"] = payload[\"noun1_distance\"] + payload[\"noun2_distance\"]\n",
    "    payload[\"path_similarity\"] = 1 / (payload[\"total_distance\"]+1)\n",
    "    payload[\"res_similarity\"] = noun1.res_similarity(noun2, brown_ic)\n",
    "    payload[\"lin_similarity\"] = noun1.lin_similarity(noun2, brown_ic)\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "def max_similarity(n1, n2, type=\"path_similarity\"):\n",
    "\n",
    "    max_s = 0\n",
    "    payl = {}\n",
    "    \n",
    "    sys1 = wn.synsets(n1, pos=wn.NOUN)\n",
    "    sys2 = wn.synsets(n2, pos=wn.NOUN)\n",
    "\n",
    "    for s1 in sys1:\n",
    "        for s2 in sys2:\n",
    "            _payl = similarity(s1, s2)\n",
    "            ps = _payl[type]\n",
    "            if ps > max_s:\n",
    "                max_s = ps\n",
    "                payl = _payl\n",
    "    \n",
    "    return payl\n",
    "\n",
    "max_similarity(\"chicken\", \"car\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79b959",
   "metadata": {},
   "source": [
    "# 3 Human Synonymy Judgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85295e",
   "metadata": {},
   "source": [
    "`mcdata.csv` contains the Miller & Charles human similarity judgements discussed in the seminar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_nlp",
   "language": "python",
   "name": "adv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
