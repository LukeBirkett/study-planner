{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Sentence Completion Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you refer to the pdf document `lab4.pdf` as well as this starter notebook\n",
    "\n",
    "The cell below will load the language_model class (developed last week) and train it using the files in the training directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  \n",
    "#this means that language_model will be reloaded when you run this cell - this is important if you change the language_model class!\n",
    "\n",
    "import os\n",
    "from language_model import * ## import language model (developed in previous lab)\n",
    "parentdir=\"/Users/juliewe/Dropbox/teaching/AdvancedNLP/2026/2026_content/week3/lab3/lab3resources/sentence-completion\" #you may need to update this \n",
    "\n",
    "trainingdir=os.path.join(parentdir,\"Holmes_Training_Data\")\n",
    "training,testing=get_training_testing(trainingdir)\n",
    "MAX_FILES=10   #use a small number here whilst developing your solutions\n",
    "mylm=language_model(trainingdir=trainingdir,files=training[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are using my language_model class (from the resources directory) rather than your own, I have added an extra step in the finalising of the probability distributions.  I have counted the number of words which were deleted from the vocabulary during the _make_unknowns step and then used this to adjust the probability of the _UNK token.  This is because it is currently the probability of ANY unknown token occurring.  The adjustment makes it an estimate of the probablity of a particular rare word.  This is important when we are doing the sentence completion challenge as otherwise OOV words will seem much more likely as the target words just due to the sheer number of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the most frequent words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=sorted(mylm.unigram.items(),key=lambda x:x[1],reverse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the vocabulary?  What kind of words are low frequency?  What kind of words are mid-frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the _adjust_unknowns step, the P(_UNK) would be much higher.  Its worth checking what this would be if you turn off self.adjust_unknwowns.  I'll leave this for you to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you can:\n",
    "* look up bigram probabilities\n",
    "* generate a sentence according to the model\n",
    "* calculate the perplexity of a test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load in and have a look at the sentence completion challenge questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, csv\n",
    "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
    "answers=os.path.join(parentdir,\"test_answer.csv\")\n",
    "\n",
    "with open(questions) as instream:\n",
    "    csvreader=csv.reader(instream)\n",
    "    lines=list(csvreader)\n",
    "qs_df=pd.DataFrame(lines[1:],columns=lines[0])\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to be able to tokenize questions so that the gaps can be located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "\n",
    "tokens=[tokenize(q) for q in qs_df['question']]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the context of the blank: looking at the preceding words (number given in window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_context(sent_tokens,window,target=\"_____\"):\n",
    "    found=-1\n",
    "    for i,token in enumerate(sent_tokens):\n",
    "        if token==target:\n",
    "            found=i\n",
    "            break \n",
    "            \n",
    "    if found>-1:\n",
    "        return sent_tokens[i-window:i]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "\n",
    "qs_df['tokens']=qs_df['question'].map(tokenize)\n",
    "qs_df['left_context']=qs_df['tokens'].map(lambda x: get_left_context(x,2))\n",
    "qs_df.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building and evaluating an SCC system\n",
    "1. always predict the same answer (e.g., \"a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scc import *\n",
    "### you can import this the above line but I have included the code here to make it easier to inspect it\n",
    "\n",
    "class question:\n",
    "    \n",
    "    def __init__(self,aline):\n",
    "        self.fields=aline\n",
    "    \n",
    "    def get_field(self,field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "    def add_answer(self,fields):\n",
    "        self.answer=fields[1]\n",
    "   \n",
    "    def chooseA(self):\n",
    "        return(\"a\")\n",
    "    \n",
    "    def predict(self,method=\"chooseA\"):\n",
    "        #eventually there will be lots of methods to choose from\n",
    "        if method==\"chooseA\":\n",
    "            return self.chooseA()\n",
    "        \n",
    "    def predict_and_score(self,method=\"chooseA\"):\n",
    "        \n",
    "        #compare prediction according to method with the correct answer\n",
    "        #return 1 or 0 accordingly\n",
    "        prediction=self.predict(method=method)\n",
    "        if prediction ==self.answer:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "class scc_reader:\n",
    "    \n",
    "    def __init__(self,qs=questions,ans=answers):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.read_files()\n",
    "        \n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "        \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions=[question(qline) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "        \n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n",
    "    \n",
    "    def predict(self,method=\"chooseA\"):\n",
    "        return [q.predict(method=method) for q in self.questions]\n",
    "    \n",
    "    def predict_and_score(self,method=\"chooseA\"):\n",
    "        scores=[q.predict_and_score(method=method) for q in self.questions]\n",
    "        return sum(scores)/len(scores)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCC = scc_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SCC.get_field(\"b)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SCC.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCC.predict_and_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the language model\n",
    "using unigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Context\n",
    "looking up context and bigram probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left and right context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backing off to unigram probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backing off might not change the decision (the correct answer may not be in the bestchoices given back by the bigram model)\n",
    "\n",
    "Investigate: \n",
    "* the effect of the amount of training data on each of the strategies\n",
    "* plot on a graph - should see a cross-over (unigram than bigram for small training data but bigram better than unigram for large training data)\n",
    "\n",
    "Extend:\n",
    "* trigram model\n",
    "* incorporation of distributional similarity / word2vec vectors\n",
    "* RNNLM ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
