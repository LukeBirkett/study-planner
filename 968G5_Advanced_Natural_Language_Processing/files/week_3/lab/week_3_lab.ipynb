{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling Lab (week 3)\n",
    "This notebook provides the \"starter\" code in the week 3 lab.  You should refer to the pdf document for what to do in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Getting Started\n",
    "We need to get the names of files in the training directory and split them into training and testing 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,random,math\n",
    "TRAINING_DIR=\"sentence-completion/Holmes_Training_Data\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "trainingfiles,heldoutfiles=get_training_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainingfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  Building a unigram model\n",
    "\n",
    "THe code below implements a simple unigram model.  The class stores the unigram probability distribution and provides methods for training and lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #convert the accumulated counts to probabilities\n",
    "        print(\"Finalising probability distribution\")\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "    \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            with open(os.path.join(self.training_dir,afile),errors='ignore') as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "       \n",
    "    def get_prob(self,token,method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FILES=5\n",
    "mylm=language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you look up some probabilities of words in your model.  Pick some words which you would expect to have high probabilities and some words which you would expect to have low probabilities.\n",
    "\n",
    "As an extension, see how these change if you use a bigger portion of the training data to train your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylm.get_prob(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Generation\n",
    "Add some functionality to your class so that you can generate a string of highly probably words.\n",
    "\n",
    "Refer to the pdf document for tips on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adding Bigrams\n",
    "\n",
    "Refer to the pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Perplexity\n",
    "\n",
    "Refer to the pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Smoothing\n",
    "\n",
    "Refer to the pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
