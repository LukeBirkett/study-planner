{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swLDp-GV7iQg"
   },
   "source": [
    "# Week 3: Basic Document Classification (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLh_fS4P7iQn"
   },
   "source": [
    "## Overview\n",
    "In labs this week (and next), the focus will be on the application of sentiment analysis. You will be using a corpus of **movie reviews**.\n",
    "\n",
    "You will be exploring various techniques that can be used to classify the sentiment of the movie reviews as either positive or negative.\n",
    "\n",
    "You will be developing your own **Word List** and **Naïve Bayes** classifiers and then comparing them to the **NLTK Naïve Bayes** classifier.\n",
    "\n",
    "First, we will need to download the movie_review corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3W2AdikDqe5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0uVSM40qe5I"
   },
   "source": [
    "The movie_reviews corpus reader provides a number of useful methods:\n",
    "   * .categories()\n",
    "   * .fileids()\n",
    "   * .words()\n",
    "   \n",
    "First, we can use `.categories()` to check the set of labels with which the reviews have been labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N3bRgahNqe5J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INtoThcCqe5J"
   },
   "source": [
    "We can use `.fileids()` to get all of the file names associated with a particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LZ00H5mdqe5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive reviews is 1000\n",
      "The number of negative reviews is 1000\n"
     ]
    }
   ],
   "source": [
    "pos_review_ids=movie_reviews.fileids('pos')\n",
    "neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "print(\"The number of positive reviews is {}\".format(len(pos_review_ids)))\n",
    "print(\"The number of negative reviews is {}\".format(len(neg_review_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoHxpQ6Sqe5L"
   },
   "source": [
    "We can use `.words()` to get back word-tokenised reviews.  The argument to `.words()` is the file id of an individual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KMvFYBPyqe5M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.words(pos_review_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "68LWdfszqe5M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movie_reviews.words(pos_review_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmkD_UEKqe5N"
   },
   "source": [
    "Note, the object returned by `movie_reviews.words()` looks a lot like a list (and behaves a lot like a list) - but it is actually a `StreamBackedCorpusView`.  This essentially means it is not necessarily all in memory  - it is retrieved from disk as needed.  If you want to see all of the words at once then you can convert it to a list using the `list()` constructor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XwmV4eh9qe5O",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', 'there', \"'\", 's', 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.', 'for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'\", '80s', 'with', 'a', '12', '-', 'part', 'series', 'called', 'the', 'watchmen', '.', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', '.', 'the', 'book', '(', 'or', '\"', 'graphic', 'novel', ',', '\"', 'if', 'you', 'will', ')', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'footnotes', '.', 'in', 'other', 'words', ',', 'don', \"'\", 't', 'dismiss', 'this', 'film', 'because', 'of', 'its', 'source', '.', 'if', 'you', 'can', 'get', 'past', 'the', 'whole', 'comic', 'book', 'thing', ',', 'you', 'might', 'find', 'another', 'stumbling', 'block', 'in', 'from', 'hell', \"'\", 's', 'directors', ',', 'albert', 'and', 'allen', 'hughes', '.', 'getting', 'the', 'hughes', 'brothers', 'to', 'direct', 'this', 'seems', 'almost', 'as', 'ludicrous', 'as', 'casting', 'carrot', 'top', 'in', ',', 'well', ',', 'anything', ',', 'but', 'riddle', 'me', 'this', ':', 'who', 'better', 'to', 'direct', 'a', 'film', 'that', \"'\", 's', 'set', 'in', 'the', 'ghetto', 'and', 'features', 'really', 'violent', 'street', 'crime', 'than', 'the', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', '?', 'the', 'ghetto', 'in', 'question', 'is', ',', 'of', 'course', ',', 'whitechapel', 'in', '1888', 'london', \"'\", 's', 'east', 'end', '.', 'it', \"'\", 's', 'a', 'filthy', ',', 'sooty', 'place', 'where', 'the', 'whores', '(', 'called', '\"', 'unfortunates', '\"', ')', 'are', 'starting', 'to', 'get', 'a', 'little', 'nervous', 'about', 'this', 'mysterious', 'psychopath', 'who', 'has', 'been', 'carving', 'through', 'their', 'profession', 'with', 'surgical', 'precision', '.', 'when', 'the', 'first', 'stiff', 'turns', 'up', ',', 'copper', 'peter', 'godley', '(', 'robbie', 'coltrane', ',', 'the', 'world', 'is', 'not', 'enough', ')', 'calls', 'in', 'inspector', 'frederick', 'abberline', '(', 'johnny', 'depp', ',', 'blow', ')', 'to', 'crack', 'the', 'case', '.', 'abberline', ',', 'a', 'widower', ',', 'has', 'prophetic', 'dreams', 'he', 'unsuccessfully', 'tries', 'to', 'quell', 'with', 'copious', 'amounts', 'of', 'absinthe', 'and', 'opium', '.', 'upon', 'arriving', 'in', 'whitechapel', ',', 'he', 'befriends', 'an', 'unfortunate', 'named', 'mary', 'kelly', '(', 'heather', 'graham', ',', 'say', 'it', 'isn', \"'\", 't', 'so', ')', 'and', 'proceeds', 'to', 'investigate', 'the', 'horribly', 'gruesome', 'crimes', 'that', 'even', 'the', 'police', 'surgeon', 'can', \"'\", 't', 'stomach', '.', 'i', 'don', \"'\", 't', 'think', 'anyone', 'needs', 'to', 'be', 'briefed', 'on', 'jack', 'the', 'ripper', ',', 'so', 'i', 'won', \"'\", 't', 'go', 'into', 'the', 'particulars', 'here', ',', 'other', 'than', 'to', 'say', 'moore', 'and', 'campbell', 'have', 'a', 'unique', 'and', 'interesting', 'theory', 'about', 'both', 'the', 'identity', 'of', 'the', 'killer', 'and', 'the', 'reasons', 'he', 'chooses', 'to', 'slay', '.', 'in', 'the', 'comic', ',', 'they', 'don', \"'\", 't', 'bother', 'cloaking', 'the', 'identity', 'of', 'the', 'ripper', ',', 'but', 'screenwriters', 'terry', 'hayes', '(', 'vertical', 'limit', ')', 'and', 'rafael', 'yglesias', '(', 'les', 'mis', '?', 'rables', ')', 'do', 'a', 'good', 'job', 'of', 'keeping', 'him', 'hidden', 'from', 'viewers', 'until', 'the', 'very', 'end', '.', 'it', \"'\", 's', 'funny', 'to', 'watch', 'the', 'locals', 'blindly', 'point', 'the', 'finger', 'of', 'blame', 'at', 'jews', 'and', 'indians', 'because', ',', 'after', 'all', ',', 'an', 'englishman', 'could', 'never', 'be', 'capable', 'of', 'committing', 'such', 'ghastly', 'acts', '.', 'and', 'from', 'hell', \"'\", 's', 'ending', 'had', 'me', 'whistling', 'the', 'stonecutters', 'song', 'from', 'the', 'simpsons', 'for', 'days', '(', '\"', 'who', 'holds', 'back', 'the', 'electric', 'car', '/', 'who', 'made', 'steve', 'guttenberg', 'a', 'star', '?', '\"', ')', '.', 'don', \"'\", 't', 'worry', '-', 'it', \"'\", 'll', 'all', 'make', 'sense', 'when', 'you', 'see', 'it', '.', 'now', 'onto', 'from', 'hell', \"'\", 's', 'appearance', ':', 'it', \"'\", 's', 'certainly', 'dark', 'and', 'bleak', 'enough', ',', 'and', 'it', \"'\", 's', 'surprising', 'to', 'see', 'how', 'much', 'more', 'it', 'looks', 'like', 'a', 'tim', 'burton', 'film', 'than', 'planet', 'of', 'the', 'apes', 'did', '(', 'at', 'times', ',', 'it', 'seems', 'like', 'sleepy', 'hollow', '2', ')', '.', 'the', 'print', 'i', 'saw', 'wasn', \"'\", 't', 'completely', 'finished', '(', 'both', 'color', 'and', 'music', 'had', 'not', 'been', 'finalized', ',', 'so', 'no', 'comments', 'about', 'marilyn', 'manson', ')', ',', 'but', 'cinematographer', 'peter', 'deming', '(', 'don', \"'\", 't', 'say', 'a', 'word', ')', 'ably', 'captures', 'the', 'dreariness', 'of', 'victorian', '-', 'era', 'london', 'and', 'helped', 'make', 'the', 'flashy', 'killing', 'scenes', 'remind', 'me', 'of', 'the', 'crazy', 'flashbacks', 'in', 'twin', 'peaks', ',', 'even', 'though', 'the', 'violence', 'in', 'the', 'film', 'pales', 'in', 'comparison', 'to', 'that', 'in', 'the', 'black', '-', 'and', '-', 'white', 'comic', '.', 'oscar', 'winner', 'martin', 'childs', \"'\", '(', 'shakespeare', 'in', 'love', ')', 'production', 'design', 'turns', 'the', 'original', 'prague', 'surroundings', 'into', 'one', 'creepy', 'place', '.', 'even', 'the', 'acting', 'in', 'from', 'hell', 'is', 'solid', ',', 'with', 'the', 'dreamy', 'depp', 'turning', 'in', 'a', 'typically', 'strong', 'performance', 'and', 'deftly', 'handling', 'a', 'british', 'accent', '.', 'ians', 'holm', '(', 'joe', 'gould', \"'\", 's', 'secret', ')', 'and', 'richardson', '(', '102', 'dalmatians', ')', 'log', 'in', 'great', 'supporting', 'roles', ',', 'but', 'the', 'big', 'surprise', 'here', 'is', 'graham', '.', 'i', 'cringed', 'the', 'first', 'time', 'she', 'opened', 'her', 'mouth', ',', 'imagining', 'her', 'attempt', 'at', 'an', 'irish', 'accent', ',', 'but', 'it', 'actually', 'wasn', \"'\", 't', 'half', 'bad', '.', 'the', 'film', ',', 'however', ',', 'is', 'all', 'good', '.', '2', ':', '00', '-', 'r', 'for', 'strong', 'violence', '/', 'gore', ',', 'sexuality', ',', 'language', 'and', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "print(list(movie_reviews.words(pos_review_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXWztGUZqe5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUYw61N7iQo"
   },
   "source": [
    "## Creating training and testing sets\n",
    "You will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance.\n",
    "\n",
    "We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n",
    "\n",
    "> Look through the code in the following cell, reading the comments and making sure that you understand each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HsMcMo5e7iQp"
   },
   "outputs": [],
   "source": [
    "import random # have a look at the documentation at https://docs.python.org/3/library/random.html\n",
    "\n",
    "\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given collection of items and ratio:\n",
    "     - partitions the collection into training and testing, where the proportion in training is ratio,\n",
    "\n",
    "    :param data: A list (or generator) of documents or doc ids\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the\n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(data)  #Found out number of samples present.  data could be a list or a generator\n",
    "    train_indices = random.sample(range(n), int(n * ratio))          #Randomly select training indices\n",
    "    test_indices = list(set(range(n)) - set(train_indices))   #Other items are testing indices\n",
    "\n",
    "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
    "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
    "\n",
    "    return (train, test)                       #Return split data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu1pTJhr7iQu"
   },
   "source": [
    "Now we can use this function to create training and testing data.  First, we need to create 4 lists:\n",
    "    * file ids  of positive docs to go in the training data\n",
    "    * file ids of positive docs to go in the testing data\n",
    "    * file ids of negative docs to go in the training data\n",
    "    * file ids of negative docs to go in the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QUVZGOpJ7iQv"
   },
   "outputs": [],
   "source": [
    "random.seed(41)  #set the random seeds so these random splits are always the same\n",
    "pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQjhg51zqe5R"
   },
   "source": [
    "Now, we want to create our labelled data sets.   We need to associate each review with its label so that later we can shuffle up all of the training data (and the testing data)\n",
    "\n",
    "### Exercise 1\n",
    "Write some python code which will construct a training set (`training`) and a test set (`testing`) from the data.  Each set should be a list of pairs where each pair is a list of words and a label, as below:\n",
    "\n",
    "<code>[([list,of,words],'label'),([list,of,words],'label'),...]</code>\n",
    "\n",
    "Hint:  You can do this with 4 list comprehensions and list concatenation.\n",
    "\n",
    "Check the size of `training` and `testing`.  Using a 70\\% split, how many should be in each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yP5wctINNDUS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set has 1400 entries, made up of 700 positive and 700 negative\n",
      "test set has 600 entries, made up of 300 positive and 300 negative\n"
     ]
    }
   ],
   "source": [
    "training_pos = [(movie_reviews.words(prev), 'pos') for prev in pos_train_ids] # loop through paths and convert into sentances\n",
    "training_neg = [(movie_reviews.words(nrev), 'neg') for nrev in neg_train_ids] # combine into a tuple with 'pos' or 'neg'\n",
    "training = training_pos + training_neg\n",
    "\n",
    "test_pos = [(movie_reviews.words(prev), 'pos') for prev in pos_test_ids]\n",
    "test_neg = [(movie_reviews.words(nrev), 'neg') for nrev in neg_test_ids]\n",
    "test = test_pos + test_neg\n",
    "\n",
    "print(f\"training set has {len(training)} entries, made up of {len(training_pos)} positive and {len(training_neg)} negative\")\n",
    "print(f\"test set has {len(test)} entries, made up of {len(test_pos)} positive and {len(test_neg)} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3SLme3Oqe5S"
   },
   "source": [
    "## Document Representations\n",
    "\n",
    "Currently, each review / document is represented as a list of tokens.  In many simple applications, the order of words in a document is deemed irrelevant and we use a **bag-of-words** representation of the document.  We can create a bag-of-words using a **dictionary** (as we did in Lab_2_2 when considering the size of the vocabulary) or we can use a library function such as **FreqDist** from nltk.probability (or Counter from Collections).  In the cell below, I generate the bag-of-words for the first review in the training set using nltk's FreqDist.  You can think of this as like a dictionary but with extra benefits.  For example, later on in the lab, we will see **it has useful methods** which allow the document representations to be added and subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zRTriMZIqe5S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 24, '.': 18, 'and': 11, 'a': 9, 'to': 8, 'the': 7, 'melvin': 6, 'his': 6, \"'\": 6, 's': 6, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "doc1 = FreqDist(training[0][0]) # grab an entry and the the sentence [0] (not the label [1])\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4qdRQEeqe5T"
   },
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Write code to use FreqDist to construct a bag-of-words representation for each document in the training and testing sets.  Store the results in two lists, `training_basic` and `testing_basic`.  Don't lose the annotations as to whether each review is positive or negative!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8Pesd-Rjqe5T"
   },
   "outputs": [],
   "source": [
    "training_basic = [[entry[0], FreqDist(entry[0]), entry[1]] for entry in training] # [freq, label]\n",
    "test_basic = [[entry[0], FreqDist(entry[0]), entry[1]] for entry in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8-sFdi8Jqe5T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n",
      "<FreqDist with 428 samples and 862 outcomes>\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "print(test_basic[0][0])\n",
    "print(test_basic[0][1])\n",
    "print(test_basic[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJd0x75aqe5U"
   },
   "source": [
    "You will notice of course that many of the words in your representations of documents are punctuation and stopwords.  This is because we haven't done any pre-processing of the wordlists.\n",
    "\n",
    "### Exercise 2.2\n",
    "\n",
    "Decide which of the following pre-processing steps to apply to the word lists:-\n",
    "* case normalisation\n",
    "* number normalisation\n",
    "* punctuation removal\n",
    "* stopword removal\n",
    "* stemmming / lemmatisation\n",
    "\n",
    "\n",
    "Apply these preprocessing steps to the original wordlist representations (stored in `training` and `testing`).  Then recreate the bag-of-words representations, storing the results in `training_norm` and `testing_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DtkW4fzYqe5U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_list(alist):\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    pl = [token.lower() for token in alist] # lower\n",
    "    pl2 = [\"NUM\" if token.isdigit() else token for token in pl] # numbers\n",
    "    pl3 = [token for token in pl2 if token.isalpha() and token not in stop] # stops and punc\n",
    "    \n",
    "    return pl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_norm = FreqDist()\n",
    "\n",
    "# for sentence in training:\n",
    "#     tokens = process_list(sentence[0])\n",
    "#     current_freq_dist = FreqDist(tokens)\n",
    "\n",
    "#     training_norm += current_freq_dist\n",
    "\n",
    "# the package provides a way to init a FreqDist and add to it\n",
    "# the alternative way is to conver to a dict and looping add to an empty dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_norm = []\n",
    "\n",
    "for sentence in training:\n",
    "    tokens = process_list(sentence[0])\n",
    "    current_freq_dist = FreqDist(tokens)\n",
    "\n",
    "    review = [current_freq_dist, sentence[1]]\n",
    "    training_norm.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'melvin': 6, 'simon': 4, 'dog': 4, 'playing': 3, 'one': 2, 'day': 2, 'serve': 2, 'carol': 2, 'threatens': 2, 'shut': 2, ...}),\n",
       " 'pos']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_norm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTGHJWSd7iQ5"
   },
   "source": [
    "## Creating word lists\n",
    "The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "572x5pEP7iQ6"
   },
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n",
    "- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n",
    "\n",
    "Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RPzluDd-7iQ6"
   },
   "outputs": [],
   "source": [
    "my_positive_word_list = [\"good\",\"great\",\"lovely\",\"best\", \"amazing\", \"top\", \"first\", \"favourite\", \"preference\", \"enjoy\", \"brilliant\", \"melon\"]\n",
    "my_negative_word_list = [\"bad\", \"terrible\", \"awful\", \"annoying\", \"sad\", \"hate\", \"avoid\", \"worst\", \"irritate\", \"downvote\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nQhC0Zaqe5W"
   },
   "source": [
    "Now lets see how often each of those words occurs in total in our positive and negative training data.  First, lets create a total of the FreqDists for positive data and for negative data.  As these are FreqDists (rather than simple dictionaries), we can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7Tg_0bttqe5X"
   },
   "outputs": [],
   "source": [
    "# this extracting all the words and their counts from the each entry and adding to holding dictionary\n",
    "\n",
    "pos_freq_dist=FreqDist()\n",
    "neg_freq_dist=FreqDist()\n",
    "\n",
    "for reviewDist,label in training_norm:\n",
    "    if label=='pos':\n",
    "        pos_freq_dist+=reviewDist\n",
    "    else:\n",
    "        neg_freq_dist+=reviewDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JNUtrGjVqe5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_freq_dist['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 859,\n",
       " 'great': 541,\n",
       " 'lovely': 20,\n",
       " 'best': 579,\n",
       " 'amazing': 82,\n",
       " 'top': 137,\n",
       " 'first': 705,\n",
       " 'favourite': 7,\n",
       " 'preference': 2,\n",
       " 'enjoy': 102,\n",
       " 'brilliant': 87,\n",
       " 'melon': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list_freq = {}\n",
    "for i in my_positive_word_list:\n",
    "    word_list_freq[i] = pos_freq_dist[i]\n",
    "word_list_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad': 709,\n",
       " 'terrible': 71,\n",
       " 'awful': 73,\n",
       " 'annoying': 88,\n",
       " 'sad': 42,\n",
       " 'hate': 46,\n",
       " 'avoid': 23,\n",
       " 'worst': 187,\n",
       " 'irritate': 2,\n",
       " 'downvote': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list_freq = {}\n",
    "for i in my_negative_word_list:\n",
    "    word_list_freq[i] = neg_freq_dist[i]\n",
    "word_list_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW9BMqP37iRH"
   },
   "source": [
    "### Exercise 3.2\n",
    "In the blank code cell below write code that uses the total frequency distributions `pos_freq_dist` and `neg_freq_dist` and the word lists `my_positive_word_list` and `my_negative_word_list` created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n",
    "- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n",
    "- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n",
    "\n",
    "You could display your findings in a table using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JPRJHuET7iRH"
   },
   "outputs": [],
   "source": [
    "pos_word_list_pos = {word: pos_freq_dist[word] for word in my_positive_word_list}\n",
    "pos_word_list_neg = {word: neg_freq_dist[word] for word in my_positive_word_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive Review Counts</th>\n",
       "      <th>Negative Negative Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>859</td>\n",
       "      <td>807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>541</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lovely</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>579</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>82</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>137</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>705</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favourite</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preference</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enjoy</th>\n",
       "      <td>102</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brilliant</th>\n",
       "      <td>87</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Positive Review Counts  Negative Negative Counts\n",
       "good                           859                       807\n",
       "great                          541                       287\n",
       "lovely                          20                        20\n",
       "best                           579                       368\n",
       "amazing                         82                        36\n",
       "top                            137                       136\n",
       "first                          705                       571\n",
       "favourite                        7                        10\n",
       "preference                       2                         1\n",
       "enjoy                          102                        62\n",
       "brilliant                       87                        36\n",
       "melon                            0                         0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Positive Review Counts': pos_word_list_pos, \n",
    "    'Negative Negative Counts': pos_word_list_neg\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bkfwFU4v7iRO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive Review Counts</th>\n",
       "      <th>Negative Negative Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>236</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>17</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>13</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>49</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate</th>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avoid</th>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>31</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irritate</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>downvote</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Positive Review Counts  Negative Negative Counts\n",
       "bad                          236                       709\n",
       "terrible                      17                        71\n",
       "awful                         13                        73\n",
       "annoying                      49                        88\n",
       "sad                           35                        42\n",
       "hate                          49                        46\n",
       "avoid                         34                        23\n",
       "worst                         31                       187\n",
       "irritate                       0                         2\n",
       "downvote                       0                         0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "neg_word_list_pos = {word: pos_freq_dist[word] for word in my_negative_word_list}\n",
    "neg_word_list_neg = {word: neg_freq_dist[word] for word in my_negative_word_list}\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Positive Review Counts': neg_word_list_pos, \n",
    "    'Negative Negative Counts': neg_word_list_neg\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OefXeFaKqe5b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bXExCJp7iRS"
   },
   "source": [
    "### Exercise 3.3\n",
    "Now, you are going to create positive and negative word lists automatically from the training data. In order to do this:\n",
    "\n",
    "1. write two new functions to help with automating the process of generating wordlists.\n",
    "\n",
    "    - `most_frequent_words` - this function should take THREE arguments: 2 frequency distributions and a natural number, k. It should order words by how much more they occur in one frequency distribution than the other.   It should then return the top k highest scoring words. You might want to use the `most_common` method from the `FreqDist` class - this returns a list of word, frequency pairs ordered by frequency.  You might also or alternatively want to use pythons built-in `sorted` function\n",
    "    - `words_above_threshold` - this function also takes three arguments: 2 frequency distributions and a natural number, k. Again, it should order words by how much more they occur in one distribution than the other.  It should return all of the words that have a score greater than k.\n",
    "\n",
    "2. Using the training data, create two sets of positive and negative word lists using these functions (1 set with each function).\n",
    "3.  Display these 4 lists (possibly in a `Pandas` dataframe?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "o6hw9SFM7iRY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counted_list = []\n",
    "for word in set(pos_freq_dist + neg_freq_dist):\n",
    "    pfreq = pos_freq_dist[word]\n",
    "    nfreq = neg_freq_dist[word]\n",
    "    diff = pfreq - nfreq\n",
    "\n",
    "    counted_list.append((word, diff))\n",
    "\n",
    "counted_list.sort(key=lambda item: item[1])\n",
    "# print(counted_list)\n",
    "\n",
    "# counted_list[:10] # negative exceeds negative\n",
    "# counted_list[-10:] # positive exceeds negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "kP84olqo7iRf"
   },
   "outputs": [],
   "source": [
    "def most_frequent_words(postive_review_dist, negative_review_dist, n):\n",
    "    _counted_list = []\n",
    "    \n",
    "    for word in set(postive_review_dist + negative_review_dist):\n",
    "        pfreq = postive_review_dist[word]\n",
    "        nfreq = negative_review_dist[word]\n",
    "        diff = pfreq - nfreq\n",
    "        _counted_list.append((word, diff))\n",
    "        \n",
    "    _counted_list.sort(key=lambda item: item[1]) # first/lowest/negative will be where negative exceeds postive\n",
    "\n",
    "    neg = _counted_list[:n]\n",
    "    pos = _counted_list[-n:]\n",
    "\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ABq5Sb0j_j2p"
   },
   "outputs": [],
   "source": [
    "freq_pos, freq_neg = most_frequent_words(pos_freq_dist, neg_freq_dist, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>film</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>life</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>also</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>great</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>story</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>films</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>supposed</td>\n",
       "      <td>-116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nothing</td>\n",
       "      <td>-137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>could</td>\n",
       "      <td>-139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>script</td>\n",
       "      <td>-144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>even</td>\n",
       "      <td>-153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>worst</td>\n",
       "      <td>-156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NUM</td>\n",
       "      <td>-171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>plot</td>\n",
       "      <td>-197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bad</td>\n",
       "      <td>-473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>movie</td>\n",
       "      <td>-605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Count\n",
       "9       film    756\n",
       "8       life    384\n",
       "7       also    300\n",
       "6      great    254\n",
       "5      story    220\n",
       "4      world    216\n",
       "3       many    213\n",
       "2      films    212\n",
       "1       best    211\n",
       "0        one    194\n",
       "19  supposed   -116\n",
       "18   nothing   -137\n",
       "17     could   -139\n",
       "16    script   -144\n",
       "15      even   -153\n",
       "14     worst   -156\n",
       "13       NUM   -171\n",
       "12      plot   -197\n",
       "11       bad   -473\n",
       "10     movie   -605"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(freq_pos + freq_neg, columns=['Word', 'Count']).sort_values(by='Count', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_above_threshold(postive_review_dist, negative_review_dist, n):\n",
    "    _pos = []\n",
    "    _neg = [] \n",
    "    _net = []\n",
    "    \n",
    "    for word in set(postive_review_dist + negative_review_dist):\n",
    "        pfreq = postive_review_dist[word]\n",
    "        nfreq = negative_review_dist[word]\n",
    "        diff = pfreq - nfreq\n",
    "\n",
    "        if diff >= n:\n",
    "            _pos.append((word, diff))\n",
    "        elif diff <= -n:\n",
    "            _neg.append((word, diff))\n",
    "        else:\n",
    "            _net.append((word, diff))\n",
    "\n",
    "    return _pos, _neg, _net         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_pos, thresh_neg, thresh_net = words_above_threshold(pos_freq_dist, neg_freq_dist, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>film</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>life</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>also</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>great</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>story</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>many</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>films</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>best</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>one</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>well</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>american</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>family</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jackie</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>first</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>although</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quite</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>performance</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>war</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>young</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>way</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>men</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>however</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>see</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>new</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mother</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>seen</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>job</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>john</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>star</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>people</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>love</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>least</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>boring</td>\n",
       "      <td>-108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>get</td>\n",
       "      <td>-109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>reason</td>\n",
       "      <td>-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>supposed</td>\n",
       "      <td>-116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>nothing</td>\n",
       "      <td>-137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>could</td>\n",
       "      <td>-139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>script</td>\n",
       "      <td>-144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>even</td>\n",
       "      <td>-153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>worst</td>\n",
       "      <td>-156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NUM</td>\n",
       "      <td>-171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>plot</td>\n",
       "      <td>-197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bad</td>\n",
       "      <td>-473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>movie</td>\n",
       "      <td>-605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Count\n",
       "15         film    756\n",
       "6          life    384\n",
       "23         also    300\n",
       "14        great    254\n",
       "2         story    220\n",
       "1         world    216\n",
       "9          many    213\n",
       "25        films    212\n",
       "26         best    211\n",
       "20          one    194\n",
       "28         well    184\n",
       "31     american    157\n",
       "30       family    157\n",
       "13       jackie    137\n",
       "7         first    134\n",
       "22     although    128\n",
       "0         quite    128\n",
       "19  performance    125\n",
       "5           war    125\n",
       "4         young    112\n",
       "17          way    111\n",
       "11          men    111\n",
       "10      however    110\n",
       "27          see    109\n",
       "29          new    109\n",
       "12       mother    108\n",
       "24         seen    104\n",
       "21          job    104\n",
       "18         john    104\n",
       "3          star    103\n",
       "8        people    101\n",
       "16         love    101\n",
       "39        least   -100\n",
       "38       stupid   -103\n",
       "46       boring   -108\n",
       "41          get   -109\n",
       "40       reason   -112\n",
       "35     supposed   -116\n",
       "34      nothing   -137\n",
       "32        could   -139\n",
       "45       script   -144\n",
       "36         even   -153\n",
       "33        worst   -156\n",
       "44          NUM   -171\n",
       "37         plot   -197\n",
       "42          bad   -473\n",
       "43        movie   -605"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_df = pd.DataFrame(thresh_pos + thresh_neg, columns=['Word', 'Count']).sort_values(by='Count', ascending=False)\n",
    "thresh_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzVP-hJH7iRi"
   },
   "source": [
    "## Creating a word list based classifier\n",
    "Now you have a number of word lists for use with a classifier.\n",
    "> Make sure you understand the following code, which will be used as the basis for creating a word list based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "CSaLWU_A7iRi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n",
      "This\n",
      "is\n",
      "an\n",
      "awful\n",
      "neg word found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, pos, neg):\n",
    "        self._pos = pos\n",
    "        self._neg = neg\n",
    "\n",
    "    def classify(self, words):\n",
    "        score = 0\n",
    "\n",
    "        for word in words:\n",
    "            print(word)\n",
    "            if word in self._pos:\n",
    "                score += 1\n",
    "                print(\"pos word found\")\n",
    "\n",
    "            elif word in self._neg:\n",
    "                score -= 1\n",
    "                print(\"neg word found\")\n",
    "\n",
    "            # else:\n",
    "            #     print(\"neutral word\")\n",
    "       \n",
    "        return \"neg\" if score < 0 else \"pos\"\n",
    "\n",
    "    ##we don't actually need to define the classify_many method as it is provided in ClassifierI\n",
    "    #def classify_many(self, docs):\n",
    "    #    return [self.classify(doc) for doc in docs]\n",
    "\n",
    "    def labels(self):\n",
    "        return (\"pos\", \"neg\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(my_positive_word_list, my_negative_word_list)\n",
    "classifier.classify(FreqDist(\"This is an awful movie movie\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irq5PVOc7iRl"
   },
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n",
    "- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n",
    "\n",
    "The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`.\n",
    "- For `score` less than 0, \"`neg`\" for negative should be returned.\n",
    "- For `score` greater than 0,  \"`pos`\" for positive should returned.\n",
    "- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "0UXUyFHM7iRm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n",
      "This\n",
      "is\n",
      "an\n",
      "awful\n",
      "neg word found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, pos, neg):\n",
    "        self._pos = pos\n",
    "        self._neg = neg\n",
    "\n",
    "    def classify(self, words):\n",
    "        score = 0\n",
    "\n",
    "        for word in words:\n",
    "            print(word)\n",
    "            if word in self._pos:\n",
    "                score += 1\n",
    "                print(\"pos word found\")\n",
    "\n",
    "            elif word in self._neg:\n",
    "                score -= 1\n",
    "                print(\"neg word found\")\n",
    "\n",
    "            # else:\n",
    "            #     print(\"neutral word\")\n",
    "       \n",
    "        return \"neg\" if score < 0 else \"pos\"\n",
    "\n",
    "    ##we don't actually need to define the classify_many method as it is provided in ClassifierI\n",
    "    #def classify_many(self, docs):\n",
    "    #    return [self.classify(doc) for doc in docs]\n",
    "\n",
    "    def labels(self):\n",
    "        return (\"pos\", \"neg\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(my_positive_word_list, my_negative_word_list)\n",
    "classifier.classify(FreqDist(\"This is an awful movie movie\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "GRJWPhUF7iRo",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Exercise 3.2\n",
    "* Extend your SimpleClassifier class so that it has a `train` function which will derive the wordlists from training data.  You could build a separate class for each way of automatically deriving wordlists (which both inherit from SimpleClassifier) OR a single class which takes an extra parameter at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KT1PbIao7iRp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h85qqrOV7iRr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uM8En0_7iRu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fWoIb8dDYzR"
   },
   "source": [
    "Try out your classifier on the test data.  We will look at how to evaluate classifiers in the next part, but in an ideal world, most of the positive test items will have been classified as 'P' and most of the negative test items will have been classified as 'N'.  Note that the batch_classify method takes a list of unlabelled documents so you can't give it a list of pairs (where each pair is doc and a label).  You can either use a list comprehension or the <code>zip(*list_of_pairs)</code> function to split a list of pairs into a pair of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6fQs92SVu39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFCEF0rx7iRx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us0zxFpsqe5g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9MTBKOuDPU9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuS6Urkaqe5g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nphj7NkSqe5g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
