{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swLDp-GV7iQg"
   },
   "source": [
    "# Week 3: Basic Document Classification (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLh_fS4P7iQn"
   },
   "source": [
    "## Overview\n",
    "In labs this week (and next), the focus will be on the application of sentiment analysis. You will be using a corpus of **movie reviews**.\n",
    "\n",
    "You will be exploring various techniques that can be used to classify the sentiment of the movie reviews as either positive or negative.\n",
    "\n",
    "You will be developing your own **Word List** and **Naïve Bayes** classifiers and then comparing them to the **NLTK Naïve Bayes** classifier.\n",
    "\n",
    "First, we will need to download the movie_review corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl (287 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.3.0 joblib-1.5.2 nltk-3.9.2 regex-2025.9.18 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3W2AdikDqe5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0uVSM40qe5I"
   },
   "source": [
    "The movie_reviews corpus reader provides a number of useful methods:\n",
    "   * .categories()\n",
    "   * .fileids()\n",
    "   * .words()\n",
    "   \n",
    "First, we can use `.categories()` to check the set of labels with which the reviews have been labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N3bRgahNqe5J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INtoThcCqe5J"
   },
   "source": [
    "We can use `.fileids()` to get all of the file names associated with a particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LZ00H5mdqe5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive reviews is 1000\n",
      "The number of negative reviews is 1000\n"
     ]
    }
   ],
   "source": [
    "pos_review_ids=movie_reviews.fileids('pos')\n",
    "neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "print(\"The number of positive reviews is {}\".format(len(pos_review_ids)))\n",
    "print(\"The number of negative reviews is {}\".format(len(neg_review_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoHxpQ6Sqe5L"
   },
   "source": [
    "We can use `.words()` to get back word-tokenised reviews.  The argument to `.words()` is the file id of an individual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KMvFYBPyqe5M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.words(pos_review_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "68LWdfszqe5M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movie_reviews.words(pos_review_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmkD_UEKqe5N"
   },
   "source": [
    "Note, the object returned by `movie_reviews.words()` looks a lot like a list (and behaves a lot like a list) - but it is actually a `StreamBackedCorpusView`.  This essentially means it is not necessarily all in memory  - it is retrieved from disk as needed.  If you want to see all of the words at once then you can convert it to a list using the `list()` constructor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XwmV4eh9qe5O",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', 'there', \"'\", 's', 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.', 'for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'\", '80s', 'with', 'a', '12', '-', 'part', 'series', 'called', 'the', 'watchmen', '.', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', '.', 'the', 'book', '(', 'or', '\"', 'graphic', 'novel', ',', '\"', 'if', 'you', 'will', ')', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'footnotes', '.', 'in', 'other', 'words', ',', 'don', \"'\", 't', 'dismiss', 'this', 'film', 'because', 'of', 'its', 'source', '.', 'if', 'you', 'can', 'get', 'past', 'the', 'whole', 'comic', 'book', 'thing', ',', 'you', 'might', 'find', 'another', 'stumbling', 'block', 'in', 'from', 'hell', \"'\", 's', 'directors', ',', 'albert', 'and', 'allen', 'hughes', '.', 'getting', 'the', 'hughes', 'brothers', 'to', 'direct', 'this', 'seems', 'almost', 'as', 'ludicrous', 'as', 'casting', 'carrot', 'top', 'in', ',', 'well', ',', 'anything', ',', 'but', 'riddle', 'me', 'this', ':', 'who', 'better', 'to', 'direct', 'a', 'film', 'that', \"'\", 's', 'set', 'in', 'the', 'ghetto', 'and', 'features', 'really', 'violent', 'street', 'crime', 'than', 'the', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', '?', 'the', 'ghetto', 'in', 'question', 'is', ',', 'of', 'course', ',', 'whitechapel', 'in', '1888', 'london', \"'\", 's', 'east', 'end', '.', 'it', \"'\", 's', 'a', 'filthy', ',', 'sooty', 'place', 'where', 'the', 'whores', '(', 'called', '\"', 'unfortunates', '\"', ')', 'are', 'starting', 'to', 'get', 'a', 'little', 'nervous', 'about', 'this', 'mysterious', 'psychopath', 'who', 'has', 'been', 'carving', 'through', 'their', 'profession', 'with', 'surgical', 'precision', '.', 'when', 'the', 'first', 'stiff', 'turns', 'up', ',', 'copper', 'peter', 'godley', '(', 'robbie', 'coltrane', ',', 'the', 'world', 'is', 'not', 'enough', ')', 'calls', 'in', 'inspector', 'frederick', 'abberline', '(', 'johnny', 'depp', ',', 'blow', ')', 'to', 'crack', 'the', 'case', '.', 'abberline', ',', 'a', 'widower', ',', 'has', 'prophetic', 'dreams', 'he', 'unsuccessfully', 'tries', 'to', 'quell', 'with', 'copious', 'amounts', 'of', 'absinthe', 'and', 'opium', '.', 'upon', 'arriving', 'in', 'whitechapel', ',', 'he', 'befriends', 'an', 'unfortunate', 'named', 'mary', 'kelly', '(', 'heather', 'graham', ',', 'say', 'it', 'isn', \"'\", 't', 'so', ')', 'and', 'proceeds', 'to', 'investigate', 'the', 'horribly', 'gruesome', 'crimes', 'that', 'even', 'the', 'police', 'surgeon', 'can', \"'\", 't', 'stomach', '.', 'i', 'don', \"'\", 't', 'think', 'anyone', 'needs', 'to', 'be', 'briefed', 'on', 'jack', 'the', 'ripper', ',', 'so', 'i', 'won', \"'\", 't', 'go', 'into', 'the', 'particulars', 'here', ',', 'other', 'than', 'to', 'say', 'moore', 'and', 'campbell', 'have', 'a', 'unique', 'and', 'interesting', 'theory', 'about', 'both', 'the', 'identity', 'of', 'the', 'killer', 'and', 'the', 'reasons', 'he', 'chooses', 'to', 'slay', '.', 'in', 'the', 'comic', ',', 'they', 'don', \"'\", 't', 'bother', 'cloaking', 'the', 'identity', 'of', 'the', 'ripper', ',', 'but', 'screenwriters', 'terry', 'hayes', '(', 'vertical', 'limit', ')', 'and', 'rafael', 'yglesias', '(', 'les', 'mis', '?', 'rables', ')', 'do', 'a', 'good', 'job', 'of', 'keeping', 'him', 'hidden', 'from', 'viewers', 'until', 'the', 'very', 'end', '.', 'it', \"'\", 's', 'funny', 'to', 'watch', 'the', 'locals', 'blindly', 'point', 'the', 'finger', 'of', 'blame', 'at', 'jews', 'and', 'indians', 'because', ',', 'after', 'all', ',', 'an', 'englishman', 'could', 'never', 'be', 'capable', 'of', 'committing', 'such', 'ghastly', 'acts', '.', 'and', 'from', 'hell', \"'\", 's', 'ending', 'had', 'me', 'whistling', 'the', 'stonecutters', 'song', 'from', 'the', 'simpsons', 'for', 'days', '(', '\"', 'who', 'holds', 'back', 'the', 'electric', 'car', '/', 'who', 'made', 'steve', 'guttenberg', 'a', 'star', '?', '\"', ')', '.', 'don', \"'\", 't', 'worry', '-', 'it', \"'\", 'll', 'all', 'make', 'sense', 'when', 'you', 'see', 'it', '.', 'now', 'onto', 'from', 'hell', \"'\", 's', 'appearance', ':', 'it', \"'\", 's', 'certainly', 'dark', 'and', 'bleak', 'enough', ',', 'and', 'it', \"'\", 's', 'surprising', 'to', 'see', 'how', 'much', 'more', 'it', 'looks', 'like', 'a', 'tim', 'burton', 'film', 'than', 'planet', 'of', 'the', 'apes', 'did', '(', 'at', 'times', ',', 'it', 'seems', 'like', 'sleepy', 'hollow', '2', ')', '.', 'the', 'print', 'i', 'saw', 'wasn', \"'\", 't', 'completely', 'finished', '(', 'both', 'color', 'and', 'music', 'had', 'not', 'been', 'finalized', ',', 'so', 'no', 'comments', 'about', 'marilyn', 'manson', ')', ',', 'but', 'cinematographer', 'peter', 'deming', '(', 'don', \"'\", 't', 'say', 'a', 'word', ')', 'ably', 'captures', 'the', 'dreariness', 'of', 'victorian', '-', 'era', 'london', 'and', 'helped', 'make', 'the', 'flashy', 'killing', 'scenes', 'remind', 'me', 'of', 'the', 'crazy', 'flashbacks', 'in', 'twin', 'peaks', ',', 'even', 'though', 'the', 'violence', 'in', 'the', 'film', 'pales', 'in', 'comparison', 'to', 'that', 'in', 'the', 'black', '-', 'and', '-', 'white', 'comic', '.', 'oscar', 'winner', 'martin', 'childs', \"'\", '(', 'shakespeare', 'in', 'love', ')', 'production', 'design', 'turns', 'the', 'original', 'prague', 'surroundings', 'into', 'one', 'creepy', 'place', '.', 'even', 'the', 'acting', 'in', 'from', 'hell', 'is', 'solid', ',', 'with', 'the', 'dreamy', 'depp', 'turning', 'in', 'a', 'typically', 'strong', 'performance', 'and', 'deftly', 'handling', 'a', 'british', 'accent', '.', 'ians', 'holm', '(', 'joe', 'gould', \"'\", 's', 'secret', ')', 'and', 'richardson', '(', '102', 'dalmatians', ')', 'log', 'in', 'great', 'supporting', 'roles', ',', 'but', 'the', 'big', 'surprise', 'here', 'is', 'graham', '.', 'i', 'cringed', 'the', 'first', 'time', 'she', 'opened', 'her', 'mouth', ',', 'imagining', 'her', 'attempt', 'at', 'an', 'irish', 'accent', ',', 'but', 'it', 'actually', 'wasn', \"'\", 't', 'half', 'bad', '.', 'the', 'film', ',', 'however', ',', 'is', 'all', 'good', '.', '2', ':', '00', '-', 'r', 'for', 'strong', 'violence', '/', 'gore', ',', 'sexuality', ',', 'language', 'and', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "print(list(movie_reviews.words(pos_review_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXWztGUZqe5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUYw61N7iQo"
   },
   "source": [
    "## Creating training and testing sets\n",
    "You will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance.\n",
    "\n",
    "We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n",
    "\n",
    "> Look through the code in the following cell, reading the comments and making sure that you understand each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HsMcMo5e7iQp"
   },
   "outputs": [],
   "source": [
    "import random # have a look at the documentation at https://docs.python.org/3/library/random.html\n",
    "\n",
    "\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given collection of items and ratio:\n",
    "     - partitions the collection into training and testing, where the proportion in training is ratio,\n",
    "\n",
    "    :param data: A list (or generator) of documents or doc ids\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the\n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(data)  #Found out number of samples present.  data could be a list or a generator\n",
    "    train_indices = random.sample(range(n), int(n * ratio))          #Randomly select training indices\n",
    "    test_indices = list(set(range(n)) - set(train_indices))   #Other items are testing indices\n",
    "\n",
    "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
    "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
    "\n",
    "    return (train, test)                       #Return split data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu1pTJhr7iQu"
   },
   "source": [
    "Now we can use this function to create training and testing data.  First, we need to create 4 lists:\n",
    "    * file ids  of positive docs to go in the training data\n",
    "    * file ids of positive docs to go in the testing data\n",
    "    * file ids of negative docs to go in the training data\n",
    "    * file ids of negative docs to go in the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QUVZGOpJ7iQv"
   },
   "outputs": [],
   "source": [
    "random.seed(41)  #set the random seeds so these random splits are always the same\n",
    "pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQjhg51zqe5R"
   },
   "source": [
    "Now, we want to create our labelled data sets.   We need to associate each review with its label so that later we can shuffle up all of the training data (and the testing data)\n",
    "\n",
    "### Exercise 1\n",
    "Write some python code which will construct a training set (`training`) and a test set (`testing`) from the data.  Each set should be a list of pairs where each pair is a list of words and a label, as below:\n",
    "\n",
    "<code>[([list,of,words],'label'),([list,of,words],'label'),...]</code>\n",
    "\n",
    "Hint:  You can do this with 4 list comprehensions and list concatenation.\n",
    "\n",
    "Check the size of `training` and `testing`.  Using a 70\\% split, how many should be in each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yP5wctINNDUS"
   },
   "outputs": [],
   "source": [
    "# movie_reviews.words(pos_review_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "kA8AJhWLqe5S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "pos_review_ids=movie_reviews.fileids('pos')\n",
    "neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "random.seed(41)\n",
    "\n",
    "pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
    "\n",
    "training = [(movie_reviews.words(prev), 'pos') for prev in pos_train_ids] + [(movie_reviews.words(nrev), 'neg') for nrev in neg_train_ids]\n",
    "test = [(movie_reviews.words(prev), 'pos') for prev in pos_test_ids] + [(movie_reviews.words(nrev), 'neg') for nrev in neg_test_ids]\n",
    "\n",
    "print(len(training))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['hello', 'kids', '.', 'today', 'the', 'movie', ...], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(training[random.randint(1, len(training))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'the', 'release', 'of', 'gattaca', ',', 'i', ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[random.randint(1, len(training))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['when', 'a', 'pair', 'of', 'films', 'from', 'the', ...], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(test[random.randint(1, len(test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reflecting', 'on', '\"', 'bedazzled', ',', '\"', 'a', ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[random.randint(1, len(test))][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3SLme3Oqe5S"
   },
   "source": [
    "## Document Representations\n",
    "\n",
    "Currently, each review / document is represented as a list of tokens.  In many simple applications, the order of words in a document is deemed irrelevant and we use a bag-of-words representation of the document.  We can create a bag-of-words using a dictionary (as we did in Lab_2_2 when considering the size of the vocabulary) or we can use a library function such as FreqDist from nltk.probability (or Counter from Collections).  In the cell below, I generate the bag-of-words for the first review in the training set using nltk's FreqDist.  You can think of this as like a dictionary but with extra benefits.  For example, later on in the lab, we will see it has useful methods which allow the document representations to be added and subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "zRTriMZIqe5S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 24, '.': 18, 'and': 11, 'a': 9, 'to': 8, 'the': 7, 'melvin': 6, 'his': 6, \"'\": 6, 's': 6, ...})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "doc1 = FreqDist(training[0][0])\n",
    "doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4qdRQEeqe5T"
   },
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Write code to use FreqDist to construct a bag-of-words representation for each document in the training and testing sets.  Store the results in two lists, `training_basic` and `testing_basic`.  Don't lose the annotations as to whether each review is positive or negative!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "8Pesd-Rjqe5T"
   },
   "outputs": [],
   "source": [
    "training_basic = [(FreqDist(review[0]), review[1]) for review in training]\n",
    "testing_basic = [(FreqDist(review[0]), review[1]) for review in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJd0x75aqe5U"
   },
   "source": [
    "You will notice of course that many of the words in your representations of documents are punctuation and stopwords.  This is because we haven't done any pre-processing of the wordlists.\n",
    "\n",
    "### Exercise 2.2\n",
    "\n",
    "Decide which of the following pre-processing steps to apply to the word lists:-\n",
    "* case normalisation\n",
    "* number normalisation\n",
    "* punctuation removal\n",
    "* stopword removal\n",
    "* stemmming / lemmatisation\n",
    "\n",
    "\n",
    "Apply these preprocessing steps to the original wordlist representations (stored in `training` and `testing`).  Then recreate the bag-of-words representations, storing the results in `training_norm` and `testing_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "DtkW4fzYqe5U"
   },
   "outputs": [],
   "source": [
    "# case normalisation: no, already done\n",
    "# number normalisation: no, numbers are often years which holds a lot of context, could process?\n",
    "# punctuation removal: yes, lots of punc\n",
    "# stopword removal: yes, lots of stopwords with little context\n",
    "# stemmming/lemmatisation: not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_list(alist):\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    pl = [token.lower() for token in alist] # lower\n",
    "    pl2 = [\"NUM\" if token.isdigit() else token for token in pl] # numbers\n",
    "    pl3 = [token for token in pl2 if token.isalpha() and token not in stop] # stops and punc\n",
    "    \n",
    "    return pl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_norm = [(FreqDist(process_list(tr_rev[0])), tr_rev[1]) for tr_rev in training]\n",
    "test_norm = [(FreqDist(process_list(ts_rev[0])), ts_rev[1]) for ts_rev in test]\n",
    "\n",
    "# test_norm[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTGHJWSd7iQ5"
   },
   "source": [
    "## Creating word lists\n",
    "The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "572x5pEP7iQ6"
   },
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n",
    "- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n",
    "\n",
    "Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "RPzluDd-7iQ6"
   },
   "outputs": [],
   "source": [
    "my_positive_word_list = [\"good\",\"great\",\"lovely\",\"best\", \"amazing\", \"top\", \"first\", \"favourite\", \"preference\", \"enjoy\"]\n",
    "my_negative_word_list = [\"bad\", \"terrible\", \"awful\", \"annoying\", \"sad\", \"hate\", \"avoid\", \"worst\", \"irritate\", \"downvote\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nQhC0Zaqe5W"
   },
   "source": [
    "Now lets see how often each of those words occurs in total in our positive and negative training data.  First, lets create a total of the FreqDists for positive data and for negative data.  As these are FreqDists (rather than simple dictionaries), we can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "7Tg_0bttqe5X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'film': 3737, 'one': 2127, 'NUM': 1892, 'movie': 1721, 'like': 1285, 'story': 893, 'time': 882, 'good': 859, 'also': 848, 'even': 804, ...})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "pos_freq_dist=FreqDist()\n",
    "neg_freq_dist=FreqDist()\n",
    "\n",
    "for reviewDist,label in training_norm:      \n",
    "    if label=='pos':\n",
    "        pos_freq_dist+=reviewDist\n",
    "    else:\n",
    "        neg_freq_dist+=reviewDist\n",
    "\n",
    "pos_freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW9BMqP37iRH"
   },
   "source": [
    "### Exercise 3.2\n",
    "In the blank code cell below write code that uses the total frequency distributions `pos_freq_dist` and `neg_freq_dist` and the word lists `my_positive_word_list` and `my_negative_word_list` created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n",
    "- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n",
    "- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n",
    "\n",
    "You could display your findings in a table using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "JPRJHuET7iRH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 859,\n",
       " 'great': 541,\n",
       " 'lovely': 20,\n",
       " 'best': 579,\n",
       " 'amazing': 82,\n",
       " 'top': 137,\n",
       " 'first': 705,\n",
       " 'favourite': 7,\n",
       " 'preference': 2,\n",
       " 'enjoy': 102}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_freq_list = {}\n",
    "\n",
    "for key in my_positive_word_list:\n",
    "    pos_freq_list[key] = pos_freq_dist[key]\n",
    "    # print(pos_freq_dist[key])\n",
    "\n",
    "pos_freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "b4aR84LE7iRL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad': 709,\n",
       " 'terrible': 71,\n",
       " 'awful': 73,\n",
       " 'annoying': 88,\n",
       " 'sad': 42,\n",
       " 'hate': 46,\n",
       " 'avoid': 23,\n",
       " 'worst': 187,\n",
       " 'irritate': 2,\n",
       " 'downvote': 0}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_freq_list = {}\n",
    "\n",
    "for key in my_negative_word_list:\n",
    "    neg_freq_list[key] = neg_freq_dist[key]\n",
    "    # print(pos_freq_dist[key])\n",
    "\n",
    "neg_freq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bXExCJp7iRS"
   },
   "source": [
    "### Exercise 3.3\n",
    "Now, you are going to create positive and negative word lists automatically from the training data. In order to do this:\n",
    "\n",
    "1. write two new functions to help with automating the process of generating wordlists.\n",
    "\n",
    "    - `most_frequent_words` - this function should take THREE arguments: 2 frequency distributions and a natural number, k. It should order words by how much more they occur in one frequency distribution than the other.   It should then return the top k highest scoring words. You might want to use the `most_common` method from the `FreqDist` class - this returns a list of word, frequency pairs ordered by frequency.  You might also or alternatively want to use pythons built-in `sorted` function\n",
    "    - `words_above_threshold` - this function also takes three arguments: 2 frequency distributions and a natural number, k. Again, it should order words by how much more they occur in one distribution than the other.  It should return all of the words that have a score greater than k.\n",
    "\n",
    "2. Using the training data, create two sets of positive and negative word lists using these functions (1 set with each function).\n",
    "3.  Display these 4 lists (possibly in a `Pandas` dataframe?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6hw9SFM7iRY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtwxYJmQ7iRb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kP84olqo7iRf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABq5Sb0j_j2p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idr8XYkWAmfl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaqsrbFTAmwI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzVP-hJH7iRi"
   },
   "source": [
    "## Creating a word list based classifier\n",
    "Now you have a number of word lists for use with a classifier.\n",
    "> Make sure you understand the following code, which will be used as the basis for creating a word list based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSaLWU_A7iRi"
   },
   "outputs": [],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, pos, neg):\n",
    "        self._pos = pos\n",
    "        self._neg = neg\n",
    "\n",
    "    def classify(self, words):\n",
    "        score = 0\n",
    "\n",
    "        # add code here that assigns an appropriate value to score\n",
    "        return \"neg\" if score < 0 else \"pos\"\n",
    "\n",
    "    ##we don't actually need to define the classify_many method as it is provided in ClassifierI\n",
    "    #def classify_many(self, docs):\n",
    "    #    return [self.classify(doc) for doc in docs]\n",
    "\n",
    "    def labels(self):\n",
    "        return (\"pos\", \"neg\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(my_positive_word_list, my_negative_word_list)\n",
    "classifier.classify(FreqDist(\"This movie was great\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irq5PVOc7iRl"
   },
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n",
    "- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n",
    "\n",
    "The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`.\n",
    "- For `score` less than 0, \"`neg`\" for negative should be returned.\n",
    "- For `score` greater than 0,  \"`pos`\" for positive should returned.\n",
    "- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UXUyFHM7iRm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "GRJWPhUF7iRo",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Exercise 3.2\n",
    "* Extend your SimpleClassifier class so that it has a `train` function which will derive the wordlists from training data.  You could build a separate class for each way of automatically deriving wordlists (which both inherit from SimpleClassifier) OR a single class which takes an extra parameter at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KT1PbIao7iRp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h85qqrOV7iRr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uM8En0_7iRu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fWoIb8dDYzR"
   },
   "source": [
    "Try out your classifier on the test data.  We will look at how to evaluate classifiers in the next part, but in an ideal world, most of the positive test items will have been classified as 'P' and most of the negative test items will have been classified as 'N'.  Note that the batch_classify method takes a list of unlabelled documents so you can't give it a list of pairs (where each pair is doc and a label).  You can either use a list comprehension or the <code>zip(*list_of_pairs)</code> function to split a list of pairs into a pair of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6fQs92SVu39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFCEF0rx7iRx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us0zxFpsqe5g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9MTBKOuDPU9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuS6Urkaqe5g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nphj7NkSqe5g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
