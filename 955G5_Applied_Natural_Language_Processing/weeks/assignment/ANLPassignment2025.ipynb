{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2S8I2ny-ovS"
   },
   "source": [
    "# ANLP Assignment: Sentiment Classification\n",
    "\n",
    "In this assignment, you will be investigating NLP methods for distinguishing positive and negative reviews written about movies.\n",
    "\n",
    "For assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about the assignment questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "The first few cells contain code to set-up the assignment and bring in some data.   In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell.  Otherwise do not change the code in these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1gXQAZas-l9c"
   },
   "outputs": [],
   "source": [
    "candidateno=291065 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk8JTP88A8vs",
    "outputId": "5ce22518-19d8-4c38-b8f9-13732a3c7a44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "#preliminary imports\n",
    "\n",
    "#set up nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "#for setting up training and testing data\n",
    "import random\n",
    "\n",
    "#useful other tools\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify.api import ClassifierI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BHBkzAccCVaZ"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the\n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = list(data)\n",
    "    n = len(data)\n",
    "    train_indices = random.sample(range(n), int(n * ratio))\n",
    "    test_indices = list(set(range(n)) - set(train_indices))\n",
    "    train = [data[i] for i in train_indices]\n",
    "    test = [data[i] for i in test_indices]\n",
    "    return (train, test)\n",
    "\n",
    "\n",
    "def get_train_test_data():\n",
    "\n",
    "    #get ids of positive and negative movie reviews\n",
    "    pos_review_ids=movie_reviews.fileids('pos')\n",
    "    neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "    #split positive and negative data into training and testing sets\n",
    "    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
    "    #add labels to the data and concatenate\n",
    "    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
    "    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n",
    "\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N3LWwBYICPP"
   },
   "source": [
    "When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HJLegkdPFUJA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of training data is 1400\n",
      "The amount of testing data is 600\n",
      "The representation of a single data item is below\n",
      "(['i', 'want', 'to', 'correct', 'what', 'i', 'wrote', ...], 'pos')\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "random.seed(candidateno)\n",
    "training_data,testing_data=get_train_test_data()\n",
    "print(\"The amount of training data is {}\".format(len(training_data)))\n",
    "print(\"The amount of testing data is {}\".format(len(testing_data)))\n",
    "print(\"The representation of a single data item is below\")\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Re-Useable Code and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(token_list: list, stop_words: list):\n",
    "    \"\"\"\n",
    "    Applies case normalization, removes numbers and punctuation, \n",
    "    and removes stopwords from a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        token_list (list): The input list of tokens (strings).\n",
    "        stop_words (set/list): A collection of words to be removed (stopwords).\n",
    "\n",
    "    Returns:\n",
    "        list: The preprocessed list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_list = [\n",
    "        token.lower()\n",
    "        for token in token_list  \n",
    "        if token.isalpha()        # removes numbers and punctuation\n",
    "        and token.lower() not in stop_words  # remove tokens in stop_words (by keeping those not in stop_words)\n",
    "    ]\n",
    "    \n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generating Positive and Negative Word Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbTq6eGv2XT2"
   },
   "source": [
    "a) **Generate** a list of 10 content words which are representative of the positive reviews in your training data.\n",
    "\n",
    "b) **Generate** a list of 10 content words which are representative of the negative reviews in your training data.\n",
    "\n",
    "c) **Explain** what you have done and why\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code (a & b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gvFu36xZ2XT5"
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "pos_freq_dist=FreqDist()\n",
    "neg_freq_dist=FreqDist()\n",
    "\n",
    "for rev,label in training_data:\n",
    "\n",
    "    # PREPROCESSING: case, number, punctuation, stopword\n",
    "    rev = preprocess_text(token_list=rev, stop_words=stop)\n",
    "\n",
    "    # FreqDist() data structure\n",
    "    if label == 'pos':\n",
    "        pos_freq_dist.update(rev) \n",
    "    elif label == 'neg':\n",
    "        neg_freq_dist.update(rev)\n",
    "    else: \n",
    "        print(\"unexpected class\")\n",
    "\n",
    "# IDENTIFY WORD DIFFERENTIAL BETWEEN CLASSSES\n",
    "all_words = set(pos_freq_dist.keys()) | set(neg_freq_dist.keys()) # all unique words\n",
    "\n",
    "word_counts = []\n",
    "\n",
    "for word in all_words:\n",
    "    pos_count = pos_freq_dist.get(word, 0)\n",
    "    neg_count = neg_freq_dist.get(word, 0)\n",
    "    difference = pos_count - neg_count\n",
    "    total = pos_count + neg_count\n",
    "    word_counts.append((word, difference, total))\n",
    "\n",
    "# IDENTIFY REVIEW DOMAIN STOP WORDS\n",
    "word_counts_copy = list(word_counts)\n",
    "word_counts_copy.sort(key=lambda item: item[2], reverse=True) # [2] = Total Corpus Count\n",
    "_domainStopWords = word_counts_copy[:5] # 5 most common review words\n",
    "domainStopWords = [word for word, diff, total in _domainStopWords]\n",
    "domain_stop_set = set(domainStopWords)\n",
    "\n",
    "# DROP DOMAIN WORDS FROM LIST\n",
    "final_word_counts = [word for word in word_counts if word[0] not in domain_stop_set]\n",
    "\n",
    "# SORY BY [1], THE CLASS/LABEL DIFFERENTIAL\n",
    "final_word_counts.sort(key=lambda item: item[1]) # low to high\n",
    "\n",
    "# COLLECT WORDS WITH THE LARGEST DIFFERENTIAL\n",
    "_negative_word_list = final_word_counts[:10] # first 10\n",
    "_positive_word_list = final_word_counts[-10:] # last 10\n",
    "\n",
    "_positive_word_list.sort(key=lambda item: item[1], reverse=True) # high to low\n",
    "\n",
    "negative_word_list = [word for word, diff, total in _negative_word_list]\n",
    "positive_word_list = [word for word, diff, total in _positive_word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movie', 'one', 'like', 'even']\n"
     ]
    }
   ],
   "source": [
    "print(domainStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JauTzY5N2XUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'plot', 'nothing', 'worst', 'script', 'stupid', 'boring', 'least', 'harry', 'supposed']\n"
     ]
    }
   ],
   "source": [
    "print(negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad', -476, 998), ('plot', -244, 1040), ('nothing', -153, 555), ('worst', -139, 205), ('script', -127, 533), ('stupid', -114, 174), ('boring', -113, 195), ('least', -107, 447), ('harry', -105, 163), ('supposed', -105, 223)]\n"
     ]
    }
   ],
   "source": [
    "print(_negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'also', 'great', 'well', 'best', 'story', 'many', 'world', 'love', 'first']\n"
     ]
    }
   ],
   "source": [
    "print(positive_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('life', 329, 1113), ('also', 327, 1395), ('great', 263, 833), ('well', 253, 1315), ('best', 245, 931), ('story', 229, 1555), ('many', 203, 901), ('world', 201, 729), ('love', 200, 800), ('first', 166, 1318)]\n"
     ]
    }
   ],
   "source": [
    "print(_positive_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code I am looping through each review in the training data whilst giving myself access to the wordlist and the label. \n",
    "\n",
    "On the all word lists I preprocess and normalise accounting for case, numbers, punctuation and stopwords. I am doing this to remove noise and get access to the words in their cleanest and more contextualised form and removing words devoid of meaningful context. I did not implement any stemming or lemmanisation as the goal is to obtain a word list, hence, readabliltiy is desired. \n",
    "\n",
    "After which I push the wordlists into pre-initalised world lists for each label. It is important to have seperate DistFreqs for the label because I want to compare the frequency between the two. \n",
    "\n",
    "For each word I calculate two metrics. The differential between the positive and negative counts, as well as, the cumulative word count for the corupus.\n",
    "\n",
    "Words that come out with a positive differential are candiates for the positive world list, and negative ones the negative word list.\n",
    "\n",
    "The cumulative word count is used to identify words that appear to be domain stop words. That is, words that appear to be disproporionately used in context of writing reviews. The frequency of these words in the domain of reviewing means they loose the context they might otherwise hold in general terms. I remove these words as candidates for the word list.\n",
    "\n",
    "Finally, the word list is sorting by the index [1] which is the word differential metric. Slices of the top and bottom 10 are taken which are the words that are most skewed towards being in positive or negative reviews, hence, should be highly representative of sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word List Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TApOQE6vND20"
   },
   "source": [
    "a) **Use** the lists generated in Q1 to build a **word list classifier** which will classify reviews as being positive or negative.\n",
    "\n",
    "b) **Explain** what you have done.\n",
    "\n",
    "[12.5\\%]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_weight_converter(sentiment_word_list: list):\n",
    "    \"\"\"\n",
    "    Takes a sentiment word list with frequency counts converts it into a Sentiment Polarity weightings.  \n",
    "\n",
    "    Args:\n",
    "        sentiment_word_list (Iterable): Nested word list comprised of word:string, differential: int and total:int. \n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary of word and its sentiment weighting\n",
    "    \"\"\"\n",
    "    sentiment_weights = {\n",
    "        word: differential / total_usage\n",
    "        for word, differential, total_usage in sentiment_word_list\n",
    "    }\n",
    "\n",
    "    return sentiment_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.29559748427672955, 'also': 0.23440860215053763, 'great': 0.3157262905162065, 'well': 0.19239543726235742, 'best': 0.2631578947368421, 'story': 0.1472668810289389, 'many': 0.2253052164261931, 'world': 0.2757201646090535, 'love': 0.25, 'first': 0.125948406676783}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_weight_converter(_positive_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad': -0.47695390781563124, 'plot': -0.23461538461538461, 'nothing': -0.2756756756756757, 'worst': -0.6780487804878049, 'script': -0.23827392120075047, 'stupid': -0.6551724137931034, 'boring': -0.5794871794871795, 'least': -0.23937360178970918, 'harry': -0.6441717791411042, 'supposed': -0.47085201793721976}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_weight_converter(_negative_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C6vK5Vyz2XUF"
   },
   "outputs": [],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "\n",
    "class ReviewClassifer(ClassifierI):\n",
    "\n",
    "    def __init__(self, pos, neg):\n",
    "        self._pos = sentiment_weight_converter(pos)\n",
    "        self._neg = sentiment_weight_converter(neg)\n",
    "\n",
    "    def classify(self, words):\n",
    "        score = 0\n",
    "\n",
    "        for word in words:\n",
    "            \n",
    "            if word in self._pos:\n",
    "                score += self._pos[word]\n",
    "\n",
    "            elif word in self._neg:\n",
    "                score += self._neg[word]\n",
    "\n",
    "        return \"neg\" if score <= 0 else \"pos\"\n",
    "\n",
    "    def labels(self):\n",
    "        return (\"pos\", \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos pos\n",
      "pos neg\n",
      "neg pos\n",
      "neg neg\n"
     ]
    }
   ],
   "source": [
    "#Example usage:\n",
    "\n",
    "classifier = ReviewClassifer(_positive_word_list, _negative_word_list)\n",
    "\n",
    "data = [\n",
    "    training_data[100], \n",
    "    training_data[500],\n",
    "    training_data[700],\n",
    "    training_data[1100]\n",
    "]\n",
    "\n",
    "for rev,label in data:\n",
    "    rev = preprocess_text(token_list=rev, stop_words=stop)\n",
    "\n",
    "    cls = classifier.classify(rev)\n",
    "\n",
    "    print(label, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have set up a list classifer class by inheriting from `ClassifierI` in the `nltk` package. \n",
    "\n",
    "This has been done to for standardization purpose to follow convention and for customization later in the assignment where I can easily knit `nltks` eval packages.\n",
    "\n",
    "Prior to writting the class I set up a function called `sentiment_weight_converter`. This function allows me to take my sentiment word list (pos or neg) and calculate a weighting for each word. The idea is to capture which words are more positive or negative.\n",
    "\n",
    "I had previously calculated the differential for each word to determine the most positive or negative words, as well as, counting the overall total frequency for each word. By taking a ratio of each per word, the score its weighted by how much the sentiment skew covers the total usage of a work\n",
    "\n",
    "- If a word is used 100 times and all seen instances are negative then it has a weighting of 1\n",
    "- If a word is used 100 times but only 80 instances are negative and 20 positive it has a differential of 60 and a weighting of 60/100 = 0.2. It is still considered a negative word but less so.\n",
    "\n",
    "$$\\text{Weighting Score} = \\frac{D}{T}$$\n",
    "\n",
    "Hopefully this approach will lead to an improve in some evaluation scores. Additionally, positive words overall seem to appear with more frequences but the negative weights appear to come out stronger so this may even find out and make classifcation better.\n",
    "\n",
    "Within the class, the `classify` method handles the computation. Looping through a review, if it sees a word in either sentiment list then the words correponding **weight** is added the `score` tally which is initalized as 0. Note, by definition negative words will have negative weights and visa versa for positive words, meaning each will pull the `score` in either direction. Finally after the loop has finished the method returns 'neg' if the score <= 0 or positive if  >0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZdDO_Y92XUH"
   },
   "source": [
    "a) **Calculate** the accuracy, precision, recall and F1 score of your classifier.\n",
    "\n",
    "b) Is it reasonable to evaluate the classifier in terms of its accuracy?  **Explain** your answer and give a counter-example (a scenario where it would / would not be reasonable to evaluate the classifier in terms of its accuracy).\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1LQc8bsA2XUI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Overall Accuracy: 0.7100\n",
      "------------------------------\n",
      "------------------------------\n",
      "Evaluation Metrics\n",
      "------------------------------\n",
      "Metrics for Class 'pos':\n",
      "  Precision: 0.6583\n",
      "  Recall:    0.8733\n",
      "  F1-Score:  0.7507\n",
      "------------------------------\n",
      "Metrics for Class 'neg':\n",
      "  Precision: 0.8119\n",
      "  Recall:    0.5467\n",
      "  F1-Score:  0.6534\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from nltk.classify.util import accuracy\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "classifier = ReviewClassifer(_positive_word_list, _negative_word_list)\n",
    "\n",
    "refsets = defaultdict(set) # init store for actual labels\n",
    "testsets = defaultdict(set) # init store for predicted labels\n",
    "\n",
    "# PROCCESS, PREDICT AND STORE\n",
    "for i, (words, label) in enumerate(testing_data):\n",
    "    \n",
    "    processed_words = preprocess_text(token_list=words, stop_words=stop)\n",
    "    \n",
    "    predicted_label = classifier.classify(processed_words)\n",
    "    \n",
    "    refsets[label].add(i)\n",
    "    testsets[predicted_label].add(i)\n",
    "\n",
    "# CALCULATE ACCURACY\n",
    "acc = nltk.classify.util.accuracy(\n",
    "    classifier, \n",
    "    [(preprocess_text(words, stop), label) for words, label in testing_data]\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Evaluation Metrics\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# CALCULATE PRECISION, RECALL, F1-SCORE\n",
    "for label in classifier.labels():\n",
    "    \n",
    "    # PRECISION: OF DOCUMENTS CLASSIFIED AS X, HOW MANY WERE ACTUALLY X\n",
    "    P = precision(refsets[label], testsets[label])\n",
    "    \n",
    "    # RECALL: OF DOCUMENTS THAT ARE ACTUALLY X, HOW MANY WERE CLASSIFIED AS X\n",
    "    R = recall(refsets[label], testsets[label])\n",
    "    \n",
    "    # F1-SCORE: The HARMONIC MEAN OF PRECISION AND RECALL\n",
    "    F = f_measure(refsets[label], testsets[label])\n",
    "    \n",
    "    print(f\"Metrics for Class '{label}':\")\n",
    "    print(f\"  Precision: {P:.4f}\")\n",
    "    print(f\"  Recall:    {R:.4f}\")\n",
    "    print(f\"  F1-Score:  {F:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_i80ceP2XUJ"
   },
   "source": [
    "Accuracy is the ratio of (True Positives + True Negatives) / Total Samples. Essentially it is the ratio of sames that were correct, irrespective of their class. We can generally say on some level that accuracy is a measure of a classifier's ability if the classes are balanced. This is because we can determine if the classifier is doing something. For example, if a dataset with binary classes is split 50/50 and the accuracy is coming out at 50% we might infer that the classifier is doing nothing more than a random guess or just programmed to always predict one type of class. This is in contrast to an imbalanced dataset that might be split 99/1. Here an accuracy of 99.5 might lead us to believe we have a strong classifier, however, the dominance of one class means that the classifier can learn to almost always predict this class and achieve a high accuracy whilst performing very poorly on the minor class. \n",
    "\n",
    "In our example, the classes in the training set are balanced 50/50 on the reviews sentiment label (pos or neg). However, what is important to note is that the features (words) are highly imbalanced. There are many more instances of the positive words in the reviews than there are the negative words. \n",
    "\n",
    "I would say that it is reasonable to use accuracy as an evaluation metric, just not in isolation nor as the final, definitive measure. Accuracy is a reasonable starting point because the classes are balanced. This means we can use it to inger whether our classifier is doing anything more than either random guessing or just guessing 1 class all of the time. If it passes this then we can begin to look at more in-depth metrics which can help us evaluate our feature imbalance. \n",
    "\n",
    "For example, our classifier encounters more positive evidence, resulting in more positive predictions. This results in a high recall for positive as it finds many of them but lower precision because it over-predicts and makes mistakes. Accuracy masks this imbalance, whereas precision and recall reveal the model's bias towards over-predicting the positive class.\n",
    "\n",
    "An example where accuracy would be a good overall metric is part-of-speech tagging where the goal is to label every word in a sentence/corpus with its correct grammatical category (e.g. Noun, Verb, Adjective etc). There are imbalances in this problem but not severe (99.5% vs 0.5%) to induce the imbalance paradox. The cost of error is generally uniform across classes, there are no catastrophic misclassifications that may be found in a rare disease problem. Overall the goal is just to get the max number for words correctly labelled.  \n",
    "\n",
    "To summarise, Accuracy has some use as an evaluation metric when class imbalance is close to 50/50 and errors between classes are generally considered equal in cost/impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) **Construct** a Naive Bayes classifier (e.g., from NLTK).\n",
    "\n",
    "b) Compare the performance of your word list classifier with the Naive Bayes classifier. Discuss your results. [12.5\\%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIS9UpmJNEAp"
   },
   "source": [
    "## NB Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Gwjig-Y12XUN"
   },
   "outputs": [],
   "source": [
    "def extract_features(word_list):\n",
    "    \"\"\"\n",
    "    Converts a list of words into a dictionary of presence features (Bag-of-Words).\n",
    "    \n",
    "    Args:\n",
    "        word_list (list): A list of tokens.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A feature set dictionary where words are keys and values are True.\n",
    "    \"\"\"\n",
    "    return dict([(word, True) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3AUsYRMa2XUN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training feature sets: 1400\n",
      "Example feature set:\n",
      "({'want': True, 'correct': True, 'wrote': True, 'former': True, 'retrospective': True, 'david': True, 'lean': True, 'war': True, 'picture': True, 'still': True, 'think': True, 'deserve': True, 'number': True, 'american': True, 'film': True, 'institute': True, 'list': True, 'greatest': True, 'movies': True, 'lumet': True, 'angry': True, 'men': True, 'wilder': True, 'witness': True, 'prosecution': True, 'kubrick': True, 'paths': True, 'glory': True, 'would': True, 'better': True, 'choices': True, 'best': True, 'oscar': True, 'deny': True, 'importance': True, 'bridge': True, 'river': True, 'kwai': True, 'cinematically': True, 'contents': True, 'set': True, 'burma': True, 'bataillon': True, 'british': True, 'soldiers': True, 'japanese': True, 'captivity': True, 'forced': True, 'build': True, 'strategically': True, 'momentous': True, 'railway': True, 'commanding': True, 'officer': True, 'colonel': True, 'nicholson': True, 'alec': True, 'guinness': True, 'insists': True, 'corresponding': True, 'geneva': True, 'conventions': True, 'officers': True, 'work': True, 'simple': True, 'workmen': True, 'struggling': True, 'toughly': True, 'col': True, 'forces': True, 'commandant': True, 'saito': True, 'sessue': True, 'hayakawa': True, 'give': True, 'way': True, 'respect': True, 'afterwards': True, 'assiduously': True, 'commits': True, 'building': True, 'considers': True, 'opportunity': True, 'raise': True, 'morale': True, 'wants': True, 'prove': True, 'superior': True, 'capabilities': True, 'high': True, 'command': True, 'sends': True, 'shall': True, 'destroy': True, 'among': True, 'shears': True, 'william': True, 'holden': True, 'escapee': True, 'prison': True, 'camp': True, 'major': True, 'warden': True, 'jack': True, 'hawkins': True, 'flaw': True, 'clich': True, 'characterization': True, 'people': True, 'presented': True, 'intellectually': True, 'inferior': True, 'incapable': True, 'consistently': True, 'question': True, 'military': True, 'spirit': True, 'seems': True, 'rather': True, 'fascinated': True, 'hierarchies': True, 'also': True, 'perceptible': True, 'conversations': True, 'regard': True, 'symptomatic': True, 'doubts': True, 'logic': True, 'somehow': True, 'unpleasant': True, 'person': True, 'audience': True, 'supposed': True, 'applaud': True, 'perseverance': True, 'concerning': True, 'spectators': True, 'neglect': True, 'risks': True, 'takes': True, 'plot': True, 'passes': True, 'means': True, 'perfect': True, 'lot': True, 'virtues': True, 'well': True, 'shows': True, 'madness': True, 'produce': True, 'minds': True, 'becomes': True, 'possessed': True, 'idea': True, 'hero': True, 'others': True, 'like': True, 'get': True, 'cynics': True, 'interesting': True, 'study': True, 'characters': True, 'clashing': True, 'interests': True, 'points': True, 'sometimes': True, 'ironic': True, 'dialogue': True, 'make': True, 'anti': True, 'despite': True, 'inconsistencies': True, 'treatment': True, 'theme': True, 'effective': True, 'atmospherically': True, 'direction': True, 'creates': True, 'suspense': True, 'especially': True, 'dramatic': True, 'though': True, 'wholly': True, 'plausible': True, 'showdown': True, 'magnificent': True, 'job': True, 'bringing': True, 'life': True, 'making': True, 'character': True, 'actors': True, 'deliver': True, 'good': True, 'performances': True, 'hildyard': True, 'fine': True, 'color': True, 'cinematography': True, 'apt': True, 'score': True, 'helpful': True, 'extraordinary': True, 'weaknesses': True, 'c': True, 'karl': True, 'rackwitz': True, 'klein': True, 'k': True, 'ris': True, 'germany': True}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "# Use the 'stop' list and 'preprocess_text' function defined in your Q1 code\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Apply preprocessing and feature extraction to the training data\n",
    "featuresets = [\n",
    "    (extract_features(preprocess_text(words, stop)), label)\n",
    "    for (words, label) in training_data\n",
    "] # Bag-of-Words (BoW)\n",
    "\n",
    "print(f\"Total training feature sets: {len(featuresets)}\")\n",
    "print(\"Example feature set:\")\n",
    "print(featuresets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bytPkuHf2XUO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully trained Naive Bayes Classifier.\n",
      "Most Informative Features\n",
      "               fashioned = True              pos : neg    =     13.7 : 1.0\n",
      "               insulting = True              neg : pos    =     11.7 : 1.0\n",
      "            breathtaking = True              pos : neg    =     11.0 : 1.0\n",
      "                  avoids = True              pos : neg    =      9.7 : 1.0\n",
      "                    bold = True              pos : neg    =      9.7 : 1.0\n",
      "                  elliot = True              pos : neg    =      9.7 : 1.0\n",
      "                  regard = True              pos : neg    =      9.7 : 1.0\n",
      "                seamless = True              pos : neg    =      9.7 : 1.0\n",
      "                  finest = True              pos : neg    =      9.3 : 1.0\n",
      "              astounding = True              pos : neg    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Train the Naive Bayes Classifier\n",
    "# Bag-of-Words (BoW) Naive Bayes Classifier\n",
    "nb_classifier = NaiveBayesClassifier.train(featuresets)\n",
    "\n",
    "print(\"\\nSuccessfully trained Naive Bayes Classifier.\")\n",
    "nb_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nb_classifier.classify(featuresets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review featuresets[0][1] has actual label of 'pos' and NB predicts 'pos'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review featuresets[0][1] has actual label of '{featuresets[0][1]}' and NB predicts '{prediction}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Naive Bayes Classifier Evaluation\n",
      "----------------------------------------\n",
      "Overall Accuracy: 0.6550\n",
      "\n",
      "----------------------------------------\n",
      "Evaluation Metrics\n",
      "----------------------------------------\n",
      "Metrics for Class 'pos':\n",
      "  Precision: 0.5924\n",
      "  Recall:    0.9933\n",
      "  F1-Score:  0.7422\n",
      "----------------------------------------\n",
      "Metrics for Class 'neg':\n",
      "  Precision: 0.9794\n",
      "  Recall:    0.3167\n",
      "  F1-Score:  0.4786\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "test_featuresets = [\n",
    "    (extract_features(preprocess_text(words, stop)), label)\n",
    "    for (words, label) in testing_data\n",
    "]\n",
    "\n",
    "refsets = defaultdict(set)\n",
    "testsets = defaultdict(set)\n",
    "\n",
    "for i, (features, label) in enumerate(test_featuresets):\n",
    "\n",
    "    predicted_label = nb_classifier.classify(features)\n",
    "    \n",
    "    # STORE TRUE AND PREDICTED LABELS\n",
    "    refsets[label].add(i)\n",
    "    testsets[predicted_label].add(i)\n",
    "\n",
    "# ACCURACY\n",
    "acc = accuracy(nb_classifier, test_featuresets)\n",
    "print(\"-\" * 40)\n",
    "print(f\"Naive Bayes Classifier Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Evaluation Metrics\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# PRECISION, RECALL, F1\n",
    "for label in nb_classifier.labels():\n",
    "\n",
    "    P = precision(refsets[label], testsets[label])\n",
    "    R = recall(refsets[label], testsets[label])\n",
    "    F = f_measure(refsets[label], testsets[label])\n",
    "    \n",
    "    print(f\"Metrics for Class '{label}':\")\n",
    "    print(f\"  Precision: {P:.4f}\")\n",
    "    print(f\"  Recall:    {R:.4f}\")\n",
    "    print(f\"  F1-Score:  {F:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance & Discuss your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, an accuracy of 65.5% looks poor. Particularly for a binary classification problem with a balanced dataset where a random guess hits 50% accuracy. This even underperforms the vastly simpler Word List Classifier (WLC) which has an accuracy of 71%.\n",
    "\n",
    "The Precision, Recall, and F1-scores tell us much more detail about performance and in particular, model bias. \n",
    "\n",
    "The Naive Bayes is suffering from classification bias towards the positive class. It prefers to predict something as positive. \n",
    "\n",
    "It has an extremely high recall of 99.33% which means it almost always finds all of the positive reviews. However, it massively over predicts positive reviews shown by a precision of 59.25%. This means 40.76% of the positive predictions were False Positives and were actually negative reviews. The F1 score of 74.22% looks fine but it is being driven by the high recall. The NB classifier is highly sensitive to positive reviews and will predict positive even if the evidence is weak. The WordList classifier followed the same sort of trend here with a lower Precision 65.83% and a higher Recall of 87.33%. The extremities of the WLC were less so but the harmonic means is largely the same with an F1-Score of 75.05%.\n",
    "\n",
    "Conversely, for the negative class, the NB has a precision of 97.94% meaning that if the model decided to predict a review is negative then it is almost always correct about that. But it is really struggling to find an adequate number of the total negative reviews with a recall of only 31.67% - which is very low. This means approximately 68% of true negative reviews were missed (False Negatives), leading to a very low coverage. This is summarized by the F1 score of 47.86%. The NB classifier is highly conservative on on negative predictions and has poor coverage. Similar to the positive class, the WLC follows the same trends as the Naive Bayes but has less extreme values. Its Precision was 81.19% and its Recall was 54.67%. However, for negative reviews the Recall is comparatively way better leading to a better harmonic mean with a F1-Score of 65.34%.\n",
    "\n",
    "| Metric   | NB ('pos') | WLC ('pos') | NB ('neg') | WLC ('neg') |\n",
    "|:---------|:----------|:-----------|:----------|:-----------|\n",
    "| Precision| 59.25%    | 65.83%     | 97.94%    | 81.19%     |\n",
    "| Recall   | 99.33%    | 87.33%     | 31.67%    | 54.67%     |\n",
    "| F1-Score | 74.22%    | 75.05%     | 47.86%    | 65.34%     |\n",
    "\n",
    "The WLC achieved a more balanced performance as represented by better F1-Scores in both classes. It would appear that the constrained sentiment features lists of the WordList classifier allowed it to generalize better to the test set. The small feature set meant that it was able to avoid the cumulative compounding effect of weak evidence that the NB fell for.\n",
    "\n",
    "Overall, the both model's sevre bias is a symptom of the underlying data feature imbalance. Whilst the classes are distributed 50/50 the total frequency of positive-skewed tokens in the corpus is much higher than the negatives. Naive Bayes is a frequency-based probabilistic model, hence, is designed to reflect this by weighting decisions towards the class with the higher volume of the dominant feature. In this scenario, the NB classifier is too sophisticated for the noise in the data, whereas the basic constraints of the Word List approach unintentionally provided superior regularization against the feature-level bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGDXaVDqOSfY"
   },
   "source": [
    "a) Design and **carry out an experiment** into the impact of the **length of the wordlists** on the wordlist classifier.  Make sure you **describe** design decisions in your experiment, include a **graph** of your results and **discuss** your conclusions.\n",
    "\n",
    "b) Would you **recommend** a wordlist classifier or a Naive Bayes classifier for future work in this area?  **Justify** your answer.\n",
    "\n",
    "[25\\%]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlxoUthX2XUP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1L7mZ-k2XUQ"
   },
   "source": [
    "b) problem to solve is feature imbalance\n",
    "\n",
    "world list approach is a feature regularizer\n",
    "\n",
    "nb inhertently is impacted by the extremeites of the data issue\n",
    "\n",
    "could try to preproccess the data to reduce the features but structure of nb is probably just too extreme. shown by lemm as feature reduction doing worse\n",
    "\n",
    "I rec word list because it can be refinded\n",
    "\n",
    "additionally as lightly experiemented in my example, words can be weighted. my hypo was that the weighting itself would help to offset (which is may have) but further changes to weighting could be experiemented with as a hyperparameter, i.e. double weights of negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFeOWIRm2XUQ"
   },
   "source": [
    "Why lemm didn't improve: https://gemini.google.com/share/6de999c3df85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK components if not already done\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Converts a Treebank POS tag (used by nltk.pos_tag) to a \n",
    "    WordNet POS tag (needed by WordNetLemmatizer).\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to Noun if unsure\n",
    "\n",
    "def preprocess_and_lemmatize(token_list: list, stop_words: list):\n",
    "    \"\"\"\n",
    "    Applies case normalization, removes non-alpha tokens, removes stopwords, \n",
    "    and applies POS-aware lemmatization.\n",
    "    \"\"\"\n",
    "    cleaned_tokens = [token.lower() for token in token_list if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    # 1. Get POS tags for the cleaned tokens\n",
    "    tagged_tokens = nltk.pos_tag(cleaned_tokens)\n",
    "    \n",
    "    # 2. Apply POS-aware lemmatization\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        w_net_pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=w_net_pos)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "        \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Define stop words list again\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lemmatized training feature sets: 1400\n"
     ]
    }
   ],
   "source": [
    "# Apply new preprocessing and feature extraction to the training data\n",
    "lemmatized_featuresets = [\n",
    "    (extract_features(preprocess_and_lemmatize(words, stop)), label)\n",
    "    for (words, label) in training_data\n",
    "]\n",
    "\n",
    "# Apply to the testing data for evaluation\n",
    "lemmatized_test_featuresets = [\n",
    "    (extract_features(preprocess_and_lemmatize(words, stop)), label)\n",
    "    for (words, label) in testing_data\n",
    "]\n",
    "\n",
    "print(f\"Total lemmatized training feature sets: {len(lemmatized_featuresets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully trained Lemmatized Naive Bayes Classifier.\n",
      "Most Informative Features\n",
      "               mesmerize = True              pos : neg    =     13.0 : 1.0\n",
      "                weakness = True              pos : neg    =     12.3 : 1.0\n",
      "            breathtaking = True              pos : neg    =     11.0 : 1.0\n",
      "                  muddle = True              neg : pos    =     10.3 : 1.0\n",
      "                    bold = True              pos : neg    =      9.7 : 1.0\n",
      "                  elliot = True              pos : neg    =      9.7 : 1.0\n",
      "                seamless = True              pos : neg    =      9.7 : 1.0\n",
      "              degenerate = True              neg : pos    =      9.0 : 1.0\n",
      "                 forrest = True              pos : neg    =      9.0 : 1.0\n",
      "                  hatred = True              pos : neg    =      9.0 : 1.0\n",
      "\n",
      "==================================================\n",
      "Lemmatized Naive Bayes Classifier Evaluation (Accuracy: 0.6383)\n",
      "==================================================\n",
      "Metrics for Class 'pos':\n",
      "  Precision: 0.5819\n",
      "  Recall:    0.9833\n",
      "  F1-Score:  0.7311\n",
      "--------------------------------------------------\n",
      "Metrics for Class 'neg':\n",
      "  Precision: 0.9462\n",
      "  Recall:    0.2933\n",
      "  F1-Score:  0.4478\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from collections import defaultdict\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "# Train the new Naive Bayes Classifier\n",
    "nb_lem_classifier = NaiveBayesClassifier.train(lemmatized_featuresets)\n",
    "\n",
    "print(\"\\nSuccessfully trained Lemmatized Naive Bayes Classifier.\")\n",
    "nb_lem_classifier.show_most_informative_features(10)\n",
    "\n",
    "# Evaluation\n",
    "refsets_lem = defaultdict(set)\n",
    "testsets_lem = defaultdict(set)\n",
    "\n",
    "for i, (features, label) in enumerate(lemmatized_test_featuresets):\n",
    "    predicted_label = nb_lem_classifier.classify(features)\n",
    "    refsets_lem[label].add(i)\n",
    "    testsets_lem[predicted_label].add(i)\n",
    "\n",
    "# ACCURACY\n",
    "acc_lem = accuracy(nb_lem_classifier, lemmatized_test_featuresets)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Lemmatized Naive Bayes Classifier Evaluation (Accuracy: {acc_lem:.4f})\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PRECISION, RECALL, F1\n",
    "for label in nb_lem_classifier.labels():\n",
    "    P_lem = precision(refsets_lem[label], testsets_lem[label])\n",
    "    R_lem = recall(refsets_lem[label], testsets_lem[label])\n",
    "    F_lem = f_measure(refsets_lem[label], testsets_lem[label])\n",
    "    \n",
    "    print(f\"Metrics for Class '{label}':\")\n",
    "    print(f\"  Precision: {P_lem:.4f}\")\n",
    "    print(f\"  Recall:    {R_lem:.4f}\")\n",
    "    print(f\"  F1-Score:  {F_lem:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
