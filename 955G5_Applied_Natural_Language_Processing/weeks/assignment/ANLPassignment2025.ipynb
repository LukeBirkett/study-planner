{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2S8I2ny-ovS"
   },
   "source": [
    "# ANLP Assignment: Sentiment Classification\n",
    "\n",
    "In this assignment, you will be investigating NLP methods for distinguishing positive and negative reviews written about movies.\n",
    "\n",
    "For assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about the assignment questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "The first few cells contain code to set-up the assignment and bring in some data.   In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell.  Otherwise do not change the code in these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "1gXQAZas-l9c"
   },
   "outputs": [],
   "source": [
    "candidateno=291065 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk8JTP88A8vs",
    "outputId": "5ce22518-19d8-4c38-b8f9-13732a3c7a44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "#preliminary imports\n",
    "\n",
    "#set up nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "#for setting up training and testing data\n",
    "import random\n",
    "\n",
    "#useful other tools\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify.api import ClassifierI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BHBkzAccCVaZ"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the\n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = list(data)\n",
    "    n = len(data)\n",
    "    train_indices = random.sample(range(n), int(n * ratio))\n",
    "    test_indices = list(set(range(n)) - set(train_indices))\n",
    "    train = [data[i] for i in train_indices]\n",
    "    test = [data[i] for i in test_indices]\n",
    "    return (train, test)\n",
    "\n",
    "\n",
    "def get_train_test_data():\n",
    "\n",
    "    #get ids of positive and negative movie reviews\n",
    "    pos_review_ids=movie_reviews.fileids('pos')\n",
    "    neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "    #split positive and negative data into training and testing sets\n",
    "    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
    "    #add labels to the data and concatenate\n",
    "    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
    "    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n",
    "\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N3LWwBYICPP"
   },
   "source": [
    "When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "HJLegkdPFUJA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of training data is 1400\n",
      "The amount of testing data is 600\n",
      "The representation of a single data item is below\n",
      "(['i', 'want', 'to', 'correct', 'what', 'i', 'wrote', ...], 'pos')\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "random.seed(candidateno)\n",
    "training_data,testing_data=get_train_test_data()\n",
    "print(\"The amount of training data is {}\".format(len(training_data)))\n",
    "print(\"The amount of testing data is {}\".format(len(testing_data)))\n",
    "print(\"The representation of a single data item is below\")\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbTq6eGv2XT2"
   },
   "source": [
    "1)  \n",
    "a) **Generate** a list of 10 content words which are representative of the positive reviews in your training data.\n",
    "\n",
    "b) **Generate** a list of 10 content words which are representative of the negative reviews in your training data.\n",
    "\n",
    "c) **Explain** what you have done and why\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "gvFu36xZ2XT5"
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "pos_freq_dist=FreqDist()\n",
    "neg_freq_dist=FreqDist()\n",
    "\n",
    "for rev,label in training_data:\n",
    "\n",
    "    # PREPROCESSING: Case normalise, number normalise (remove), punctuation removal and stopword removal\n",
    "    rev = [\n",
    "        token.lower()\n",
    "        for token in rev \n",
    "        if token.isalpha()\n",
    "        and token.lower() not in stop\n",
    "    ]\n",
    "\n",
    "    # DISTRIBUTE WORD LISTS INTO FreqDist() FORMAT\n",
    "    if label == 'pos':\n",
    "        pos_freq_dist.update(rev) \n",
    "    elif label == 'neg':\n",
    "        neg_freq_dist.update(rev)\n",
    "    else: \n",
    "        print(\"classes are broken\")\n",
    "\n",
    "# COLLECT ALL WORDS, DIFFERENTIAL BETWEEN LABELS AND TOTAL CORPUS COUNT\n",
    "all_words = set(pos_freq_dist.keys()) | set(neg_freq_dist.keys())\n",
    "word_counts = []\n",
    "for word in all_words:\n",
    "    pos_count = pos_freq_dist.get(word, 0)\n",
    "    neg_count = neg_freq_dist.get(word, 0)\n",
    "    difference = pos_count - neg_count\n",
    "    total = pos_count + neg_count\n",
    "    word_counts.append((word, difference, total))\n",
    "\n",
    "# IDENTIFY REVIEW DOMAIN STOP WORDS\n",
    "word_counts_copy = list(word_counts)\n",
    "word_counts_copy.sort(key=lambda item: item[2], reverse=True) # [2] = Total Corpus Count\n",
    "_domainStopWords = word_counts_copy[:5] # 5 most common review words\n",
    "domainStopWords = [word for word, diff, total in _domainStopWords]\n",
    "domain_stop_set = set(domainStopWords)\n",
    "\n",
    "# DROP DOMAIN WORDS FROM LIST\n",
    "final_word_counts = [word for word in word_counts if word[0] not in domain_stop_set]\n",
    "\n",
    "# SORY BY [1], THE CLASS/LABEL DIFFERENTIAL\n",
    "final_word_counts.sort(key=lambda item: item[1])\n",
    "\n",
    "# COLLECT WORDS WITH THE LARGEST DIFFERENTIAL\n",
    "_negative_word_list = final_word_counts[:10]\n",
    "_positive_word_list = final_word_counts[-10:]\n",
    "_positive_word_list.sort(key=lambda item: item[1], reverse=True)\n",
    "\n",
    "negative_word_list = [word for word, diff, total in _negative_word_list]\n",
    "positive_word_list = [word for word, diff, total in _positive_word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movie', 'one', 'like', 'even']\n"
     ]
    }
   ],
   "source": [
    "print(domainStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "JauTzY5N2XUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'plot', 'even', 'nothing', 'worst', 'script', 'stupid', 'boring', 'least', 'supposed']\n"
     ]
    }
   ],
   "source": [
    "print(negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'also', 'great', 'well', 'best', 'story', 'many', 'world', 'love', 'first']\n"
     ]
    }
   ],
   "source": [
    "print(positive_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "In this code I am looping through each review in the training data whilst giving myself access to the wordlist and the label. \n",
    "\n",
    "On the all word lists I preprocess and normalise accounting for case, numbers, punctuation and stopwords. I am doing this to remove noise and get access to the words in their cleanest and more contextualised form and removing words devoid of meaningful context. I did not implement any stemming or lemmanisation as the goal is to obtain a word list, hence, readabliltiy is desired. \n",
    "\n",
    "After which I push the wordlists into pre-initalised world lists for each label. It is important to have seperate DistFreqs for the label because I want to compare the frequency between the two. \n",
    "\n",
    "For each word I calculate two metrics. The differential between the positive and negative counts, as well as, the cumulative word count for the corupus.\n",
    "\n",
    "Words that come out with a positive differential are candiates for the positive world list, and negative ones the negative word list.\n",
    "\n",
    "The cumulative word count is used to identify words that appear to be domain stop words. That is, words that appear to be disproporionately used in context of writing reviews. The frequency of these words in the domain of reviewing means they loose the context they might otherwise hold in general terms. I remove these words as candidates for the word list.\n",
    "\n",
    "Finally, the word list is sorting by the index [1] which is the word differential metric. Slices of the top and bottom 10 are taken which are the words that are most skewed towards being in positive or negative reviews, hence, should be highly representative of sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TApOQE6vND20"
   },
   "source": [
    "2)\n",
    "a) **Use** the lists generated in Q1 to build a **word list classifier** which will classify reviews as being positive or negative.\n",
    "\n",
    "b) **Explain** what you have done.\n",
    "\n",
    "[12.5\\%]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BThDMrcmODJy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6vK5Vyz2XUF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZdDO_Y92XUH"
   },
   "source": [
    "3)\n",
    "a) **Calculate** the accuracy, precision, recall and F1 score of your classifier.\n",
    "\n",
    "b) Is it reasonable to evaluate the classifier in terms of its accuracy?  **Explain** your answer and give a counter-example (a scenario where it would / would not be reasonable to evaluate the classifier in terms of its accuracy).\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LQc8bsA2XUI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_i80ceP2XUJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbYwwhcs2XUL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVZp0N5J2XUL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIS9UpmJNEAp"
   },
   "source": [
    "4)\n",
    "a)  **Construct** a Naive Bayes classifier (e.g., from NLTK).\n",
    "\n",
    "b)  **Compare** the performance of your word list classifier with the Naive Bayes classifier.  **Discuss** your results.\n",
    "\n",
    "[12.5\\%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gwjig-Y12XUN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AUsYRMa2XUN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bytPkuHf2XUO"
   },
   "source": [
    "However, if your goal were to build a full-scale machine learning classifier (like the Na√Øve Bayes mentioned in your labs) where the feature space is very large and the focus is purely on classification accuracy, then lemmatization (which provides a real dictionary word) would be strongly recommended to reduce noise and aggregate related features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGDXaVDqOSfY"
   },
   "source": [
    "5)\n",
    "a) Design and **carry out an experiment** into the impact of the **length of the wordlists** on the wordlist classifier.  Make sure you **describe** design decisions in your experiment, include a **graph** of your results and **discuss** your conclusions.\n",
    "\n",
    "b) Would you **recommend** a wordlist classifier or a Naive Bayes classifier for future work in this area?  **Justify** your answer.\n",
    "\n",
    "[25\\%]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlxoUthX2XUP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1L7mZ-k2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFeOWIRm2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VT82P88M2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym-TGvYS2XUR"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
