{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2S8I2ny-ovS"
   },
   "source": [
    "# ANLP Assignment: Sentiment Classification\n",
    "\n",
    "In this assignment, you will be investigating NLP methods for distinguishing positive and negative reviews written about movies.\n",
    "\n",
    "For assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about the assignment questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "The first few cells contain code to set-up the assignment and bring in some data.   In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell.  Otherwise do not change the code in these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1gXQAZas-l9c"
   },
   "outputs": [],
   "source": [
    "candidateno=291065 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk8JTP88A8vs",
    "outputId": "5ce22518-19d8-4c38-b8f9-13732a3c7a44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "#preliminary imports\n",
    "\n",
    "#set up nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "#for setting up training and testing data\n",
    "import random\n",
    "\n",
    "#useful other tools\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify.api import ClassifierI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BHBkzAccCVaZ"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the\n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = list(data)\n",
    "    n = len(data)\n",
    "    train_indices = random.sample(range(n), int(n * ratio))\n",
    "    test_indices = list(set(range(n)) - set(train_indices))\n",
    "    train = [data[i] for i in train_indices]\n",
    "    test = [data[i] for i in test_indices]\n",
    "    return (train, test)\n",
    "\n",
    "\n",
    "def get_train_test_data():\n",
    "\n",
    "    #get ids of positive and negative movie reviews\n",
    "    pos_review_ids=movie_reviews.fileids('pos')\n",
    "    neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "    #split positive and negative data into training and testing sets\n",
    "    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
    "    #add labels to the data and concatenate\n",
    "    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
    "    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n",
    "\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N3LWwBYICPP"
   },
   "source": [
    "When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HJLegkdPFUJA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of training data is 1400\n",
      "The amount of testing data is 600\n",
      "The representation of a single data item is below\n",
      "(['i', 'want', 'to', 'correct', 'what', 'i', 'wrote', ...], 'pos')\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "random.seed(candidateno)\n",
    "training_data,testing_data=get_train_test_data()\n",
    "print(\"The amount of training data is {}\".format(len(training_data)))\n",
    "print(\"The amount of testing data is {}\".format(len(testing_data)))\n",
    "print(\"The representation of a single data item is below\")\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Re-Useable Code and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(token_list: list, stop_words: list):\n",
    "    \"\"\"\n",
    "    Applies case normalization, removes numbers and punctuation, \n",
    "    and removes stopwords from a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        token_list (list): The input list of tokens (strings).\n",
    "        stop_words (set/list): A collection of words to be removed (stopwords).\n",
    "\n",
    "    Returns:\n",
    "        list: The preprocessed list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_list = [\n",
    "        token.lower()\n",
    "        for token in token_list  \n",
    "        if token.isalpha()        # removes numbers and punctuation\n",
    "        and token.lower() not in stop_words  # remove tokens in stop_words (by keeping those not in stop_words)\n",
    "    ]\n",
    "    \n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generating Positive and Negative Word Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbTq6eGv2XT2"
   },
   "source": [
    "a) **Generate** a list of 10 content words which are representative of the positive reviews in your training data.\n",
    "\n",
    "b) **Generate** a list of 10 content words which are representative of the negative reviews in your training data.\n",
    "\n",
    "c) **Explain** what you have done and why\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code (a & b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "gvFu36xZ2XT5"
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "pos_freq_dist=FreqDist()\n",
    "neg_freq_dist=FreqDist()\n",
    "\n",
    "for rev,label in training_data:\n",
    "\n",
    "    # PREPROCESSING: case, number, punctuation, stopword\n",
    "    rev = preprocess_text(token_list=rev, stop_words=stop)\n",
    "\n",
    "    # FreqDist() data structure\n",
    "    if label == 'pos':\n",
    "        pos_freq_dist.update(rev) \n",
    "    elif label == 'neg':\n",
    "        neg_freq_dist.update(rev)\n",
    "    else: \n",
    "        print(\"unexpected class\")\n",
    "\n",
    "# IDENTIFY WORD DIFFERENTIAL BETWEEN CLASSSES\n",
    "all_words = set(pos_freq_dist.keys()) | set(neg_freq_dist.keys()) # all unique words\n",
    "\n",
    "word_counts = []\n",
    "\n",
    "for word in all_words:\n",
    "    pos_count = pos_freq_dist.get(word, 0)\n",
    "    neg_count = neg_freq_dist.get(word, 0)\n",
    "    difference = pos_count - neg_count\n",
    "    total = pos_count + neg_count\n",
    "    word_counts.append((word, difference, total))\n",
    "\n",
    "# IDENTIFY REVIEW DOMAIN STOP WORDS\n",
    "word_counts_copy = list(word_counts)\n",
    "word_counts_copy.sort(key=lambda item: item[2], reverse=True) # [2] = Total Corpus Count\n",
    "_domainStopWords = word_counts_copy[:5] # 5 most common review words\n",
    "domainStopWords = [word for word, diff, total in _domainStopWords]\n",
    "domain_stop_set = set(domainStopWords)\n",
    "\n",
    "# DROP DOMAIN WORDS FROM LIST\n",
    "final_word_counts = [word for word in word_counts if word[0] not in domain_stop_set]\n",
    "\n",
    "# SORY BY [1], THE CLASS/LABEL DIFFERENTIAL\n",
    "final_word_counts.sort(key=lambda item: item[1]) # low to high\n",
    "\n",
    "# COLLECT WORDS WITH THE LARGEST DIFFERENTIAL\n",
    "_negative_word_list = final_word_counts[:10] # first 10\n",
    "_positive_word_list = final_word_counts[-10:] # last 10\n",
    "\n",
    "_positive_word_list.sort(key=lambda item: item[1], reverse=True) # high to low\n",
    "\n",
    "negative_word_list = [word for word, diff, total in _negative_word_list]\n",
    "positive_word_list = [word for word, diff, total in _positive_word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movie', 'one', 'like', 'even']\n"
     ]
    }
   ],
   "source": [
    "print(domainStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JauTzY5N2XUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'plot', 'nothing', 'worst', 'script', 'stupid', 'boring', 'least', 'harry', 'supposed']\n"
     ]
    }
   ],
   "source": [
    "print(negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad', -476, 998), ('plot', -244, 1040), ('nothing', -153, 555), ('worst', -139, 205), ('script', -127, 533), ('stupid', -114, 174), ('boring', -113, 195), ('least', -107, 447), ('harry', -105, 163), ('supposed', -105, 223)]\n"
     ]
    }
   ],
   "source": [
    "print(_negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'also', 'great', 'well', 'best', 'story', 'many', 'world', 'love', 'first']\n"
     ]
    }
   ],
   "source": [
    "print(positive_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('life', 329, 1113), ('also', 327, 1395), ('great', 263, 833), ('well', 253, 1315), ('best', 245, 931), ('story', 229, 1555), ('many', 203, 901), ('world', 201, 729), ('love', 200, 800), ('first', 166, 1318)]\n"
     ]
    }
   ],
   "source": [
    "print(_positive_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explanation (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code I am looping through each review in the training data whilst giving myself access to the wordlist and the label. \n",
    "\n",
    "On the all word lists I preprocess and normalise accounting for case, numbers, punctuation and stopwords. I am doing this to remove noise and get access to the words in their cleanest and more contextualised form and removing words devoid of meaningful context. I did not implement any stemming or lemmanisation as the goal is to obtain a word list, hence, readabliltiy is desired. \n",
    "\n",
    "After which I push the wordlists into pre-initalised world lists for each label. It is important to have seperate DistFreqs for the label because I want to compare the frequency between the two. \n",
    "\n",
    "For each word I calculate two metrics. The differential between the positive and negative counts, as well as, the cumulative word count for the corupus.\n",
    "\n",
    "Words that come out with a positive differential are candiates for the positive world list, and negative ones the negative word list.\n",
    "\n",
    "The cumulative word count is used to identify words that appear to be domain stop words. That is, words that appear to be disproporionately used in context of writing reviews. The frequency of these words in the domain of reviewing means they loose the context they might otherwise hold in general terms. I remove these words as candidates for the word list.\n",
    "\n",
    "Finally, the word list is sorting by the index [1] which is the word differential metric. Slices of the top and bottom 10 are taken which are the words that are most skewed towards being in positive or negative reviews, hence, should be highly representative of sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word List Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TApOQE6vND20"
   },
   "source": [
    "a) **Use** the lists generated in Q1 to build a **word list classifier** which will classify reviews as being positive or negative.\n",
    "\n",
    "b) **Explain** what you have done.\n",
    "\n",
    "[12.5\\%]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_weight_converter(sentiment_word_list: list):\n",
    "    \"\"\"\n",
    "    Takes a sentiment word list with frequency counts converts it into a Sentiment Polarity weightings.  \n",
    "\n",
    "    Args:\n",
    "        sentiment_word_list (Iterable): Nested word list comprised of word:string, differential: int and total:int. \n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary of word and its sentiment weighting\n",
    "    \"\"\"\n",
    "    sentiment_weights = {\n",
    "        word: differential / total_usage\n",
    "        for word, differential, total_usage in sentiment_word_list\n",
    "    }\n",
    "\n",
    "    return sentiment_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.29559748427672955, 'also': 0.23440860215053763, 'great': 0.3157262905162065, 'well': 0.19239543726235742, 'best': 0.2631578947368421, 'story': 0.1472668810289389, 'many': 0.2253052164261931, 'world': 0.2757201646090535, 'love': 0.25, 'first': 0.125948406676783}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_weight_converter(_positive_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad': -0.47695390781563124, 'plot': -0.23461538461538461, 'nothing': -0.2756756756756757, 'worst': -0.6780487804878049, 'script': -0.23827392120075047, 'stupid': -0.6551724137931034, 'boring': -0.5794871794871795, 'least': -0.23937360178970918, 'harry': -0.6441717791411042, 'supposed': -0.47085201793721976}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_weight_converter(_negative_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "C6vK5Vyz2XUF"
   },
   "outputs": [],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "\n",
    "class ReviewClassifer(ClassifierI):\n",
    "\n",
    "    def __init__(self, pos, neg):\n",
    "        self._pos = sentiment_weight_converter(pos)\n",
    "        self._neg = sentiment_weight_converter(neg)\n",
    "\n",
    "    def classify(self, words):\n",
    "        score = 0\n",
    "\n",
    "        for word in words:\n",
    "            \n",
    "            if word in self._pos:\n",
    "                score += self._pos[word]\n",
    "\n",
    "            elif word in self._neg:\n",
    "                score += self._neg[word]\n",
    "\n",
    "        return \"neg\" if score <= 0 else \"pos\"\n",
    "\n",
    "    def labels(self):\n",
    "        return (\"pos\", \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos pos\n",
      "pos neg\n",
      "neg pos\n",
      "neg neg\n"
     ]
    }
   ],
   "source": [
    "#Example usage:\n",
    "\n",
    "classifier = ReviewClassifer(_positive_word_list, _negative_word_list)\n",
    "\n",
    "data = [\n",
    "    training_data[100], \n",
    "    training_data[500],\n",
    "    training_data[700],\n",
    "    training_data[1100]\n",
    "]\n",
    "\n",
    "for rev,label in data:\n",
    "    rev = preprocess_text(token_list=rev, stop_words=stop)\n",
    "\n",
    "    cls = classifier.classify(rev)\n",
    "\n",
    "    print(label, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have set up a list classifer class by inheriting from `ClassifierI` in the `nltk` package. \n",
    "\n",
    "This has been done to for standardization purpose to follow convention and for customization later in the assignment where I can easily knit `nltks` eval packages.\n",
    "\n",
    "Prior to writting the class I set up a function called `sentiment_weight_converter`. This function allows me to take my sentiment word list (pos or neg) and calculate a weighting for each word. The idea is to capture which words are more positive or negative.\n",
    "\n",
    "I had previously calculated the differential for each word to determine the most positive or negative words, as well as, counting the overall total frequency for each word. By taking a ratio of each per word, the score its weighted by how much the sentiment skew covers the total usage of a work\n",
    "\n",
    "- If a word is used 100 times and all seen instances are negative then it has a weighting of 1\n",
    "- If a word is used 100 times but only 80 instances are negative and 20 positive it has a differential of 60 and a weighting of 60/100 = 0.2. It is still considered a negative word but less so.\n",
    "\n",
    "$$\\text{Weighting Score} = \\frac{D}{T}$$\n",
    "\n",
    "Hopefully this approach will lead to an improve in some evaluation scores. Additionally, positive words overall seem to appear with more frequences but the negative weights appear to come out stronger so this may even find out and make classifcation better.\n",
    "\n",
    "Within the class, the `classify` method handles the computation. Looping through a review, if it sees a word in either sentiment list then the words correponding **weight** is added the `score` tally which is initalized as 0. Note, by definition negative words will have negative weights and visa versa for positive words, meaning each will pull the `score` in either direction. Finally after the loop has finished the method returns 'neg' if the score <= 0 or positive if  >0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZdDO_Y92XUH"
   },
   "source": [
    "a) **Calculate** the accuracy, precision, recall and F1 score of your classifier.\n",
    "\n",
    "b) Is it reasonable to evaluate the classifier in terms of its accuracy?  **Explain** your answer and give a counter-example (a scenario where it would / would not be reasonable to evaluate the classifier in terms of its accuracy).\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "1LQc8bsA2XUI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Overall Accuracy: 0.7100\n",
      "------------------------------\n",
      "------------------------------\n",
      "Evaluation Metrics\n",
      "------------------------------\n",
      "Metrics for Class 'pos':\n",
      "  Precision: 0.6583\n",
      "  Recall:    0.8733\n",
      "  F1-Score:  0.7507\n",
      "------------------------------\n",
      "Metrics for Class 'neg':\n",
      "  Precision: 0.8119\n",
      "  Recall:    0.5467\n",
      "  F1-Score:  0.6534\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from nltk.classify.util import accuracy\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "classifier = ReviewClassifer(_positive_word_list, _negative_word_list)\n",
    "\n",
    "refsets = defaultdict(set) # init store for actual labels\n",
    "testsets = defaultdict(set) # init store for predicted labels\n",
    "\n",
    "# PROCCESS, PREDICT AND STORE\n",
    "for i, (words, label) in enumerate(testing_data):\n",
    "    \n",
    "    processed_words = preprocess_text(token_list=words, stop_words=stop)\n",
    "    \n",
    "    predicted_label = classifier.classify(processed_words)\n",
    "    \n",
    "    refsets[label].add(i)\n",
    "    testsets[predicted_label].add(i)\n",
    "\n",
    "# CALCULATE ACCURACY\n",
    "acc = nltk.classify.util.accuracy(\n",
    "    classifier, \n",
    "    [(preprocess_text(words, stop), label) for words, label in testing_data]\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Evaluation Metrics\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# CALCULATE PRECISION, RECALL, F1-SCORE\n",
    "for label in classifier.labels():\n",
    "    \n",
    "    # PRECISION: OF DOCUMENTS CLASSIFIED AS X, HOW MANY WERE ACTUALLY X\n",
    "    P = precision(refsets[label], testsets[label])\n",
    "    \n",
    "    # RECALL: OF DOCUMENTS THAT ARE ACTUALLY X, HOW MANY WERE CLASSIFIED AS X\n",
    "    R = recall(refsets[label], testsets[label])\n",
    "    \n",
    "    # F1-SCORE: The HARMONIC MEAN OF PRECISION AND RECALL\n",
    "    F = f_measure(refsets[label], testsets[label])\n",
    "    \n",
    "    print(f\"Metrics for Class '{label}':\")\n",
    "    print(f\"  Precision: {P:.4f}\")\n",
    "    print(f\"  Recall:    {R:.4f}\")\n",
    "    print(f\"  F1-Score:  {F:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_i80ceP2XUJ"
   },
   "source": [
    "b) Is it reasonable to evaluate the classifier in terms of its accuracy? Explain your answer and give a counter-example.\n",
    "\n",
    "Evaluating a classifier primarily using Accuracy is generally reasonable only if the class distribution in the test data is roughly balanced. If the classes are heavily imbalanced, Accuracy becomes an unreliable and potentially misleading metric.Explanation: The Problem with ImbalanceAccuracy is defined as the number of correct predictions divided by the total number of predictions:$$\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Samples}}$$When the classes are balanced (e.g., 50% positive reviews, 50% negative reviews), a high accuracy score genuinely reflects the classifier's ability to distinguish between both classes.However, if the classes are imbalanced, a classifier can achieve a deceptively high accuracy by simply guessing the majority class most of the time. This is known as the Accuracy Paradox.Counter-Example (Scenario where it is NOT Reasonable)It is NOT reasonable to rely on Accuracy when a dataset exhibits severe class imbalance.Scenario: Rare Disease Diagnosis (99% Negative, 1% Positive)Test Data Distribution: 990 people without the disease (Negative), 10 people with the disease (Positive). (Total = 1000)Classifier Type: A \"lazy\" classifier is built that always predicts \"Negative.\"Result:Correct Predictions (TP + TN): 990 (All the true negatives)Total Predictions: 1000Accuracy: $990 / 1000 = \\mathbf{99\\%}$In this scenario, the $\\mathbf{99\\%}$ accuracy suggests a highly effective model, but it is completely useless: it failed to identify a single person with the disease (Recall for the positive class is $0\\%$). The high Accuracy is entirely a product of the class imbalance, making it an unreasonable metric for assessment.Scenario where it IS ReasonableIt IS reasonable to rely on Accuracy when the class distribution is roughly balanced (e.g., between 40%-60%).Scenario: Fair Coin Toss Prediction (50% Heads, 50% Tails)Test Data Distribution: 500 Heads, 500 Tails.Result: If a classifier achieves $\\mathbf{75\\%}$ Accuracy, we know it is genuinely better than random guessing (50%) and is successfully distinguishing between the two outcomes $75\\%$ of the time. There is no large majority class to game the score.ConclusionFor your assignment, given that the test sets in NLP assignments are often balanced, Accuracy is a fine first metric. However, because it hides the specific failure modes you observed (low Recall for 'neg'), it is always more reasonable to use the F1-Score, Precision, and Recall, as these metrics explicitly measure performance on the individual classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "VbYwwhcs2XUL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 600\n",
      "Positive Samples ('pos'): 300 (50.0%)\n",
      "Negative Samples ('neg'): 300 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming 'testing_data' is a list of tuples: [(word_list, label), ...]\n",
    "labels = [label for words, label in testing_data]\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = len(labels)\n",
    "\n",
    "# Calculate the percentage for each class\n",
    "pos_count = label_counts['pos']\n",
    "neg_count = label_counts['neg']\n",
    "pos_percentage = (pos_count / total_samples) * 100\n",
    "neg_percentage = (neg_count / total_samples) * 100\n",
    "\n",
    "print(f\"Total Samples: {total_samples}\")\n",
    "print(f\"Positive Samples ('pos'): {pos_count} ({pos_percentage:.1f}%)\")\n",
    "print(f\"Negative Samples ('neg'): {neg_count} ({neg_percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVZp0N5J2XUL"
   },
   "source": [
    "That is a fantastic point and an insightful connection to make between your **feature engineering** (the word lists) and your **evaluation results** (Accuracy, Precision, Recall)!\n",
    "\n",
    "You are absolutely correct: in this specific context, where you are using a **word list classifier**, the **balance of the features (the words)** is often more critical to the evaluation discussion than the balance of the overall documents.\n",
    "\n",
    "---\n",
    "\n",
    "## Word Feature Imbalance vs. Document Imbalance\n",
    "\n",
    "| Consideration | Document Imbalance (Sample Counts) | Feature Imbalance (Word Counts) |\n",
    "| :---: | :---: | :---: |\n",
    "| **What it Measures** | Whether the number of 'pos' reviews equals the number of 'neg' reviews. | Whether the total frequency of positive sentiment words equals the total frequency of negative sentiment words in the entire corpus. |\n",
    "| **Impact on Accuracy** | Directly leads to the **Accuracy Paradox** (deceptively high Accuracy). | **Indirectly** influences Accuracy, but directly explains the **Precision/Recall trade-off**. |\n",
    "| **Your Finding** | Balanced (e.g., 50%/50%). | Unbalanced (More positive words). |\n",
    "\n",
    "### Impact of Feature Imbalance on Accuracy\n",
    "\n",
    "When your documents are balanced (50% 'pos' / 50% 'neg'), but your **corpus contains significantly more positive words** (higher frequency of features):\n",
    "\n",
    "1.  **Accuracy is still a reasonable starting point:** Because the document counts are balanced, your overall $\\mathbf{71\\%}$ Accuracy is a reliable measure of how many times the model got the answer right, regardless of class. You are not susceptible to the Accuracy Paradox.\n",
    "\n",
    "2.  **Accuracy is *INSUFFICIENT* for full assessment:** The greater frequency of positive words makes the classifier inherently more sensitive to the positive class. This sensitivity creates the **Precision/Recall imbalance** you observed.\n",
    "\n",
    "| Feature Finding | Classifier Behavior | Evaluation Result |\n",
    "| :--- | :--- | :--- |\n",
    "| **More Positive Words** | The classifier encounters positive evidence more often, resulting in many 'pos' predictions. | High **Recall for 'pos'** (it finds many of them) but lower **Precision for 'pos'** (it over-predicts and makes mistakes). |\n",
    "| **Fewer Negative Words** | The classifier encounters negative evidence less often, leading to fewer 'neg' predictions. | High **Precision for 'neg'** (it's cautious, so when it predicts 'neg', it's usually right) but lower **Recall for 'neg'** (it misses many of them). |\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Your thinking is **correct**. For your assignment, the key takeaway is that the **imbalance in your corpus word counts (features)** is the direct cause of the unbalanced $\\text{Precision}/\\text{Recall}$ scores, even though the **document counts (samples)** are balanced. This feature imbalance is why **Accuracy alone is insufficient** to fully describe your classifier's performance, as it hides the underlying class-specific bias caused by your feature lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a great request to solidify your understanding of when Accuracy is an appropriate metric. The key principle is **balance and equal cost of errors**.\n",
    "\n",
    "Here is an NLP example where using **Accuracy** as the primary evaluation metric is perfectly acceptable and reasonable.\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario: Part-of-Speech (POS) Tagging\n",
    "\n",
    "### The Task\n",
    "\n",
    "**Part-of-Speech (POS) Tagging** is a core NLP task where the goal is to label every word in a sentence with its correct grammatical category (e.g., Noun, Verb, Adjective, Preposition, etc.).\n",
    "\n",
    "* **Input:** \"The quick brown fox\"\n",
    "* **Output:** \"The/DT quick/JJ brown/JJ fox/NN\" (DT=Determiner, JJ=Adjective, NN=Noun)\n",
    "\n",
    "### Why Accuracy is Reasonable Here\n",
    "\n",
    "1.  **High-Dimensionality, Low Imbalance (Relatively):** While there are many more nouns than conjunctions in any given corpus, the task is about getting *every single word* right. The most common POS tags (like Noun, Verb, Determiner, Preposition) still appear with high frequency and no single class overwhelmingly dominates to the degree seen in severe binary imbalance (e.g., 99% vs 1%).\n",
    "2.  **Equal Cost of Error:** In POS tagging, misclassifying a **Noun** as a **Verb** is generally considered just as problematic as misclassifying a **Determiner** as an **Adverb**. There is no \"critical\" class where errors are catastrophically worse than others (unlike diagnosing a disease or detecting hate speech).\n",
    "3.  **Simplicity of Goal:** The goal of a POS tagger is straightforward: **get the maximum number of words tagged correctly.** Since every token is a prediction and every correct prediction is equally valued, **Overall Accuracy** is the most direct measure of the model's success.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Because the goal is simply to maximize the total number of correct tags across a large, varied, and generally well-distributed set of classes, **Accuracy** provides a complete and non-misleading picture of the POS tagger's overall performance. If a model has 97% Accuracy, it means 97 out of every 100 words are tagged correctly, which is precisely what the user of a POS tagger wants to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to summarise, accuracy is a good metric to look at when absolute accurary accross an entired set is the most important thing, and when there are no imbalances which can hide deficencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refine it slightly, Accuracy is a good metric when:\n",
    "\n",
    "Balance: The class distribution of the data is roughly balanced (e.g., close to 50%/50%), preventing the Accuracy Paradox.\n",
    "\n",
    "Equal Cost: The goal is to maximize the overall number of correct predictions, and errors in all classes are considered to have a roughly equal cost or impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIS9UpmJNEAp"
   },
   "source": [
    "a)  **Construct** a Naive Bayes classifier (e.g., from NLTK).\n",
    "\n",
    "b)  **Compare** the performance of your word list classifier with the Naive Bayes classifier.  **Discuss** your results.\n",
    "\n",
    "[12.5\\%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gwjig-Y12XUN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AUsYRMa2XUN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bytPkuHf2XUO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGDXaVDqOSfY"
   },
   "source": [
    "a) Design and **carry out an experiment** into the impact of the **length of the wordlists** on the wordlist classifier.  Make sure you **describe** design decisions in your experiment, include a **graph** of your results and **discuss** your conclusions.\n",
    "\n",
    "b) Would you **recommend** a wordlist classifier or a Naive Bayes classifier for future work in this area?  **Justify** your answer.\n",
    "\n",
    "[25\\%]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlxoUthX2XUP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1L7mZ-k2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFeOWIRm2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VT82P88M2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym-TGvYS2XUR"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
