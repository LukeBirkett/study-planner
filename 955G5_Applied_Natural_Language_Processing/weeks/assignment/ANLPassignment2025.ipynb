{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2S8I2ny-ovS"
   },
   "source": [
    "# ANLP Assignment: Sentiment Classification\n",
    "\n",
    "In this assignment, you will be investigating NLP methods for distinguishing positive and negative reviews written about movies.\n",
    "\n",
    "For assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about the assignment questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "The first few cells contain code to set-up the assignment and bring in some data.   In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell.  Otherwise do not change the code in these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1gXQAZas-l9c"
   },
   "outputs": [],
   "source": [
    "candidateno=291065 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk8JTP88A8vs",
    "outputId": "5ce22518-19d8-4c38-b8f9-13732a3c7a44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "#preliminary imports\n",
    "\n",
    "#set up nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "#for setting up training and testing data\n",
    "import random\n",
    "\n",
    "#useful other tools\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify.api import ClassifierI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BHBkzAccCVaZ"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the\n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = list(data)\n",
    "    n = len(data)\n",
    "    train_indices = random.sample(range(n), int(n * ratio))\n",
    "    test_indices = list(set(range(n)) - set(train_indices))\n",
    "    train = [data[i] for i in train_indices]\n",
    "    test = [data[i] for i in test_indices]\n",
    "    return (train, test)\n",
    "\n",
    "\n",
    "def get_train_test_data():\n",
    "\n",
    "    #get ids of positive and negative movie reviews\n",
    "    pos_review_ids=movie_reviews.fileids('pos')\n",
    "    neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "    #split positive and negative data into training and testing sets\n",
    "    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
    "    #add labels to the data and concatenate\n",
    "    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
    "    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n",
    "\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N3LWwBYICPP"
   },
   "source": [
    "When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HJLegkdPFUJA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of training data is 1400\n",
      "The amount of testing data is 600\n",
      "The representation of a single data item is below\n",
      "(['i', 'want', 'to', 'correct', 'what', 'i', 'wrote', ...], 'pos')\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "random.seed(candidateno)\n",
    "training_data,testing_data=get_train_test_data()\n",
    "print(\"The amount of training data is {}\".format(len(training_data)))\n",
    "print(\"The amount of testing data is {}\".format(len(testing_data)))\n",
    "print(\"The representation of a single data item is below\")\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Re-Useable Code and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(token_list: list, stop_words: list):\n",
    "    \"\"\"\n",
    "    Applies case normalization, removes numbers and punctuation, \n",
    "    and removes stopwords from a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        token_list (list): The input list of tokens (strings).\n",
    "        stop_words (set/list): A collection of words to be removed (stopwords).\n",
    "\n",
    "    Returns:\n",
    "        list: The preprocessed list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_list = [\n",
    "        token.lower()\n",
    "        for token in token_list  \n",
    "        if token.isalpha()        # removes numbers and punctuation\n",
    "        and token.lower() not in stop_words  # remove tokens in stop_words (by keeping those not in stop_words)\n",
    "    ]\n",
    "    \n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generating Positive and Negative Word Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbTq6eGv2XT2"
   },
   "source": [
    "a) **Generate** a list of 10 content words which are representative of the positive reviews in your training data.\n",
    "\n",
    "b) **Generate** a list of 10 content words which are representative of the negative reviews in your training data.\n",
    "\n",
    "c) **Explain** what you have done and why\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code (a & b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "gvFu36xZ2XT5"
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "pos_freq_dist=FreqDist()\n",
    "neg_freq_dist=FreqDist()\n",
    "\n",
    "for rev,label in training_data:\n",
    "\n",
    "    # PREPROCESSING: case, number, punctuation, stopword\n",
    "    rev = preprocess_text(token_list=rev, stop_words=stop)\n",
    "\n",
    "    # FreqDist() data structure\n",
    "    if label == 'pos':\n",
    "        pos_freq_dist.update(rev) \n",
    "    elif label == 'neg':\n",
    "        neg_freq_dist.update(rev)\n",
    "    else: \n",
    "        print(\"unexpected class\")\n",
    "\n",
    "# IDENTIFY WORD DIFFERENTIAL BETWEEN CLASSSES\n",
    "all_words = set(pos_freq_dist.keys()) | set(neg_freq_dist.keys()) # all unique words\n",
    "\n",
    "word_counts = []\n",
    "\n",
    "for word in all_words:\n",
    "    pos_count = pos_freq_dist.get(word, 0)\n",
    "    neg_count = neg_freq_dist.get(word, 0)\n",
    "    difference = pos_count - neg_count\n",
    "    total = pos_count + neg_count\n",
    "    word_counts.append((word, difference, total))\n",
    "\n",
    "# IDENTIFY REVIEW DOMAIN STOP WORDS\n",
    "word_counts_copy = list(word_counts)\n",
    "word_counts_copy.sort(key=lambda item: item[2], reverse=True) # [2] = Total Corpus Count\n",
    "_domainStopWords = word_counts_copy[:5] # 5 most common review words\n",
    "domainStopWords = [word for word, diff, total in _domainStopWords]\n",
    "domain_stop_set = set(domainStopWords)\n",
    "\n",
    "# DROP DOMAIN WORDS FROM LIST\n",
    "final_word_counts = [word for word in word_counts if word[0] not in domain_stop_set]\n",
    "\n",
    "# SORY BY [1], THE CLASS/LABEL DIFFERENTIAL\n",
    "final_word_counts.sort(key=lambda item: item[1]) # low to high\n",
    "\n",
    "# COLLECT WORDS WITH THE LARGEST DIFFERENTIAL\n",
    "_negative_word_list = final_word_counts[:10] # first 10\n",
    "_positive_word_list = final_word_counts[-10:] # last 10\n",
    "\n",
    "_positive_word_list.sort(key=lambda item: item[1], reverse=True) # high to low\n",
    "\n",
    "negative_word_list = [word for word, diff, total in _negative_word_list]\n",
    "positive_word_list = [word for word, diff, total in _positive_word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movie', 'one', 'like', 'even']\n"
     ]
    }
   ],
   "source": [
    "print(domainStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JauTzY5N2XUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'plot', 'nothing', 'worst', 'script', 'stupid', 'boring', 'least', 'harry', 'supposed']\n"
     ]
    }
   ],
   "source": [
    "print(negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad', -476, 998), ('plot', -244, 1040), ('nothing', -153, 555), ('worst', -139, 205), ('script', -127, 533), ('stupid', -114, 174), ('boring', -113, 195), ('least', -107, 447), ('harry', -105, 163), ('supposed', -105, 223)]\n"
     ]
    }
   ],
   "source": [
    "print(_negative_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'also', 'great', 'well', 'best', 'story', 'many', 'world', 'love', 'first']\n"
     ]
    }
   ],
   "source": [
    "print(positive_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('life', 329, 1113), ('also', 327, 1395), ('great', 263, 833), ('well', 253, 1315), ('best', 245, 931), ('story', 229, 1555), ('many', 203, 901), ('world', 201, 729), ('love', 200, 800), ('first', 166, 1318)]\n"
     ]
    }
   ],
   "source": [
    "print(_positive_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explanation (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code I am looping through each review in the training data whilst giving myself access to the wordlist and the label. \n",
    "\n",
    "On the all word lists I preprocess and normalise accounting for case, numbers, punctuation and stopwords. I am doing this to remove noise and get access to the words in their cleanest and more contextualised form and removing words devoid of meaningful context. I did not implement any stemming or lemmanisation as the goal is to obtain a word list, hence, readabliltiy is desired. \n",
    "\n",
    "After which I push the wordlists into pre-initalised world lists for each label. It is important to have seperate DistFreqs for the label because I want to compare the frequency between the two. \n",
    "\n",
    "For each word I calculate two metrics. The differential between the positive and negative counts, as well as, the cumulative word count for the corupus.\n",
    "\n",
    "Words that come out with a positive differential are candiates for the positive world list, and negative ones the negative word list.\n",
    "\n",
    "The cumulative word count is used to identify words that appear to be domain stop words. That is, words that appear to be disproporionately used in context of writing reviews. The frequency of these words in the domain of reviewing means they loose the context they might otherwise hold in general terms. I remove these words as candidates for the word list.\n",
    "\n",
    "Finally, the word list is sorting by the index [1] which is the word differential metric. Slices of the top and bottom 10 are taken which are the words that are most skewed towards being in positive or negative reviews, hence, should be highly representative of sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word List Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TApOQE6vND20"
   },
   "source": [
    "a) **Use** the lists generated in Q1 to build a **word list classifier** which will classify reviews as being positive or negative.\n",
    "\n",
    "b) **Explain** what you have done.\n",
    "\n",
    "[12.5\\%]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_weight_converter(sentiment_word_list: list):\n",
    "    \"\"\"\n",
    "    Takes a sentiment word list with frequency counts converts it into a Sentiment Polarity weightings.  \n",
    "\n",
    "    Args:\n",
    "        sentiment_word_list (Iterable): Nested word list comprised of word:string, differential: int and total:int. \n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary of word and its sentiment weighting\n",
    "    \"\"\"\n",
    "    sentiment_weights = {\n",
    "        word: differential / total_usage\n",
    "        for word, differential, total_usage in sentiment_word_list\n",
    "    }\n",
    "\n",
    "    return sentiment_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.29559748427672955, 'also': 0.23440860215053763, 'great': 0.3157262905162065, 'well': 0.19239543726235742, 'best': 0.2631578947368421, 'story': 0.1472668810289389, 'many': 0.2253052164261931, 'world': 0.2757201646090535, 'love': 0.25, 'first': 0.125948406676783}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_weight_converter(_positive_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad': -0.47695390781563124, 'plot': -0.23461538461538461, 'nothing': -0.2756756756756757, 'worst': -0.6780487804878049, 'script': -0.23827392120075047, 'stupid': -0.6551724137931034, 'boring': -0.5794871794871795, 'least': -0.23937360178970918, 'harry': -0.6441717791411042, 'supposed': -0.47085201793721976}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_weight_converter(_negative_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "C6vK5Vyz2XUF"
   },
   "outputs": [],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "\n",
    "class ReviewClassifer(ClassifierI):\n",
    "\n",
    "    def __init__(self, pos, neg):\n",
    "        self._pos = sentiment_weight_converter(pos)\n",
    "        self._neg = sentiment_weight_converter(neg)\n",
    "\n",
    "    def classify(self, words):\n",
    "        score = 0\n",
    "\n",
    "        for word in words:\n",
    "            \n",
    "            if word in self._pos:\n",
    "                score += self._pos[word]\n",
    "\n",
    "            elif word in self._neg:\n",
    "                score += self._neg[word]\n",
    "\n",
    "        return \"neg\" if score <= 0 else \"pos\"\n",
    "\n",
    "    def labels(self):\n",
    "        return (\"pos\", \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos pos\n",
      "pos neg\n",
      "neg pos\n",
      "neg neg\n"
     ]
    }
   ],
   "source": [
    "#Example usage:\n",
    "\n",
    "classifier = ReviewClassifer(_positive_word_list, _negative_word_list)\n",
    "\n",
    "data = [\n",
    "    training_data[100], \n",
    "    training_data[500],\n",
    "    training_data[700],\n",
    "    training_data[1100]\n",
    "]\n",
    "\n",
    "for rev,label in data:\n",
    "    rev = preprocess_text(token_list=rev, stop_words=stop)\n",
    "\n",
    "    cls = classifier.classify(rev)\n",
    "\n",
    "    print(label, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have set up a list classifer class by inheriting from `ClassifierI` in the `nltk` package. \n",
    "\n",
    "This has been done to for standardization purpose to follow convention and for customization later in the assignment where I can easily knit `nltks` eval packages.\n",
    "\n",
    "Prior to writting the class I set up a function called `sentiment_weight_converter`. This function allows me to take my sentiment word list (pos or neg) and calculate a weighting for each word. The idea is to capture which words are more positive or negative.\n",
    "\n",
    "I had previously calculated the differential for each word to determine the most positive or negative words, as well as, counting the overall total frequency for each word. By taking a ratio of each per word, the score its weighted by how much the sentiment skew covers the total usage of a work\n",
    "\n",
    "- If a word is used 100 times and all seen instances are negative then it has a weighting of 1\n",
    "- If a word is used 100 times but only 80 instances are negative and 20 positive it has a differential of 60 and a weighting of 60/100 = 0.2. It is still considered a negative word but less so.\n",
    "\n",
    "$$\\text{Weighting Score} = \\frac{D}{T}$$\n",
    "\n",
    "Hopefully this approach will lead to an improve in some evaluation scores. Additionally, positive words overall seem to appear with more frequences but the negative weights appear to come out stronger so this may even find out and make classifcation better.\n",
    "\n",
    "Within the class, the `classify` method handles the computation. Looping through a review, if it sees a word in either sentiment list then the words correponding **weight** is added the `score` tally which is initalized as 0. Note, by definition negative words will have negative weights and visa versa for positive words, meaning each will pull the `score` in either direction. Finally after the loop has finished the method returns 'neg' if the score <= 0 or positive if  >0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZdDO_Y92XUH"
   },
   "source": [
    "a) **Calculate** the accuracy, precision, recall and F1 score of your classifier.\n",
    "\n",
    "b) Is it reasonable to evaluate the classifier in terms of its accuracy?  **Explain** your answer and give a counter-example (a scenario where it would / would not be reasonable to evaluate the classifier in terms of its accuracy).\n",
    "\n",
    "[20\\%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LQc8bsA2XUI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_i80ceP2XUJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbYwwhcs2XUL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVZp0N5J2XUL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIS9UpmJNEAp"
   },
   "source": [
    "a)  **Construct** a Naive Bayes classifier (e.g., from NLTK).\n",
    "\n",
    "b)  **Compare** the performance of your word list classifier with the Naive Bayes classifier.  **Discuss** your results.\n",
    "\n",
    "[12.5\\%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gwjig-Y12XUN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AUsYRMa2XUN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bytPkuHf2XUO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGDXaVDqOSfY"
   },
   "source": [
    "a) Design and **carry out an experiment** into the impact of the **length of the wordlists** on the wordlist classifier.  Make sure you **describe** design decisions in your experiment, include a **graph** of your results and **discuss** your conclusions.\n",
    "\n",
    "b) Would you **recommend** a wordlist classifier or a Naive Bayes classifier for future work in this area?  **Justify** your answer.\n",
    "\n",
    "[25\\%]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlxoUthX2XUP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1L7mZ-k2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFeOWIRm2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VT82P88M2XUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym-TGvYS2XUR"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
