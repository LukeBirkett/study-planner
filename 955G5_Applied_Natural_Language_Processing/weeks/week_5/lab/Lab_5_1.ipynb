{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2ZYtrASijE_"
   },
   "source": [
    "# Week 5: Document Similarity\n",
    "\n",
    "In some applications, it may be difficult to define the classes that we want to use in classification ahead of time.  Or, classes might be made up various subclasses (which differ in terms of the vocabulary used).  In both of these cases (and others), it might be more appropriate to think about **document similarity**.  For a new document, can we find the most similar document in our collection?\n",
    "\n",
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjmEqk0WinXj"
   },
   "outputs": [],
   "source": [
    "###uncomment if working on colab\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "06fwJf9ci2tq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDpwcq7UijFF"
   },
   "source": [
    "Now lets get a document collection.  We are going to use the Gutenberg collection of books.  We will get the tokenised content of each book and store it in a dictionary (key = the fileid of the book) for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P1DgvRiKU8nV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "book_ids=gutenberg.fileids()\n",
    "books={b:gutenberg.words(b) for b in book_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ol3UqQxHU8nW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[book_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6trbihIijFK"
   },
   "source": [
    "We now need to normalise the tokens in the documents and construct a *bag-of-words* document representation.  Combining some of the functionality we have been working over the past few weeks (which we have imported from utils.py), we could use something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-7cR-gxUijFP"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def normalise(wordlist):\n",
    "    lowered=[word.lower() for word in wordlist]\n",
    "    filtered=[word for word in lowered if word.isalpha() and word not in stop]\n",
    "    return filtered\n",
    "\n",
    "book_reps={key:FreqDist(normalise(book)) for key,book in books.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75oZH7ozk4Wv"
   },
   "source": [
    "Let's have a look at the representation of first book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lTfwqLWPijFR"
   },
   "outputs": [],
   "source": [
    "# print(book_reps[book_ids[0]].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sokt9hE9ijFU"
   },
   "source": [
    "## Measuring Similarity\n",
    "We are going to use the cosine measure to determine how similar two books are.  This can be defined in terms of the dot products of vectors:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mbox{sim}_{\\mbox{cosine}}(A,B) = \\frac{A.B}{\\sqrt{A.A \\times B.B}}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where the dot product of two vectors, A and B, is defined as:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "A.B = \\sum_{\\mbox{f}} \\mbox{weight}(A,f)\\times \\mbox{weight}(B,f) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "and $\\mbox{weight}(X,f)$ tells us the value associated with feature $f$ in the vector representation of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the reasons that Cosine Similarity is so populat and useful is because it is scale/magnitue invariant. This means that the similarity is based only on the angle, not the length\n",
    "- A longer document will naturually have higher counts which will lead to a large vector length (magnitude)\n",
    "- Thematically related documents will show up as different due to lengths, not content\n",
    "- Cosine similarity divides the dot product by the product of the magnitudes (L2 Normalization), effectively normalizing the vectors to a unit length. This makes the score independent of the document size.\n",
    "- Measures like Euclidean distance will begin to struggle as high dimensions and struggle from Curse of Dimensionality where the distances between all points tend to become large and approximately equal, making the metric less discriminatory.\n",
    "- Cosine Similarity is easily interpretable:\n",
    "    - **1 = perfect allignment, vectors point in same direction;**\n",
    "    - **0 = orthogonal, no relationship;**\n",
    "    - **-1 = Perfect opposition (vectors point in diametrically opposite directions)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "* Write a function `dot` which takes two documents (represented as dictionaries or `FreqDist`s) and returns their dot product\n",
    "* Test it out on the first two books in Gutenberg.  You should get the answer 3882298!\n",
    "* Why is the number so large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5tCel2Q8ijFV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of 'austen-emma.txt' and 'austen-persuasion.txt': 3882298\n"
     ]
    }
   ],
   "source": [
    "# Get the first two books\n",
    "book1_id = book_ids[0]  # austen-emma.txt\n",
    "book2_id = book_ids[1]  # austen-persuasion.txt\n",
    "\n",
    "doc1 = book_reps[book1_id]\n",
    "doc2 = book_reps[book2_id]\n",
    "\n",
    "def dot(doc_a, doc_b):\n",
    "    \"\"\"\n",
    "    Calculates the dot product of two document frequency representations.\n",
    "    \n",
    "    Args:\n",
    "        doc_a (dict or FreqDist): Frequency distribution of the first document.\n",
    "        doc_b (dict or FreqDist): Frequency distribution of the second document.\n",
    "        \n",
    "    Returns:\n",
    "        int: The dot product.\n",
    "    \"\"\"\n",
    "    dot_product = 0\n",
    "    \n",
    "    # Set removes 0 counts etc. Minimises set size\n",
    "    keys_a = set(doc_a.keys())\n",
    "    keys_b = set(doc_b.keys())\n",
    "    \n",
    "    # Intersection where words exist in both\n",
    "    common_keys = keys_a.intersection(keys_b)\n",
    "    \n",
    "    # Calculate the sum of the products of common word frequencies\n",
    "    for word in common_keys:\n",
    "        # FreqDist/dict lookup returns the count (frequency)\n",
    "        dot_product += doc_a[word] * doc_b[word]\n",
    "        \n",
    "    return dot_product\n",
    "\n",
    "# Test the function\n",
    "result = dot(doc1, doc2)\n",
    "print(f\"Dot product of '{book1_id}' and '{book2_id}': {result}\") \n",
    "# Result should be 3882298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dictionaries are implicitly representing sparse vectors.\n",
    "- A sparse vector is a vector in which most of the elements are zero.\n",
    "- Dense vectors are things link lists and sets.\n",
    "- You loop only over the common words found via set intersection. This is highly efficient since you skip all the zero-count dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ER7pPHzOijFY"
   },
   "source": [
    "### Exercise 1.2\n",
    "* Write a function `cos_sim` which takes two documents (represented as dictionaries or `FreqDist`s) and returns their cosine similarity.\n",
    "* Your function should make 3 calls to the `dot` function you have already defined\n",
    "* If you test it out on the first two documents in the finance collection you should get 0.72 (to 2S.F.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "n_pDnxOfijFZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Cosine Similarity ---\n",
      "Document 1: test/14826\n",
      "Document 2: test/14828\n",
      "-----------------------------------\n",
      "Dot Product (A . B): 635\n",
      "Magnitude ||A||: 72.77\n",
      "Magnitude ||B||: 15.46\n",
      "-----------------------------------\n",
      "Cosine Similarity: 0.5644\n",
      "Target Check (0.72): 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('reuters')\n",
    "\n",
    "def dot(doc_a, doc_b):\n",
    "    \"\"\"\n",
    "    Calculates the dot product of two document frequency representations \n",
    "    (sparse vectors represented as FreqDists or dictionaries).\n",
    "    \n",
    "    Args:\n",
    "        doc_a (dict or FreqDist): Frequency distribution of the first document.\n",
    "        doc_b (dict or FreqDist): Frequency distribution of the second document.\n",
    "        \n",
    "    Returns:\n",
    "        int: The dot product.\n",
    "    \"\"\"\n",
    "    dot_product = 0\n",
    "    \n",
    "    # Efficiently find the intersection of words present in both documents\n",
    "    keys_a = set(doc_a.keys())\n",
    "    keys_b = set(doc_b.keys())\n",
    "    common_keys = keys_a.intersection(keys_b)\n",
    "    \n",
    "    # Calculate the sum of the products of common word frequencies\n",
    "    for word in common_keys:\n",
    "        dot_product += doc_a[word] * doc_b[word]\n",
    "        \n",
    "    return dot_product\n",
    "\n",
    "def cos_sim(doc_a, doc_b):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two documents (FreqDists).\n",
    "    \n",
    "    Cosine Similarity = (A . B) / (||A|| * ||B||)\n",
    "    \n",
    "    Args:\n",
    "        doc_a (FreqDist): Frequency distribution of the first document.\n",
    "        doc_b (FreqDist): Frequency distribution of the second document.\n",
    "        \n",
    "    Returns:\n",
    "        float: The cosine similarity score (between 0 and 1 for positive counts).\n",
    "    \"\"\"\n",
    "    # 1. Calculate the dot product (A . B)\n",
    "    numerator = dot(doc_a, doc_b)\n",
    "    \n",
    "    # Handle the edge case where there is no overlap at all (division by zero is imminent)\n",
    "    if numerator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 2. Calculate the magnitude of A (||A|| = sqrt(A . A))\n",
    "    # We use dot(A, A) to calculate the sum of squared frequencies\n",
    "    mag_a = math.sqrt(dot(doc_a, doc_a))\n",
    "    \n",
    "    # 3. Calculate the magnitude of B (||B|| = sqrt(B . B))\n",
    "    mag_b = math.sqrt(dot(doc_b, doc_b))\n",
    "    \n",
    "    # Denominator is ||A|| * ||B||\n",
    "    denominator = mag_a * mag_b\n",
    "    \n",
    "    # Final calculation\n",
    "    return numerator / denominator\n",
    "\n",
    "# --- Test Case Setup (Reuters Corpus) ---\n",
    "\n",
    "# Get the file IDs for the 'finance' category\n",
    "# finance_files = reuters.fileids('finance')\n",
    "finance_files = reuters.fileids()\n",
    "\n",
    "# Select the first two documents\n",
    "file1_id = finance_files[0]\n",
    "file2_id = finance_files[1]\n",
    "\n",
    "# Function to preprocess and create FreqDist\n",
    "def create_doc_rep(file_id):\n",
    "    # Get words, convert to lowercase, and filter out punctuation/single characters\n",
    "    words = [w.lower() for w in reuters.words(file_id) if w.isalnum() and len(w) > 1]\n",
    "    return FreqDist(words)\n",
    "\n",
    "# Create the document representations (FreqDists)\n",
    "doc1_rep = create_doc_rep(file1_id)\n",
    "doc2_rep = create_doc_rep(file2_id)\n",
    "\n",
    "print(f\"--- Testing Cosine Similarity ---\")\n",
    "print(f\"Document 1: {file1_id}\")\n",
    "print(f\"Document 2: {file2_id}\")\n",
    "\n",
    "# Calculate and print the result\n",
    "similarity_score = cos_sim(doc1_rep, doc2_rep)\n",
    "\n",
    "print(\"-\" * 35)\n",
    "print(f\"Dot Product (A . B): {dot(doc1_rep, doc2_rep)}\")\n",
    "print(f\"Magnitude ||A||: {math.sqrt(dot(doc1_rep, doc1_rep)):.2f}\")\n",
    "print(f\"Magnitude ||B||: {math.sqrt(dot(doc2_rep, doc2_rep)):.2f}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"Cosine Similarity: {similarity_score:.4f}\")\n",
    "print(f\"Target Check (0.72): {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The reason that $\\sqrt{\\mathbf{A} \\cdot \\mathbf{A}}$ gives you the magnitude (or length) of vector $\\mathbf{A}$ is due to the Pythagorean theorem and the definition of the dot product.\n",
    "- In basic geometry, the length of a vector $\\mathbf{A}$ in 2D space ($\\mathbf{A} = [A_1, A_2]$) is found using the Pythagorean theorem: $\\text{Length} = \\sqrt{A_1^2 + A_2^2}$\n",
    "- This extends to any dimensions: $\\text{Magnitude } ||\\mathbf{A}|| = \\sqrt{\\sum_{i=1}^{n} A_i^2}$\n",
    "- Dot product is a shortcut to squared: $\\mathbf{A} \\cdot \\mathbf{A} = \\sum_{i=1}^{n} A_i \\times A_i$\n",
    "- To find the magnitude ($||\\mathbf{A}||$) itself, you just take the square root of both sides: $||\\mathbf{A}|| = \\sqrt{\\mathbf{A} \\cdot \\mathbf{A}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_2viT-oijFf"
   },
   "source": [
    "### Exercise 1.3\n",
    "* Write some code that will compute the similarity of every document in a collection with every document in another collection\n",
    "* Write code to compute the average similarity of two collections\n",
    "* Compute (and display) the average similarity of the book collection to itself\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "REy6s_38ijFf"
   },
   "outputs": [],
   "source": [
    "# read in corpus\n",
    "# create vectors of file_ids\n",
    "# place into matrix\n",
    "# create a functions to calculate the dot products and cos sim\n",
    "# compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two doc similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exercise 1.3, Part 1: Pairwise Similarity Matrix ---\n",
      "Comparing 5 Gutenberg documents (Rows) against 5 Reuters documents (Columns).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('austen-emma.txt', 'test/14826'): 0.6990508219606616,\n",
       " ('austen-emma.txt', 'test/14828'): 0.5210845796118742,\n",
       " ('austen-emma.txt', 'test/14829'): 0.5270836145598962,\n",
       " ('austen-emma.txt', 'test/14832'): 0.3566578418230858,\n",
       " ('austen-emma.txt', 'test/14833'): 0.4716770234364965,\n",
       " ('austen-persuasion.txt', 'test/14826'): 0.7352379490021591,\n",
       " ('austen-persuasion.txt', 'test/14828'): 0.5495933524288018,\n",
       " ('austen-persuasion.txt', 'test/14829'): 0.5740870233893807,\n",
       " ('austen-persuasion.txt', 'test/14832'): 0.37840912635574064,\n",
       " ('austen-persuasion.txt', 'test/14833'): 0.4806626042634651,\n",
       " ('austen-sense.txt', 'test/14826'): 0.7192064821343386,\n",
       " ('austen-sense.txt', 'test/14828'): 0.5277445411458346,\n",
       " ('austen-sense.txt', 'test/14829'): 0.5510520810848243,\n",
       " ('austen-sense.txt', 'test/14832'): 0.37137623980338824,\n",
       " ('austen-sense.txt', 'test/14833'): 0.4879439180416038,\n",
       " ('bible-kjv.txt', 'test/14826'): 0.7345602743193906,\n",
       " ('bible-kjv.txt', 'test/14828'): 0.600705201547581,\n",
       " ('bible-kjv.txt', 'test/14829'): 0.64550400119102,\n",
       " ('bible-kjv.txt', 'test/14832'): 0.3616795616287925,\n",
       " ('bible-kjv.txt', 'test/14833'): 0.37971477125231134,\n",
       " ('blake-poems.txt', 'test/14826'): 0.7030708888575088,\n",
       " ('blake-poems.txt', 'test/14828'): 0.5617419141843581,\n",
       " ('blake-poems.txt', 'test/14829'): 0.6311294907597698,\n",
       " ('blake-poems.txt', 'test/14832'): 0.37373049166798206,\n",
       " ('blake-poems.txt', 'test/14833'): 0.3669775078302629}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import reuters, gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- NLTK Corpus Download Checks ---\n",
    "# (Omitted repeated download logic for brevity, assumed to be run previously)\n",
    "\n",
    "# --- Similarity Functions ---\n",
    "\n",
    "def dot(doc_a, doc_b):\n",
    "    \"\"\"Calculates the dot product of two document frequency representations.\"\"\"\n",
    "    dot_product = 0\n",
    "    keys_a = set(doc_a.keys())\n",
    "    keys_b = set(doc_b.keys())\n",
    "    common_keys = keys_a.intersection(keys_b)\n",
    "    \n",
    "    for word in common_keys:\n",
    "        # Use .get(word, 0) for robustness, though FreqDist handles missing keys gracefully\n",
    "        dot_product += doc_a.get(word, 0) * doc_b.get(word, 0)\n",
    "        \n",
    "    return dot_product\n",
    "\n",
    "def cos_sim(doc_a, doc_b):\n",
    "    \"\"\"Calculates the cosine similarity between two documents (FreqDists).\"\"\"\n",
    "    numerator = dot(doc_a, doc_b)\n",
    "    \n",
    "    if numerator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate magnitudes: ||A|| = sqrt(A . A)\n",
    "    mag_a = math.sqrt(dot(doc_a, doc_a))\n",
    "    mag_b = math.sqrt(dot(doc_b, doc_b))\n",
    "    \n",
    "    denominator = mag_a * mag_b\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "# --- Document Preprocessing Function ---\n",
    "\n",
    "def create_doc_rep(corpus_source, file_id):\n",
    "    \"\"\"Gets words from the specified corpus, preprocesses them, and creates a FreqDist.\"\"\"\n",
    "    corpus = gutenberg if corpus_source == 'gutenberg' else reuters\n",
    "    words = [w.lower() for w in corpus.words(file_id) if w.isalnum() and len(w) > 1] # TODO: Remove stop words\n",
    "    return FreqDist(words)\n",
    "\n",
    "# --- Collection Setup (Assuming downloads are complete) ---\n",
    "\n",
    "LIMIT = 5 # SAMPLE OF COLLECTIONS\n",
    "\n",
    "# Collection 1: Gutenberg Books\n",
    "gutenberg_files = gutenberg.fileids()[:LIMIT]\n",
    "gutenberg_collection = {\n",
    "    fid: create_doc_rep('gutenberg', fid) for fid in gutenberg_files # proccesses > freq dist > dictionary {filename:Bag-of-Words}\n",
    "}\n",
    "\n",
    "finance_files = reuters.fileids()[:LIMIT]\n",
    "reuters_collection = {\n",
    "    fid: create_doc_rep('reuters', fid) for fid in finance_files\n",
    "}\n",
    "\n",
    "# --- Exercise 1.3, Part 1: Compute All Pairwise Similarities ---\n",
    "\n",
    "print(\"\\n--- Exercise 1.3, Part 1: Pairwise Similarity Matrix ---\")\n",
    "print(f\"Comparing {len(gutenberg_collection)} Gutenberg documents (Rows) against {len(reuters_collection)} Reuters documents (Columns).\\n\")\n",
    "\n",
    "# Store results in a dictionary where key is the pair (Gutenberg ID, Reuters ID)\n",
    "similarity_matrix = {}\n",
    "gutenberg_ids = list(gutenberg_collection.keys())\n",
    "reuters_ids = list(reuters_collection.keys())\n",
    "\n",
    "# Outer loop: Iterate through Collection A (Gutenberg)\n",
    "for g_id in gutenberg_ids:\n",
    "    g_doc = gutenberg_collection[g_id]\n",
    "    \n",
    "    # Inner loop: Iterate through Collection B (Reuters)\n",
    "    for r_id in reuters_ids:\n",
    "        r_doc = reuters_collection[r_id]\n",
    "        \n",
    "        similarity = cos_sim(g_doc, r_doc)\n",
    "        similarity_matrix[(g_id, r_id)] = similarity \n",
    "        \n",
    "        # NOTE THIS IS A CONCEPTUAL MATRIX IN DICT FORM\n",
    "        # REFERENCED USING similarity_matrix[('austen-sense.txt', 'test/14826')]\n",
    "        # YOU CAN IMAGINE THAT THE FILE NAMES ARE THE ROW AND COL AT ANY INTERSECT\n",
    "        # IF YOU WANTED TO LOOP THROUGH THE DICT YOU WOULD UNPACK THE KEYS AND COMBINE\n",
    "        # for (g_id, r_id), similarity in similarity_matrix.items():\n",
    "        # ACCESS MIN MAX USING\n",
    "        # most_similar_pair, max_score = max(similarity_matrix.items(), key=lambda item: item[1])\n",
    "\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity (Python): 0.532387\n"
     ]
    }
   ],
   "source": [
    "all_similarities = similarity_matrix.values()\n",
    "total_sum = sum(all_similarities)\n",
    "count = len(all_similarities)\n",
    "\n",
    "average_similarity = total_sum / count\n",
    "\n",
    "print(f\"Average Similarity (Python): {average_similarity:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('austen-emma.txt', 'austen-emma.txt'): 1.0,\n",
       " ('austen-emma.txt', 'austen-persuasion.txt'): 0.972601491890471,\n",
       " ('austen-emma.txt', 'austen-sense.txt'): 0.9745607724805279,\n",
       " ('austen-emma.txt', 'bible-kjv.txt'): 0.7883510086647902,\n",
       " ('austen-emma.txt', 'blake-poems.txt'): 0.7731805641072257,\n",
       " ('austen-persuasion.txt', 'austen-emma.txt'): 0.972601491890471,\n",
       " ('austen-persuasion.txt', 'austen-persuasion.txt'): 1.0,\n",
       " ('austen-persuasion.txt', 'austen-sense.txt'): 0.9735350105647143,\n",
       " ('austen-persuasion.txt', 'bible-kjv.txt'): 0.8334059915176326,\n",
       " ('austen-persuasion.txt', 'blake-poems.txt'): 0.8148398416369143,\n",
       " ('austen-sense.txt', 'austen-emma.txt'): 0.9745607724805279,\n",
       " ('austen-sense.txt', 'austen-persuasion.txt'): 0.9735350105647143,\n",
       " ('austen-sense.txt', 'austen-sense.txt'): 0.9999999999999999,\n",
       " ('austen-sense.txt', 'bible-kjv.txt'): 0.7956832606338139,\n",
       " ('austen-sense.txt', 'blake-poems.txt'): 0.7821380724609821,\n",
       " ('bible-kjv.txt', 'austen-emma.txt'): 0.7883510086647902,\n",
       " ('bible-kjv.txt', 'austen-persuasion.txt'): 0.8334059915176326,\n",
       " ('bible-kjv.txt', 'austen-sense.txt'): 0.7956832606338139,\n",
       " ('bible-kjv.txt', 'bible-kjv.txt'): 1.0,\n",
       " ('bible-kjv.txt', 'blake-poems.txt'): 0.9258319346012152,\n",
       " ('blake-poems.txt', 'austen-emma.txt'): 0.7731805641072257,\n",
       " ('blake-poems.txt', 'austen-persuasion.txt'): 0.8148398416369143,\n",
       " ('blake-poems.txt', 'austen-sense.txt'): 0.7821380724609821,\n",
       " ('blake-poems.txt', 'bible-kjv.txt'): 0.9258319346012152,\n",
       " ('blake-poems.txt', 'blake-poems.txt'): 1.0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIMIT = 5 # SAMPLE OF COLLECTIONS\n",
    "\n",
    "# Collection 1: Gutenberg Books\n",
    "gutenberg_files = gutenberg.fileids()[:LIMIT]\n",
    "gutenberg_collection = {\n",
    "    fid: create_doc_rep('gutenberg', fid) for fid in gutenberg_files # proccesses > freq dist > dictionary {filename:Bag-of-Words}\n",
    "}\n",
    "\n",
    "similarity_matrix = {}\n",
    "gutenberg_ids = list(gutenberg_collection.keys())\n",
    "\n",
    "for g_id_1 in gutenberg_ids:\n",
    "    g_doc_1 = gutenberg_collection[g_id_1]\n",
    "    \n",
    "    for g_id_2 in gutenberg_ids:\n",
    "        g_doc_2 = gutenberg_collection[g_id_2]\n",
    "        \n",
    "        similarity = cos_sim(g_doc_1, g_doc_2)\n",
    "        similarity_matrix[(g_id_1, g_id_2)] = similarity \n",
    "    \n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- That is a profoundly important observation, and it highlights the key conceptual difference between sparse vector models (like Bag-of-Words) and dense vector models (like Word2Vec or BERT) in NLP!\n",
    "- Baseline only considered the intersectioned words\n",
    "- The measure only tells you how much the two documents overlap relative to their own content, not relative to a universal, fixed, conceptual space.\n",
    "- Your intuition points to the true limitation of this method: it captures lexical similarity (do they use the same words?) but completely misses semantic similarity (do they use different words to talk about the same idea?).\n",
    "- The cosine similarity will be low because it sees large vs. big and dog vs. hound as completely different, even though they mean the same thing.\n",
    "- This limitation is the primary motivation for the shift from sparse models to modern dense embedding models (like Word2Vec, GloVe, or BERT), where words are mapped to continuous, dense vectors that capture meaning, not just frequency.\n",
    "- However, for your exercise, the sparse dictionary method is the correct and expected implementation for calculating cosine similarity on simple Bag-of-Words document representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Turning a Conceptual Matrix (Dict, Sparse) into a normal matrix (vectorized, dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- moving from a sparse data representation (the dictionary) to one optimized for numerical computation and indexing (a NumPy array or Pandas DataFrame)\n",
    "- To convert your dictionary-based conceptual matrix into a dense, index-based matrix, you need to first establish the definitive order of your rows and columns.\n",
    "- The most efficient way to achieve this is using the Pandas library, which is built on top of NumPy and excels at handling labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dict -> Series (Pandas) -> Dataframe (Pandas) -> Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Vectorized Similarity Matrix (Pandas DataFrame) ---\n",
      "                     test/14826  test/14842  test/14845\n",
      "austen-sense.txt          0.534       0.701       0.622\n",
      "chesterton-ball.txt       0.311       0.450       0.389\n",
      "\n",
      "--- Accessing by Row/Column Name ---\n",
      "Similarity (austen-sense.txt vs test/14842): 0.7010\n",
      "\n",
      "--- Pure NumPy Array ---\n",
      "[[0.534 0.701 0.622]\n",
      " [0.311 0.45  0.389]]\n",
      "\n",
      "--- Accessing by Row/Column Index ---\n",
      "Similarity (Index [0, 2]): 0.6220\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming the 'similarity_matrix' dictionary from your previous code is available here.\n",
    "# For demonstration purposes, we'll create a small sample dictionary:\n",
    "\n",
    "similarity_matrix = {\n",
    "    ('austen-sense.txt', 'test/14826'): 0.534,\n",
    "    ('chesterton-ball.txt', 'test/14826'): 0.311,\n",
    "    ('austen-sense.txt', 'test/14842'): 0.701,\n",
    "    ('chesterton-ball.txt', 'test/14842'): 0.450,\n",
    "    ('austen-sense.txt', 'test/14845'): 0.622,\n",
    "    ('chesterton-ball.txt', 'test/14845'): 0.389\n",
    "}\n",
    "\n",
    "# --- Conversion to a Vectorized Matrix (Pandas DataFrame) ---\n",
    "\n",
    "# 1. Create a Series from the dictionary.\n",
    "# The keys (tuples) automatically become a MultiIndex (a hierarchical index).\n",
    "similarity_series = pd.Series(similarity_matrix)\n",
    "\n",
    "# 2. Convert the Series into a DataFrame using the .unstack() method.\n",
    "# The unstack() operation moves the inner level of the MultiIndex (Reuters ID)\n",
    "# to become the column headings, creating the final matrix structure.\n",
    "vectorized_matrix = similarity_series.unstack()\n",
    "\n",
    "print(\"--- Vectorized Similarity Matrix (Pandas DataFrame) ---\")\n",
    "print(vectorized_matrix)\n",
    "print(\"\\n--- Accessing by Row/Column Name ---\")\n",
    "print(f\"Similarity (austen-sense.txt vs test/14842): {vectorized_matrix.loc['austen-sense.txt', 'test/14842']:.4f}\")\n",
    "\n",
    "# --- Conversion to a Pure NumPy Array (for Index-Only Access) ---\n",
    "\n",
    "# If you only need the raw numbers without the row/column names:\n",
    "numpy_matrix = vectorized_matrix.values # values is pandas property/attribute turning df into numpy\n",
    "\n",
    "print(\"\\n--- Pure NumPy Array ---\")\n",
    "print(numpy_matrix)\n",
    "print(\"\\n--- Accessing by Row/Column Index ---\")\n",
    "# Accessing Row 0 (austen-sense.txt), Column 2 (test/14845)\n",
    "print(f\"Similarity (Index [0, 2]): {numpy_matrix[0, 2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dict -> Numpy Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dictionary to NumPy Matrix (Manual Mapping) ---\n",
      "\n",
      "Row Mapping (Key -> Index): {'austen-sense.txt': 0, 'chesterton-ball.txt': 1}\n",
      "Col Mapping (Key -> Index): {'test/14826': 0, 'test/14842': 1, 'test/14845': 2}\n",
      "\n",
      "Resulting NumPy Matrix:\n",
      "[[0.534 0.701 0.622]\n",
      " [0.311 0.45  0.389]]\n",
      "\n",
      "--- Accessing by Index ---\n",
      "Similarity (Index [0, 2]): 0.6220\n",
      "Row 0 is: austen-sense.txt\n",
      "Col 2 is: test/14845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Example Similarity Matrix (Using the provided data) ---\n",
    "# NOTE: In your full solution, you would use the actual dictionary created\n",
    "# by your nested loops.\n",
    "similarity_matrix = {\n",
    "    ('austen-sense.txt', 'test/14826'): 0.534,\n",
    "    ('chesterton-ball.txt', 'test/14826'): 0.311,\n",
    "    ('austen-sense.txt', 'test/14842'): 0.701,\n",
    "    ('chesterton-ball.txt', 'test/14842'): 0.450,\n",
    "    ('austen-sense.txt', 'test/14845'): 0.622,\n",
    "    ('chesterton-ball.txt', 'test/14845'): 0.389\n",
    "}\n",
    "\n",
    "print(\"--- Dictionary to NumPy Matrix (Manual Mapping) ---\")\n",
    "\n",
    "# --- 2. Extract and Map Keys ---\n",
    "\n",
    "# Get all unique row keys (Gutenberg IDs) and sort them\n",
    "row_keys = sorted(list(set(k[0] for k in similarity_matrix.keys())))\n",
    "\n",
    "# Get all unique column keys (Reuters IDs) and sort them\n",
    "col_keys = sorted(list(set(k[1] for k in similarity_matrix.keys())))\n",
    "\n",
    "# Create mapping dictionaries: String Key -> Integer Index\n",
    "row_map = {key: i for i, key in enumerate(row_keys)}\n",
    "col_map = {key: i for i, key in enumerate(col_keys)}\n",
    "\n",
    "# --- 3. Initialize NumPy Array ---\n",
    "\n",
    "rows = len(row_keys)\n",
    "cols = len(col_keys)\n",
    "\n",
    "# Create a NumPy array of the correct size, initialized to zeros\n",
    "numpy_matrix = np.zeros((rows, cols))\n",
    "\n",
    "# --- 4. Populate the Array ---\n",
    "\n",
    "# Loop through the similarity dictionary\n",
    "for (r_key, c_key), sim_score in similarity_matrix.items():\n",
    "    # Look up the integer indices using our maps\n",
    "    r_idx = row_map[r_key] # get the id from the map using the filename key from the original matrix\n",
    "    c_idx = col_map[c_key]\n",
    "    \n",
    "    # Assign the similarity score to the correct (row, column) position\n",
    "    numpy_matrix[r_idx, c_idx] = sim_score\n",
    "\n",
    "# --- Display Results ---\n",
    "\n",
    "print(f\"\\nRow Mapping (Key -> Index): {row_map}\")\n",
    "print(f\"Col Mapping (Key -> Index): {col_map}\")\n",
    "\n",
    "print(\"\\nResulting NumPy Matrix:\")\n",
    "print(numpy_matrix)\n",
    "print(\"\\n--- Accessing by Index ---\")\n",
    "# Example: Row 0 (austen-sense.txt), Col 2 (test/14845)\n",
    "print(f\"Similarity (Index [0, 2]): {numpy_matrix[0, 2]:.4f}\")\n",
    "\n",
    "# You would also keep the row_keys and col_keys lists if you needed\n",
    "# to reference the names later:\n",
    "print(f\"Row 0 is: {row_keys[0]}\")\n",
    "print(f\"Col 2 is: {col_keys[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpnSYEMlijFi"
   },
   "source": [
    "## Beyond Frequency\n",
    "Frequency of a word in a document does not make a very good weight because some words occur very frequently in all documents.  If two rare words occur in both of our pair of documents, that should add more to their perceived similarity than if two common words occur in both of our pair of documents.\n",
    "\n",
    "### TF-IDF\n",
    "A commonly used weight is tf-idf which stands for **term frequency, inverse document frequency**\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mbox{tf-idf}(D_i,f) = tf(D_i,f) \\times idf(D_i,f)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $tf(D_i,f)$ is simply the frequency of feature f in document $D_i$\n",
    "and\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "idf(D_i,f) = log \\frac{N}{df(f)}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $N$ is the total number of documents and $\\mbox{df}(f)$ is the number of documents containing $f$:  \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "df(f)=|\\{i|\\mbox{freq}(D_i,f)>0\\}|\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The code below will take a list of documents (represented as dictionaries) and compute the document frequency for each feature.  Test it out on one the collection of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dWqctt_ijFj"
   },
   "outputs": [],
   "source": [
    "def doc_freq(doclist):\n",
    "    df={}\n",
    "    for doc in doclist:\n",
    "        for feat in doc.keys():\n",
    "            df[feat]=df.get(feat,0)+1\n",
    "            \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lL19Sg8lTEf"
   },
   "outputs": [],
   "source": [
    "doc_freq(book_reps.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b78F6_p2ijFm"
   },
   "source": [
    "### Exercise 2.1\n",
    "* Write a function which will compute the idf values for features given a list of documents\n",
    "* Use it to compute idf values for features given the entire list of books in the book collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWo0NQ5SijFm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiFxAmK1ijFp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOL89cF-ijFs"
   },
   "source": [
    "### Exercise 2.2\n",
    "* Write a function `convert_to_tfidf` that takes two arguments:\n",
    "    * a dictionary of documents mapping fileids to documents\n",
    "        * where each document is represented as a dictionary or FreqDist {feat:freq})\n",
    "    * a dictionary containing idf values\n",
    "* and outputs a dictionary of documents where each document is represented as a dictionary or FreqDist with tfidf weights {feat:tfidf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khxO_GxLijFv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfQIkEK4ijFy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbxJQxEQijF1"
   },
   "source": [
    "### Exercise 2.3\n",
    "* Recompute the average similarity between the collection of books (as in Ex 1.3).\n",
    "* What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yyc36RIeijF1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffcgIYBhijF4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI0J_G2rU8nl"
   },
   "source": [
    "### Exercise 2.4\n",
    "For each book in the collection, find it's most similar book (NOT INCLUDING ITSELF!).\n",
    "Output your results in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkiwGRAuU8nm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJeYsGqgU8nm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huEGHZ3sU8nm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
