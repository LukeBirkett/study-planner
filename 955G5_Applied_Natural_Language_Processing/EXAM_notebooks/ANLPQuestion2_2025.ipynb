{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVGC8vt4prsG"
      },
      "source": [
        "# Applied Natural Language Processing 955G5\n",
        "## Computer Based Examination, 2024\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVjTFWYKpvRS"
      },
      "outputs": [],
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 11111111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTDVYEeyZxPS"
      },
      "source": [
        "#Question 2 (50 marks)\n",
        "\n",
        "This questions is about document classification.\n",
        "\n",
        "You will construct a model using the sentences in the lists `spam` and `notspam` and apply the model to the sentence assigned to the variable `unlabelled`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tftq795GZuDb"
      },
      "outputs": [],
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "\n",
        "spam =[\"free laser hair treatment will improve your dating prospects\",\n",
        "       \"leverage this weird blockchain hack to generate free cash\",\n",
        "       \"new blockchain dating app needs single men\",\n",
        "       \"change your life prospects with a single liposuction treatment\",\n",
        "       \"one weird trick for free real estate\"]\n",
        "\n",
        "notspam=[\"can we reschedule for next week\",\n",
        "         \"i will be back in the office on monday \",\n",
        "         \"thanks for all your hard work this week\",\n",
        "         \"i like your proposal in principle but will need to see more detail\",\n",
        "         \"will you be free at 3 pm\"]\n",
        "\n",
        "unlabelled=\"blockchain proposal\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfslnjtbb12B"
      },
      "source": [
        "a)\n",
        "\n",
        "i) Use the NLTK word tokenizer to tokenize the sentences in the lists `spam` and `notspam` and produce two new lists called `spam_tokenized` and `notspam_tokenized`.\n",
        "\n",
        "For example, the list `[\"this is a sentence\", \"this is another\"]` would become `[[\"this\", \"is\", \"a\", \"sentence\"], [\"this\", \"is\", \"another\"]]`.\n",
        "\n",
        "[5 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t87u3PS2cNMh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "spam_tokenized = [word_tokenize(s_sent) for s_sent in spam]\n",
        "notspam_tokenized = [word_tokenize(ns_sent) for ns_sent in notspam]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['free',\n",
              "  'laser',\n",
              "  'hair',\n",
              "  'treatment',\n",
              "  'will',\n",
              "  'improve',\n",
              "  'your',\n",
              "  'dating',\n",
              "  'prospects'],\n",
              " ['leverage',\n",
              "  'this',\n",
              "  'weird',\n",
              "  'blockchain',\n",
              "  'hack',\n",
              "  'to',\n",
              "  'generate',\n",
              "  'free',\n",
              "  'cash']]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spam_tokenized[0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GroEkOZgcMYx"
      },
      "source": [
        "ii) Construct a training dataset from the lists of spam and not spam tokenized sentences, consisting of a list of two element tuples in which the first element contains the list of tokens and the second element is either `1` indicating spam or `0` indicating not spam.\n",
        "\n",
        "For example, the spam sentence `[\"free\", \"crypto\"]` would be represented in the training set as `([\"free\", \"crypto\"], 1)`\n",
        "\n",
        "[4 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YtqPrbVTeL0q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(['free', 'laser', 'hair', 'treatment', 'will', 'improve', 'your', 'dating', 'prospects'], 1), (['leverage', 'this', 'weird', 'blockchain', 'hack', 'to', 'generate', 'free', 'cash'], 1)]\n"
          ]
        }
      ],
      "source": [
        "spam_labeled = [(sentence, 1) for sentence in spam_tokenized]\n",
        "notspam_labeled = [(sentence, 0) for sentence in notspam_tokenized]\n",
        "training = spam_labeled + notspam_labeled\n",
        "print(training[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqyB8-sqejm6"
      },
      "source": [
        "iii) Convert the sentences in the training dataset to a bag-of-words representation using the FreqDist class. For example, the document `[\"this\", \"is\", \"a\", \"sentence\"]` would become `FreqDist({'this': 1, 'is': 1, 'a': 1, 'sentence': 1})`. The result should a be a list of pairs, in which one item is the bag-of-words sentence representation and the other is the `0`/`1` label.\n",
        "\n",
        "[6 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-9Kl_ItueoaU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[FreqDist({'free': 1, 'laser': 1, 'hair': 1, 'treatment': 1, 'will': 1, 'improve': 1, 'your': 1, 'dating': 1, 'prospects': 1}),\n",
              " 1]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from nltk import FreqDist\n",
        "BoW = [[FreqDist(sent),label] for sent, label in training]\n",
        "BoW[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKKuhOEDfTP3"
      },
      "source": [
        "b)\n",
        "\n",
        "i) From the training data, calculate the prior probabilities, $p(c)$, of a sentence being in each class, $c \\in \\{ 0 , 1 \\}$.\n",
        "$$ p(c) = \\frac{freq(c)}{N}$$\n",
        "Here, $freq(c)$ is the number of sentences with label $c$, and $N$ is the total number of documents.\n",
        "\n",
        "The result should be a list, called `class_priors`, with two entries corresponding to the two classes. The first item in the list, `class_priors[0]`, should correspond to the probability of a sentence not being spam and the last item, `class_priors[1]`, should correspond to the probability of a sentence being spam.  \n",
        "\n",
        "So, for example, if 90% of the sentences were not spam, then `class_priors` would take the value `[0.9, 0.1]`.\n",
        "\n",
        "[6 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YUkF5QwKgtEL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5, 0.5]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total = len(training)\n",
        "counter = 0\n",
        "for s,label in training:\n",
        "    counter += label\n",
        "\n",
        "class_priors = [\n",
        "    counter/total,\n",
        "    (total-counter)/total,\n",
        "]\n",
        "\n",
        "class_priors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYr71EFKhyba"
      },
      "source": [
        "ii) Calculate the conditional probabilities, $p(f|c)$, of each token feature, $f$, given each sentiment class, $c$.\n",
        "$$ p(f|c) = \\frac{freq(f,c)}{freq(c)}$$\n",
        "\n",
        "Where $freq(f,c)$ is the number of documents in class $c$ containing feature $f$ and $freq(c)$ is the total number of documents in class $c$.\n",
        "\n",
        "The result should be a dictionary of lists called `cond_probs`, with the keys of the outer dictionary being token features, and the corresponding values being a list of the two conditional class probabilities.\n",
        "\n",
        "For example, the token `'to'` occurs once in the five spam sentences and once in the five non-spam sentences. So, `cond_probs['to']` should contain the value `[0.2,0.2]`.\n",
        "\n",
        "[10 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1]\n",
            "[0.2, 0.2]\n"
          ]
        }
      ],
      "source": [
        "doc_spam_count = 0\n",
        "\n",
        "cond_probs = {}\n",
        "\n",
        "for sent,label in training:\n",
        "    doc_spam_count += label\n",
        "    for word in set(sent):\n",
        "        word_lst = cond_probs.get(word,[0,0])\n",
        "        word_lst[label] += 1\n",
        "        cond_probs[word] = word_lst\n",
        "\n",
        "print(cond_probs[\"to\"])\n",
        "\n",
        "doc_not_spam_count = len(training) - doc_spam_count\n",
        "\n",
        "for key in cond_probs.keys():\n",
        "    current = cond_probs[key] # direct reference\n",
        "    current[0] = 0 if current[0] == 0 else current[0]/doc_not_spam_count\n",
        "    current[1] = 0 if current[1] == 0 else current[1]/doc_spam_count\n",
        "\n",
        "print(cond_probs[\"to\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgU4yI5YqJka"
      },
      "source": [
        "iii) For each token in the sentence `unlabelled`, print out the conditional probabilities for that token. Calculate and print the final Naive Bayes probabilities of the two classes for the whole sentence.\n",
        "\n",
        "[6 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "Q4CI0NYrrBA5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conditonal Probabilities for 'blockchain' is [0, 0.4]\n",
            "Conditonal Probabilities for 'proposal' is [0.2, 0]\n",
            "Naive Nayes Probability for Class '0' is 0.0\n",
            "Naive Nayes Probability for Class '1' is 0\n"
          ]
        }
      ],
      "source": [
        "for token in word_tokenize(unlabelled):\n",
        "    print(f\"Conditonal Probabilities for '{token}' is {cond_probs[token]}\")\n",
        "\n",
        "for class_index in range(len(class_priors)):\n",
        "    calcuate = class_priors[class_index]\n",
        "    for token in word_tokenize(unlabelled):\n",
        "        calcuate = 0 if cond_probs[token][class_index]==0 else calcuate * cond_probs[token][class_index]\n",
        "    print(f\"Naive Nayes Probability for Class '{class_index}' is {calcuate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fSLG0dF2Cz0"
      },
      "source": [
        "c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4mTvxf9aIHv"
      },
      "source": [
        "i) Explain the ideas behind the Naive Bayes model.\n",
        "\n",
        "Starting from the assumption that we want to find the class that maximises  $p(class|document)$ , explain how Bayes theorem is used and what naive assumption is made about the features in the document. Describe the priors and conditional probabilities that are used to predict the most likely class for a document.\n",
        "\n",
        "[5 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRiuzYuRaYZX"
      },
      "source": [
        "In order to understand the idea behind the Naive Bayes model we first need to instantiate our problem. \n",
        "\n",
        "We have any number data instances which are documents. A document is a data structure which is made up of a sequence features with a given order (words, punctuation, numbers). We can respresent this an tuple of features. We use a tuple because the features are indexed in a given order so it retains the format of a sentence or paragraph:\n",
        "\n",
        "$$ f_1^2 = (f_1, f_2,...,f_n) $$\n",
        "\n",
        "We want to assign our documents a class, this is something that they belong, i.e. categorizing articles in a newspaper as \"football\", \"politics\" etc. With this, there exists a possible set of classes $C$. An individual class from within the set is denoted $c$ which can be denoted $c \\in C$. \n",
        "\n",
        "The goal of a \"model\" is to look at the features $f_1^2$ and assign a class based on the features. If the number of classes $C > 1$ then the selection of a class with be derived based on the class which is most likely, this is written $P(c|f_1^2)$\n",
        "\n",
        "Bayes Theorem provides us with a framework to executre this allocation. \n",
        "\n",
        "$$\\operatorname*{argmax}_{c} P(c|f_1^n) = \\operatorname*{argmax}_{c} \\frac{P(f_1^n|c) \\cdot P(c)}{P(f_1^n)}$$\n",
        "\n",
        "This equation is made up of 5 parts: \n",
        "- Prior\n",
        "- Conditional Probability/Likelihood\n",
        "- Evidence \n",
        "- Posterior Probability\n",
        "- Argmax\n",
        "\n",
        "The Prior $P(c)$ is the distribution of the class accross all instances. If there all 100 pieces of data, 50 are labelled X and 50 labelled Y then `Class X = 0.5` and `Class Y = 0.5`. In the structure of Bayes, it tells of the before information. Before we know anything about a peice of data, what is the global assignments of classes. \n",
        "\n",
        "The Conditional Probability/Likelihood $P(f_1^n|c)$ says, given we know the class of something, what is the max probability that is comes from some set of features. In Bayes it represents the update mechanism. Now that we have an actual data instance, we use it to improve and inform the prior distribution. \n",
        "\n",
        "The Evidence $P(f_1^2)$ is probability that a data instances in the set of all possible feature sets. It should be notes that this probability is the same for all feature sets, therefore, for Bayes, this is just a normalization term. It ensures that all final probabilties sum to one. However, in terms of ratios, it doesn't do anything. The highest result will still be the most likely even if the number computed isn't a real probability. As a result this term is often dropped for simplicity. \n",
        "\n",
        "On the left handside of the equation is the posterior $P(c|f_1^n)$. This is outcome of updating the prior with the conditional probability. It's tells us, given we have seen a particular feature set, what class is it most likely from. \n",
        "\n",
        "Finally, there is argmax. This tells from all of the possible feature sets, select the one that is most likely. \n",
        "\n",
        "[naive assumptions to simplify, avoid huge computations, curse of dims, tiny miniscule probs, sentence is a sparse vector of all possible sentences, likely 0 exact occur, joint probabilities that account for the specific sequence and context of every word, transfers data structure to bow instead of list/tuple, product of features]\n",
        "\n",
        "In its \"pure\" form, calculating the joint probability $P(f_1, f_2, ..., f_n|c)$ is computationally intractable due to the \"curse of dimensionality.\" To calculate the exact sequence probability, we would need to account for the context and dependencies of every word (the Chain Rule). In most datasets, specific long sequences are unique, leading to sparse vectors where the probability of an exact match is likely zero.\n",
        "\n",
        "To solve this, we make the Naive Independence Assumption: we assume that every feature $f_i$ is conditionally independent of every other feature given the class. This simplifies the likelihood into a product of individual probabilities:\n",
        "\n",
        "$$P(f_1, ..., f_n|c) \\approx P(f_1|c) \\times P(f_2|c) \\times ... \\times P(f_n|c)$$\n",
        "\n",
        "This transforms the data structure from a specific ordered sequence into a Bag-of-Words representation. While we lose word order and context, we gain a model that can effectively estimate probabilities from smaller training sets by looking at individual word frequencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wODZIWbS2pk_"
      },
      "source": [
        "ii) Should we apply smoothing to the data above? Justify your answer.\n",
        "\n",
        "[3 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJNS6sOUBlaF"
      },
      "source": [
        "Most NLP applications struggle with the concept of sparseness. There is a huge number of features (words) in a langauge to think of and outside the common connective words, most words by definition are use relatively sparsely in general. The specific issue we faced in our example was that the words in the test phrase \"blockchain proposal\" did not occur in both classes. Taking \"blockchain\" as an example, it occured in the `spam` class meaning the NB model computed a probability of 0.4 for it, however, it did not occur in the `not_spam` class meaning the probability here had to be 0. When computing the probability for a whole sentence, whether we are applying the naive assumption or not, we need to in some way take the product of all features in a sentence. This means a 0 probability feature will cause the probability for the sentence to be computed as 0. We can correct for this by using a smooth technique. Here, we adjust our computations so that by default, all words seen in the entire vocabularly, in this case both spam and not_spam words, are appended with an initalising count of 1. That means the `spam` blockchain count would move from 2 to 3 and the `not_spam` count would move from 0 to 1. This means that we go to compute the class posterior probabilties again, it will not some out as 0 as we will have an eligible result which can be used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTpEyWIr09fE"
      },
      "source": [
        "iii) Recalculate the conditional probabilities after applying add one smoothing.\n",
        "\n",
        "[3 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "68dQjZx61T33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1]\n",
            "[0.2857142857142857, 0.2857142857142857]\n"
          ]
        }
      ],
      "source": [
        "_doc_spam_count = 0\n",
        "\n",
        "sm_cond_probs = {}\n",
        "\n",
        "for _sent,_label in training:\n",
        "    _doc_spam_count += _label\n",
        "    for _word in set(_sent):\n",
        "        _word_lst = sm_cond_probs.get(_word,[0,0])\n",
        "        _word_lst[_label] += 1\n",
        "        sm_cond_probs[_word] = _word_lst\n",
        "\n",
        "print(sm_cond_probs[\"to\"])\n",
        "\n",
        "_doc_not_spam_count = len(training) - _doc_spam_count\n",
        "\n",
        "for _key in sm_cond_probs.keys():\n",
        "    _current = sm_cond_probs[_key] # direct reference\n",
        "    _current[0] = (_current[0]+1)/(_doc_not_spam_count + 2)\n",
        "    _current[1] = (_current[1]+1)/(_doc_spam_count + 2)\n",
        "\n",
        "print(sm_cond_probs[\"to\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vEIrnuC2KeW"
      },
      "source": [
        "iv) Make a Naive Bayes prediction for the `unlabelled` sentence using the smoothed probabilities.\n",
        "\n",
        "[2 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "PxHJ57un2kkc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conditonal Probabilities for 'blockchain' is [0.14285714285714285, 0.42857142857142855]\n",
            "Conditonal Probabilities for 'proposal' is [0.2857142857142857, 0.14285714285714285]\n",
            "Naive Nayes Probability for Class '0' is 0.02040816326530612\n",
            "Naive Nayes Probability for Class '1' is 0.03061224489795918\n"
          ]
        }
      ],
      "source": [
        "for token in word_tokenize(unlabelled):\n",
        "    print(f\"Conditonal Probabilities for '{token}' is {sm_cond_probs[token]}\")\n",
        "\n",
        "for class_index in range(len(class_priors)):\n",
        "    calcuate = class_priors[class_index]\n",
        "    for token in word_tokenize(unlabelled):\n",
        "        calcuate = 0 if sm_cond_probs[token][class_index]==0 else calcuate * sm_cond_probs[token][class_index]\n",
        "    print(f\"Naive Nayes Probability for Class '{class_index}' is {calcuate}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "generative_ai_disabled": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_exam",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
