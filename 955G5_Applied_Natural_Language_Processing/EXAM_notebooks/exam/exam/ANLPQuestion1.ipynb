{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M3r84BT5WeN"
      },
      "source": [
        "#Applied Natural Language Processing 955G5\n",
        "\n",
        "##Computer Based Examination, 2026\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibHDq-Im5Re4"
      },
      "outputs": [],
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 291065"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259FEqEb5xRr"
      },
      "source": [
        "#Question 1 (50 Marks)\n",
        "\n",
        "This question is about document similarity and information retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "89Wx4WPa5weU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from math import log2, sqrt\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"punkt_tab\")\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "\n",
        "\n",
        "# This is corpus of lyrics, artists and song titles which we will use in this question.\n",
        "corpus = [(\"Get up, stand up. Stand up for your right. Get up, stand up. Don't give up the fight.\", \"Bob Marley\", \"Get Up, Stand Up\"),\n",
        "          (\"Cause all around the gold is glistening. Making sure it keeps us hypnotized.\", \"John Martyn\", \"Don't Want to Know\"),\n",
        "          (\"You're told that you must. And you must earn a living. And you must earn a crust. And be like everybody else.\", \"Reverend and the Makers\", \"Heavyweight Champion of the World\"),\n",
        "          (\"Another year and then you'd be happy. Just one more year and then you'd be happy. But you're cryin', you're cryin' now.\", \"Gerry Rafferty\", \"Baker Street\"),\n",
        "          (\"You only live twice. Or so it seems. One life for yourself. And one for your dreams.\", \"Nancy Sinatra\", \"You Only Live Twice\"),\n",
        "          (\"For what is a man, what has he got? If not himself, then he has naught.\", \"Frank Sinatra\", \"My Way\")]\n",
        "\n",
        "# This is a query that we will retrieve relevant documents for.\n",
        "query = \"In life, what have we got to stand up for?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee37cbR8ebN9"
      },
      "source": [
        "a) Preprocess the information in the list `corpus` by following the steps below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGruck7ketTb"
      },
      "source": [
        "i) Create three dictionaries - `lyrics`, `artists` and `titles` - corresponding to the three elements in each tuple of `corpus`. Each key in these dictionaries should be the position in the original list and each value should be a string from the tuple at that position in the list.\n",
        "\n",
        "Each value in `lyrics` should be item 0 from a tuple, i.e. the lyric itself. Each value in `artists` should be item 1 from the tuple, i.e. the artist associated with the lyric. Each value in `titles` should be item 2 in a tuple, i.e. the song title of the lyric.\n",
        "\n",
        "So for example, the example corpus `[(\"Freedom\", \"Aretha\", \"Think\")]` would be broken into three dictionaries: `{0: \"Freedom\"}`, `{0: \"Aretha\"}` and `{0: \"Think\"}`.\n",
        "\n",
        "(7 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lf-jmySgFZAP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get up, stand up. Stand up for your right. Get up, stand up. Don't give up the fight.\n",
            "Bob Marley\n",
            "Get Up, Stand Up\n"
          ]
        }
      ],
      "source": [
        "lyrics, artists, titles = {}, {}, {}\n",
        "\n",
        "for index,sent in enumerate(corpus):\n",
        "    lyrics[index] = sent[0]\n",
        "    artists[index] = sent[1]\n",
        "    titles[index] = sent[2]\n",
        "\n",
        "print(lyrics[0])\n",
        "print(artists[0])\n",
        "print(titles[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HKbt1Npi6U_"
      },
      "source": [
        "ii) Tokenise the quotation strings in the dictionary `lyrics` to produce a new dictionary,  called `tokenised`, with the same keys, in which each value is a list of tokens.\n",
        "\n",
        "So, for example the example dictionary `{0: \"Choose life.\"}` would become `{0: [\"Choose\", \"life\", \".\"]}`.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VpJk-SrsGXSn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Get',\n",
              " 'up',\n",
              " ',',\n",
              " 'stand',\n",
              " 'up',\n",
              " '.',\n",
              " 'Stand',\n",
              " 'up',\n",
              " 'for',\n",
              " 'your',\n",
              " 'right',\n",
              " '.',\n",
              " 'Get',\n",
              " 'up',\n",
              " ',',\n",
              " 'stand',\n",
              " 'up',\n",
              " '.',\n",
              " 'Do',\n",
              " \"n't\",\n",
              " 'give',\n",
              " 'up',\n",
              " 'the',\n",
              " 'fight',\n",
              " '.']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenised = {}\n",
        "for key in lyrics.keys():\n",
        "    current = lyrics.get(key)\n",
        "    current = word_tokenize(current)\n",
        "    tokenised[key] = current\n",
        "tokenised[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMa4g0kGjOze"
      },
      "source": [
        "iii) Case normalise the tokenised strings and remove stopwords and punctuation, putting the results into a new dictionary called `normalised`, with the same keys as `tokenised` and the values being normalised lists of tokens.\n",
        "\n",
        "(6 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HZVk2RcMGQNY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['get', 'stand', 'stand', 'right', 'get', 'stand', 'do', 'give', 'fight']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalised = {}\n",
        "\n",
        "for key in tokenised.keys():\n",
        "    current = tokenised.get(key)\n",
        "    normed = [w.lower() for w in current if w.isalpha() and w not in stop]\n",
        "    normalised[key] = normed\n",
        "normalised[0] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFcNSwE-mmdt"
      },
      "source": [
        "iv) Describe two other forms of pre-processing that could be applied to text documents.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9iw4TowmvAQ"
      },
      "source": [
        "Another form of normalisation could be number normalisation. This is where we convert any reference to a number to the string 'NUM'. We do this because numbers are sparse features. For example, even if we were referring to semi-modern history there are still 100 of numbers to represent the years for the past few centuries. In terms of being a token, a year may only come up once across several documents. Instead, just understanding the context that this token is a number can be enough and allows all numbers to be groups removing the issue of sparseness. An example would be turning \"1980\" to \"NUM\". \n",
        "\n",
        "Another form is Lemmatisation. This is where we reduce works to their dictionary form, i.e. their lemma, which is the derivative of their past-of-speech. It is similar to another form of normalisation called stemming which reduces a word by chopping off its pre-fix or suffix but is slightly more advanced because the remaining root word will always be a true dictionary word. This is important for many NLP models and applications where results and methods need to be interpretable. This type of normalisation is also very helpful for feature sparsity as it allows mutual words that are derived from the same meaning to be grouped under the same feature, thus reducing the feature sparseness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzmBCqijjpj0"
      },
      "source": [
        "b) Convert each document, ie each list of tokens, into a tfidf representation by following the steps below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49faFMRdG0W"
      },
      "source": [
        "i) Calculate document frequencies for each token found in the documents contained in `normalised` and put the results in a dictionary called `df`. That is count how many different entries in the dictionary each token is found in. For example, the token `\"one\"` occurs in two different sets of lyrics, so `df[\"one\"]` should be 2.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Wo96NEz6Hcz8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique = set([token for key in normalised.keys() for token in normalised[key]])\n",
        "\n",
        "df = {}\n",
        "\n",
        "for token in unique:\n",
        "    counter = 0\n",
        "    for key in normalised.keys():\n",
        "        if token in normalised[key]:\n",
        "            counter += 1\n",
        "    df[token] = counter\n",
        "df[\"one\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67kNG26edztz"
      },
      "source": [
        "ii) Calculate inverse document frequencies from the document frequencies derived in the previous question and put the results in a dictionary called `idf`.\n",
        "\n",
        "$$IDF(w) = \\log_2 \\left( \\frac{N}{DF(w)} \\right) $$\n",
        "\n",
        "where $N$ is the total number of documents and $DF(w)$ is the document frequency of the word $w$.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wqTvv8c_wK0T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'earn': 2.584962500721156,\n",
              " 'keeps': 2.584962500721156,\n",
              " 'you': 1.584962500721156,\n",
              " 'get': 2.584962500721156,\n",
              " 'fight': 2.584962500721156,\n",
              " 'around': 2.584962500721156,\n",
              " 'like': 2.584962500721156,\n",
              " 'dreams': 2.584962500721156,\n",
              " 'life': 2.584962500721156,\n",
              " 'glistening': 2.584962500721156,\n",
              " 'but': 2.584962500721156,\n",
              " 'or': 2.584962500721156,\n",
              " 'happy': 2.584962500721156,\n",
              " 'year': 2.584962500721156,\n",
              " 'man': 2.584962500721156,\n",
              " 'stand': 2.584962500721156,\n",
              " 'gold': 2.584962500721156,\n",
              " 'must': 2.584962500721156,\n",
              " 'hypnotized': 2.584962500721156,\n",
              " 'just': 2.584962500721156,\n",
              " 'if': 2.584962500721156,\n",
              " 'seems': 2.584962500721156,\n",
              " 'one': 1.584962500721156,\n",
              " 'live': 2.584962500721156,\n",
              " 'us': 2.584962500721156,\n",
              " 'another': 2.584962500721156,\n",
              " 'living': 2.584962500721156,\n",
              " 'cryin': 2.584962500721156,\n",
              " 'twice': 2.584962500721156,\n",
              " 'everybody': 2.584962500721156,\n",
              " 'for': 2.584962500721156,\n",
              " 'crust': 2.584962500721156,\n",
              " 'right': 2.584962500721156,\n",
              " 'sure': 2.584962500721156,\n",
              " 'told': 2.584962500721156,\n",
              " 'cause': 2.584962500721156,\n",
              " 'do': 2.584962500721156,\n",
              " 'got': 2.584962500721156,\n",
              " 'give': 2.584962500721156,\n",
              " 'making': 2.584962500721156,\n",
              " 'and': 1.584962500721156,\n",
              " 'else': 2.584962500721156,\n",
              " 'naught': 2.584962500721156}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idf = {}\n",
        "\n",
        "num_docs = len(normalised.keys())\n",
        "\n",
        "for key in df.keys():\n",
        "    current_df = df[key]\n",
        "    _idf = log2(num_docs/current_df)\n",
        "    idf[key]=_idf\n",
        "\n",
        "idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbTHONWreDTN"
      },
      "source": [
        "iv) Convert each document in `normalised` from a list of tokens to a dictionary of term frequencies and put the results in a dictionary called `tf`.\n",
        "\n",
        "The keys of `tf` should be positions in the original list `corpus` and the values should be the term frequency dictionaries, which map from tokens to frequency in each document.\n",
        "\n",
        "So, for example, `{0: [\"Hello\", \"Bob\"]}` would become `{0: {\"Hello\": 1, \"Bob\": 1}}`.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "M5cfBQOTw7i_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'get': 2, 'stand': 3, 'right': 1, 'do': 1, 'give': 1, 'fight': 1}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "tf = {}\n",
        "\n",
        "for key in normalised.keys():\n",
        "    tf[key] = dict(FreqDist(normalised[key]))\n",
        "\n",
        "tf[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvH-9m9fWdV"
      },
      "source": [
        "v) Convert the raw term frequencies in `tf` to tfidf values using the dictionary `idf`.\n",
        "\n",
        "(3 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "WPL9avGW5rOU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: {'get': 5.169925001442312,\n",
              "  'stand': 7.754887502163468,\n",
              "  'right': 2.584962500721156,\n",
              "  'do': 2.584962500721156,\n",
              "  'give': 2.584962500721156,\n",
              "  'fight': 2.584962500721156},\n",
              " 1: {'cause': 2.584962500721156,\n",
              "  'around': 2.584962500721156,\n",
              "  'gold': 2.584962500721156,\n",
              "  'glistening': 2.584962500721156,\n",
              "  'making': 2.584962500721156,\n",
              "  'sure': 2.584962500721156,\n",
              "  'keeps': 2.584962500721156,\n",
              "  'us': 2.584962500721156,\n",
              "  'hypnotized': 2.584962500721156},\n",
              " 2: {'you': 1.584962500721156,\n",
              "  'told': 2.584962500721156,\n",
              "  'must': 7.754887502163468,\n",
              "  'and': 4.754887502163468,\n",
              "  'earn': 5.169925001442312,\n",
              "  'living': 2.584962500721156,\n",
              "  'crust': 2.584962500721156,\n",
              "  'like': 2.584962500721156,\n",
              "  'everybody': 2.584962500721156,\n",
              "  'else': 2.584962500721156},\n",
              " 3: {'another': 2.584962500721156,\n",
              "  'year': 5.169925001442312,\n",
              "  'happy': 5.169925001442312,\n",
              "  'just': 2.584962500721156,\n",
              "  'one': 1.584962500721156,\n",
              "  'but': 2.584962500721156,\n",
              "  'cryin': 5.169925001442312},\n",
              " 4: {'you': 1.584962500721156,\n",
              "  'live': 2.584962500721156,\n",
              "  'twice': 2.584962500721156,\n",
              "  'or': 2.584962500721156,\n",
              "  'seems': 2.584962500721156,\n",
              "  'one': 3.169925001442312,\n",
              "  'life': 2.584962500721156,\n",
              "  'and': 1.584962500721156,\n",
              "  'dreams': 2.584962500721156},\n",
              " 5: {'for': 2.584962500721156,\n",
              "  'man': 2.584962500721156,\n",
              "  'got': 2.584962500721156,\n",
              "  'if': 2.584962500721156,\n",
              "  'naught': 2.584962500721156}}"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = {}\n",
        "\n",
        "for key in normalised.keys():\n",
        "    current = tf.get(key,{})\n",
        "    current_tfidf = {} # avoid updating via direct ref\n",
        "    for entry in current.keys():\n",
        "        tf_value = current.get(entry)\n",
        "        idf_value = idf.get(entry)\n",
        "        tfidf_value = tf_value * idf_value\n",
        "        current_tfidf[entry] = tfidf_value\n",
        "    tfidf[key] = current_tfidf\n",
        "tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKSzAq1zfj-L"
      },
      "source": [
        "c) In the following steps, preprocess the string `query` and convert it to a tfidf representation. Then use this to find relevant quotations in the index `tfidf`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GF8dFWsHWlr"
      },
      "source": [
        "i) Define a function `dot` which takes two dictionaries containing tfidf values as inputs and calculates their dot product.\n",
        "\n",
        "(3 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ChDKHmrJFAiz"
      },
      "outputs": [],
      "source": [
        "def dot(dict1, dict2):\n",
        "    _c = 0\n",
        "    for word in dict1.keys():\n",
        "        if word in dict2.keys():\n",
        "            _c += dict1[word] * dict2[word]\n",
        "    return _c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS6SEhUEHpsm"
      },
      "source": [
        "ii) Define a function, `sim`, which takes two dictionaries containing tfidf values as inputs and , using your `dot` function, calculates their cosine similarity.\n",
        "\n",
        "(3 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "9kL3ufDkFOuX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "def sim(dict1, dict2):\n",
        "    sim=dot(dict1,dict2)/(sqrt(dot(dict1,dict1)*dot(dict2,dict2)))\n",
        "    return sim\n",
        "\n",
        "print(sim(tfidf[0],tfidf[0]))\n",
        "print(sim(tfidf[0],tfidf[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x17i627INSk"
      },
      "source": [
        "iii) Preprocess the string `query` and convert it to a tfidf representation. Then calculate its cosine similarity to all the documents in the dictionary `tfidf`.\n",
        "\n",
        "For any document with a non-zero similarity, print out the similarity and the corresponding lyric and title from the relevant dictionaries.\n",
        "\n",
        "(8 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mf_pHqq4kzk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: In life, what have we got to stand up for?\n",
            "---\n",
            "Title: Get Up, Stand Up | Artist: Bob Marley\n",
            "Has a Cosine Similarity of 0.4201\n",
            "\"Get up, stand up. Stand up for your right. Get up, stand up. Don't give up the fight.\"\n",
            "---\n",
            "Title: You Only Live Twice | Artist: Nancy Sinatra\n",
            "Has a Cosine Similarity of 0.2009\n",
            "\"You only live twice. Or so it seems. One life for yourself. And one for your dreams.\"\n",
            "---\n",
            "Title: My Way | Artist: Frank Sinatra\n",
            "Has a Cosine Similarity of 0.2582\n",
            "\"For what is a man, what has he got? If not himself, then he has naught.\"\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "print(f\"Query: {query}\")\n",
        "print(\"---\")\n",
        "\n",
        "# PREPROCCESS\n",
        "q = word_tokenize(query)\n",
        "q_norm = [t.lower() for t in q if t.isalpha() and t.lower() not in stop]\n",
        "q_freq = dict(FreqDist(q_norm))\n",
        "\n",
        "q_tfidf = {}\n",
        "\n",
        "for key in q_freq.keys():\n",
        "    _tf_q = q_freq.get(key)\n",
        "    _idf_q = idf.get(key,0)\n",
        "    _tfidf_q = _tf_q * _idf_q\n",
        "    q_tfidf[key] = _tfidf_q\n",
        "\n",
        "for song in tfidf.keys():\n",
        "    score = sim(q_tfidf, tfidf[song])\n",
        "    if score > 0:\n",
        "        print(f\"Title: {titles[song]} | Artist: {artists[song]}\")\n",
        "        print(f\"Has a Cosine Similarity of {score:.4f}\")\n",
        "        print(f'\"{lyrics[song]}\"')\n",
        "        print(\"---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "generative_ai_disabled": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_exam",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
