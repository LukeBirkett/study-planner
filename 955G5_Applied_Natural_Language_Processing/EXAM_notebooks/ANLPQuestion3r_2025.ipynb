{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M3r84BT5WeN"
      },
      "source": [
        "#Applied Natural Language Processing\n",
        "\n",
        "##Computer Based Examination, 2025\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ibHDq-Im5Re4"
      },
      "outputs": [],
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 11111111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259FEqEb5xRr"
      },
      "source": [
        "\n",
        "\n",
        "#Question 3 (50 Marks)\n",
        "\n",
        "This question is about POS tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "89Wx4WPa5weU"
      },
      "outputs": [],
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "import nltk\n",
        "\n",
        "# This is a list of sentences with POS tags which we will use in this question.\n",
        "tagged_sents=[['The_DT', 'coursework_NN', 'involved_VBN', 'using_VBG', 'Naive_JJ', 'Bayes_NNP', 'to_TO', 'model_VB', 'the_DT', 'sentiment_NN', 'of_IN', 'moview_NN', 'reviews_NNS', '._.'],\n",
        "              ['A_DT', 'model_JJ', 'solution_NN', 'for_IN', 'this_DT', 'problem_NN', 'was_VBD', 'provided_VBN', 'in_IN', 'the_DT', 'labs_NN', '._.'],\n",
        "              ['Some_DT', 'students_NNS', 'used_VBD', 'a_DT', 'language_NN', 'model_NN', 'to_TO', 'generate_VB', 'an_DT', 'answer_NN', '._.']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MUMcHxbBsBi"
      },
      "source": [
        "a) Follow the steps below to preprocess the POS tagged sentences, producing a list of lists of tokens and a list of lists of POS tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDwzz0oCL8P"
      },
      "source": [
        "i) Use the `.split()` method to separate the tokens from the POS tags in the list `tagged_sents`, and strip off the tags to produce a list of lists of strings, called `tokens`, corresponding to just the tokens in each sentence.\n",
        "\n",
        "So, for example `[[\"Alice_NNP\", \"runs_VBZ\"], [\"Bob_NNP\", \"walks_VBZ\"]]` would produce `[[\"Alice\", \"runs\"], [\"Bob\", \"walks\"]]`.\n",
        "\n",
        "(8 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9fY75wgToDMo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['The', 'coursework', 'involved', 'using', 'Naive', 'Bayes', 'to', 'model', 'the', 'sentiment', 'of', 'moview', 'reviews', '.'], ['A', 'model', 'solution', 'for', 'this', 'problem', 'was', 'provided', 'in', 'the', 'labs', '.'], ['Some', 'students', 'used', 'a', 'language', 'model', 'to', 'generate', 'an', 'answer', '.']]\n",
            "['Some', 'students', 'used', 'a', 'language', 'model', 'to', 'generate', 'an', 'answer', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = []\n",
        "\n",
        "for sent in tagged_sents:\n",
        "    _sent = []\n",
        "    for token in sent:\n",
        "        _sent.append(token.split(\"_\")[0])\n",
        "    tokens.append(_sent)\n",
        "\n",
        "print(tokens)\n",
        "print(tokens[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glog6pY1NkcV"
      },
      "source": [
        "ii) Use the `.split()` method to separate the tokens from the POS tags in the list `tagged_sents`, producing a list of lists of POS tags, called `tags`.\n",
        "\n",
        "So, for example `[[\"Alice_NNP\", \"runs_VBZ\"], [\"chase_VBP\", \"Bob_NNP\"]]` would produce `[[\"NNP\", \"VBZ\"], [\"VBP\", \"NNP\"]]`.\n",
        "\n",
        "(6 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2ZgLxYBConpu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['DT', 'NN', 'VBN', 'VBG', 'JJ', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', '.'], ['DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NN', '.'], ['DT', 'NNS', 'VBD', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']]\n",
            "['DT', 'NNS', 'VBD', 'DT', 'NN', 'NN', 'TO', 'VB', 'DT', 'NN', '.']\n"
          ]
        }
      ],
      "source": [
        "tags = []\n",
        "\n",
        "for sent in tagged_sents:\n",
        "    _sent = []\n",
        "    for tag in sent:\n",
        "        _sent.append(tag.split(\"_\")[1])\n",
        "    tags.append(_sent)\n",
        "\n",
        "print(tags)\n",
        "print(tags[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhzmSCQTRhJo"
      },
      "source": [
        "b) Now, follow the steps below to derive the transition and emission probabilities of a Hidden Markov Model for the POS tag sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lz2eIJ6Ry6w"
      },
      "source": [
        "i) Describe the two assumptions that a Hidden Markov Model for sequence tagging is based on.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2vWgcgIUnvp"
      },
      "source": [
        "Horizton and Vertical Assumptions\n",
        "\n",
        "The Vertical assumption states that the current word depends only on its tag meaning it does not see any previous context in the sentence $P(w_i | t_i)$\n",
        "\n",
        "The Horiztonal assumption states that the current tag depends only on the tag preceeding it $P(t_i | t_{i-1})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDlCGNMdSHBx"
      },
      "source": [
        "ii) Create a dictionary, called `poscounts`, containing the total counts for each tag in the list `tags`. The keys of `poscounts` should be tags and the values should be the frequencies of those tags.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ASbaMzO74wMg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'DT': 8,\n",
              " 'NN': 9,\n",
              " 'VBN': 2,\n",
              " 'VBG': 1,\n",
              " 'JJ': 2,\n",
              " 'NNP': 1,\n",
              " 'TO': 2,\n",
              " 'VB': 2,\n",
              " 'IN': 3,\n",
              " 'NNS': 2,\n",
              " '.': 3,\n",
              " 'VBD': 2}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "poscounts = {}\n",
        "\n",
        "for sent in tags:\n",
        "    for tag in sent:\n",
        "        current = poscounts.get(tag,0)+1\n",
        "        poscounts[tag] = current\n",
        "\n",
        "poscounts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ54_g706lpv"
      },
      "source": [
        "iii) Using the lists `tags` and `tokens`, calculate the emission probabilities: $$p(token|tag)$$\n",
        "\n",
        "Put these probabilities in a dictionary of dictionaries, called `emission`, with outer keys being POS tags, inner keys being tokens and inner values being the emission probabilities.\n",
        "\n",
        "(8 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Ttpe2z36R2n0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'DT': {'The': 1, 'the': 2, 'A': 1, 'this': 1, 'Some': 1, 'a': 1, 'an': 1}, 'NN': {'coursework': 1, 'sentiment': 1, 'moview': 1, 'solution': 1, 'problem': 1, 'labs': 1, 'language': 1, 'model': 1, 'answer': 1}, 'VBN': {'involved': 1, 'provided': 1}, 'VBG': {'using': 1}, 'JJ': {'Naive': 1, 'model': 1}, 'NNP': {'Bayes': 1}, 'TO': {'to': 2}, 'VB': {'model': 1, 'generate': 1}, 'IN': {'of': 1, 'for': 1, 'in': 1}, 'NNS': {'reviews': 1, 'students': 1}, '.': {'.': 3}, 'VBD': {'was': 1, 'used': 1}}\n",
            "{'DT': {'The': 0.125, 'the': 0.25, 'A': 0.125, 'this': 0.125, 'Some': 0.125, 'a': 0.125, 'an': 0.125}, 'NN': {'coursework': 0.1111111111111111, 'sentiment': 0.1111111111111111, 'moview': 0.1111111111111111, 'solution': 0.1111111111111111, 'problem': 0.1111111111111111, 'labs': 0.1111111111111111, 'language': 0.1111111111111111, 'model': 0.1111111111111111, 'answer': 0.1111111111111111}, 'VBN': {'involved': 0.5, 'provided': 0.5}, 'VBG': {'using': 1.0}, 'JJ': {'Naive': 0.5, 'model': 0.5}, 'NNP': {'Bayes': 1.0}, 'TO': {'to': 1.0}, 'VB': {'model': 0.5, 'generate': 0.5}, 'IN': {'of': 0.3333333333333333, 'for': 0.3333333333333333, 'in': 0.3333333333333333}, 'NNS': {'reviews': 0.5, 'students': 0.5}, '.': {'.': 1.0}, 'VBD': {'was': 0.5, 'used': 0.5}}\n"
          ]
        }
      ],
      "source": [
        "emission = {}\n",
        "\n",
        "for sent_index in range(len(tags)):\n",
        "    for pos_index in range(len(tokens[sent_index])): \n",
        "        \n",
        "        token = tokens[sent_index][pos_index]\n",
        "        tag = tags[sent_index][pos_index]\n",
        "        \n",
        "        current_tag = emission.get(tag,{})\n",
        "\n",
        "        current_token = current_tag.get(token,0)+1\n",
        "        current_tag[token] = current_token\n",
        "\n",
        "        emission[tag] = current_tag\n",
        "\n",
        "print(emission)\n",
        "\n",
        "for _tag in emission.keys():\n",
        "    denominator = sum(emission.get(_tag).values())\n",
        "    update = emission.get(_tag)\n",
        "    for key in update.keys():\n",
        "        update[key] = update.get(key)/denominator\n",
        "\n",
        "print(emission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugDBChyQ8MHu"
      },
      "source": [
        "iv) Using the list `tags`, calculate the transition probabilities: $$p(tag_i|tag_{i-1})$$\n",
        "\n",
        "You will need to keep track of both the current tag, $tag_i$, and the previous tag, $tag_{i-1}$, and you will need to introduce a special tag, e.g. \"START\", for the previous tag at the beginning of a sequence.\n",
        "\n",
        "For example, the sequence of tags `[\"NNP\", \"VBZ\"]` would give rise to the following values of $i$, $tag_{i-1}$ and $tag_i$:\n",
        "\n",
        "|i|$tag_{i-1}$|$tag_i$|\n",
        "|---|---|---|\n",
        "|0|START|NNP|\n",
        "|1|NNP|VBZ|\n",
        "\n",
        "Calculate the conditional probabilities of $tag_i$ given $tag_{i-1}$ and put the results in a dictionary of dictionaries, called `transition`, with outer keys being previous tags, inner keys being current tags and inner values being transition probabilities.\n",
        "\n",
        "(10 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "bbRg_hyU_76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'START': {'DT': 3}, 'DT': {'NN': 6, 'JJ': 1, 'NNS': 1}, 'NN': {'VBN': 1, 'IN': 2, 'NNS': 1, 'VBD': 1, '.': 2, 'NN': 1, 'TO': 1}, 'VBN': {'VBG': 1, 'IN': 1}, 'VBG': {'JJ': 1}, 'JJ': {'NNP': 1, 'NN': 1}, 'NNP': {'TO': 1}, 'TO': {'VB': 2}, 'VB': {'DT': 2}, 'IN': {'NN': 1, 'DT': 2}, 'NNS': {'.': 1, 'VBD': 1}, 'VBD': {'VBN': 1, 'DT': 1}}\n",
            "{'START': {'DT': 1.0}, 'DT': {'NN': 0.75, 'JJ': 0.125, 'NNS': 0.125}, 'NN': {'VBN': 0.1111111111111111, 'IN': 0.2222222222222222, 'NNS': 0.1111111111111111, 'VBD': 0.1111111111111111, '.': 0.2222222222222222, 'NN': 0.1111111111111111, 'TO': 0.1111111111111111}, 'VBN': {'VBG': 0.5, 'IN': 0.5}, 'VBG': {'JJ': 1.0}, 'JJ': {'NNP': 0.5, 'NN': 0.5}, 'NNP': {'TO': 1.0}, 'TO': {'VB': 1.0}, 'VB': {'DT': 1.0}, 'IN': {'NN': 0.3333333333333333, 'DT': 0.6666666666666666}, 'NNS': {'.': 0.5, 'VBD': 0.5}, 'VBD': {'VBN': 0.5, 'DT': 0.5}}\n"
          ]
        }
      ],
      "source": [
        "transition = {}\n",
        "\n",
        "for sent in tags:\n",
        "    sent = [\"START\"] + sent\n",
        "    for ind in range(0, len(sent) - 1, 1):\n",
        "        \n",
        "        tag_prev = sent[ind]\n",
        "        tag = sent[ind+1]\n",
        "\n",
        "        outer = transition.get(tag_prev,{})\n",
        "        counter = outer.get(tag,0)+1\n",
        "        outer[tag] = counter\n",
        "        transition[tag_prev] = outer\n",
        "\n",
        "print(transition)\n",
        "    \n",
        "for _tag in transition.keys():\n",
        "    denominator = sum(transition.get(_tag).values())\n",
        "    update = transition.get(_tag)\n",
        "    for key in update.keys():\n",
        "        update[key] = update.get(key)/denominator\n",
        "\n",
        "print(transition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDevraTjRH5X"
      },
      "source": [
        "v) Given a sequence of tokens, we want to find the most probable sequence of tags corresponding to those tokens. The brute force approach would simply evaluate the probability of every sequence of tags. Explain why we would want to avoid that approach and describe an alternative algorithm.\n",
        "\n",
        "(6 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7YwQI7MZxEY"
      },
      "source": [
        "The reason why we have to avoid brute force for most sequence tagging problems in NLP is because the number of computations grows to a number which not managable. A brute force approach requires us to compute, store and update every sequence possible. For example, if we have a sequence with 8 positions and 10 possible tags then for the final sequence of computations we will have to compute $10^8$ possibilities which is 100,000,000 calculations. A real world tagging example, like the Penn Treebank has 45 tags, meaning computation soon becomes impossible.  A more efficent approach is the Vitberbi Algorithm. This draws on of the Vertical and Horizontal assumptions of Hidden Markov Models to allow for modularity and a reduction in total calcualtions need. Viterbi works by selecting the most probabile sequence at each position for each tag and then storing each tags sequence in a \"trellis\" of columns. At any position, in order to prediect the next word, the Viterbi Algoritm consults the positions tag as per the vertical assumption. However, the positions tag relies on the previous tag as per the horizontal assumption. In a brute force scenario this logic would permeate all the way back through every possible sequence which could get us to the current positions tag. However, Viterbi freeze and stores just the `n` number of best sequences that lead to each tag. Thus, when invoking the horizontal assumption, a consulting the previous tag, there is only `n` calculations to compute where `n` is the number of tags. In our toy example, of 8 sequence positions and 10 tags this means the complexity is $10^8=800$ which is a huge reducition compared to brute force. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdxdMimsLwpx"
      },
      "source": [
        "vi) If we are tagging a sequence and the current token is `\"model\"` and the previous tag is `\"TO\"`, then we want know to which tag in the current position  would maximise the probability of seeing that token.\n",
        "\n",
        "Define a function that takes a token, a previous tag, a dictionary of emission probabilities and a dictionary of transition probabilities and returns a dictionary of the probabilities $p(token, tag_i | tag_{i-1})$ for all values of $tag_i$.\n",
        "\n",
        "Apply this function to the case where the current token is `\"model\"` and the previous tag is `\"TO\"`.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Me8nvZbRIbw"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'model'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m emission\n\u001b[32m      4\u001b[39m transition\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43memission\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m transition\n",
            "\u001b[31mKeyError\u001b[39m: 'model'"
          ]
        }
      ],
      "source": [
        "token = \"model\"\n",
        "prev_tag = \"TO\"\n",
        "emission\n",
        "transition\n",
        "\n",
        "\n",
        "\n",
        "emission\n",
        "\n",
        "transition\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_exam",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
