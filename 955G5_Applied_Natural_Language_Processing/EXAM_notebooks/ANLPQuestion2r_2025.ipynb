{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh003D9cig7w"
      },
      "source": [
        "# Applied Natural Language Processing 955G5\n",
        "## Computer Based Examination, 2025\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dVoZCYKqihTF"
      },
      "outputs": [],
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 11111111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgnCxc1qFUTS"
      },
      "source": [
        "# Question 2 (50 Marks)\n",
        "\n",
        "This question is about semantics and distributional representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pviDy-pFpKF",
        "outputId": "ef053c8b-22d8-4132-fa05-134395cbc6bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/lukebirkett/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from math import log, sqrt\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "sentences=[['A', 'semantic', 'network', ',', 'or', 'frame', 'network', 'is', 'a', 'knowledge', 'base', 'that', 'represents', 'semantic', 'connections', 'between', 'concepts', 'in', 'a', 'network', '.'],\n",
        "           ['In', 'mathematics', 'and', 'computer', 'science', ',', 'graph', 'theory', 'is', 'the', 'study', 'of', 'graphs', ',', 'which', 'are', 'mathematical', 'structures', 'used', 'to', 'model', 'pairwise', 'relations', 'between', 'objects', '.'],\n",
        "           ['A', 'neural', 'network', 'consists', 'of', 'connected', 'units', 'or', 'nodes', 'called', 'artificial', 'neurons', 'connected', 'by', 'edges', ',', 'which', 'model', 'the', 'synapses', 'in', 'the', 'brain', '.'],\n",
        "           ['A', 'connectome', 'is', 'a', 'comprehensive', 'map', 'of', 'neural', 'connections', 'in', 'the', 'brain', ',', 'and', 'may', 'be', 'thought', 'of', 'as', 'its', \"'wiring\", 'diagram', \"'\", '.']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkDE9kBBXyet"
      },
      "source": [
        "a) Build distributional representations of the words in `sentences` in the following steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uAYbz4RGz0U"
      },
      "source": [
        "i) Write a function that takes a list and an index as input and returns a list containing the two context tokens each side of that index, if they exist.\n",
        "\n",
        "For example, given the list `[\"This\", \"is\", \"a\", \"sentence\", \".\"]` and the index `1`, the function would return `[\"This\", \"a\", \"sentence\"]`. But given the same list and the index `0` would return `[\"is\", \"a\"]`.\n",
        "\n",
        "(4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QOyPJu5iI7wv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['is', 'a']\n",
            "['This', 'a', 'sentence']\n",
            "['This', 'is', 'sentence', '.']\n",
            "['is', 'a', '.']\n",
            "['a', 'sentence']\n",
            "['sentence', '.']\n"
          ]
        }
      ],
      "source": [
        "def window_slicer(lst, index):\n",
        "    # 'window' is 2 as per the prompt \"two tokens each side\"\n",
        "    left_context = lst[max(0, index - 2) : index]\n",
        "    right_context = lst[index + 1 : index + 3]\n",
        "    return left_context + right_context\n",
        "\n",
        "example_sentence = [\"This\", \"is\", \"a\", \"sentence\", \".\"]\n",
        "print(window_slicer(example_sentence, 0))\n",
        "print(window_slicer(example_sentence, 1))\n",
        "print(window_slicer(example_sentence, 2))\n",
        "print(window_slicer(example_sentence, 3))\n",
        "print(window_slicer(example_sentence, 4))\n",
        "print(window_slicer(example_sentence, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD6kgDsNI87x"
      },
      "source": [
        "ii) Using the function above, compute counts of token co-occurrences in the `sentences` list. The result should be a dictionary of dictionaries called `word_feature_counts`, with the keys of the outer dictionaries being the word tokens from `sentences` and the inner keys being the context token features returned by your function. For example, given the sentences `[\"This\", \"is\", \"an\", \"example\"],[\"This\", \"is\", \"another\"]]`, the tokens `This` and `is` co-occur twice. So, `word_feature_counts[\"is\"][\"This\"]` would return the number `2`.\n",
        "\n",
        "(8 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'A': 1, 'network': 1, ',': 1, 'that': 1, 'represents': 1, 'connections': 1, 'between': 1}\n"
          ]
        }
      ],
      "source": [
        "def co_occurance(textlist):\n",
        "\n",
        "    coocs = {}\n",
        "\n",
        "    for sent in textlist:\n",
        "        for index in range(len(sent)):\n",
        "\n",
        "            token = sent[index]\n",
        "            window_features = window_slicer(sent, index)\n",
        "\n",
        "            current = coocs.get(token, {})\n",
        "\n",
        "            for feature in window_features:\n",
        "                current[feature] = current.get(feature, 0) + 1\n",
        "            \n",
        "            coocs[token] = current\n",
        "\n",
        "    return coocs\n",
        "\n",
        "word_feature_counts = co_occurance(sentences)\n",
        "print(word_feature_counts[\"semantic\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBRd5gTfLwUI"
      },
      "source": [
        "b) So far, we have built a dictionary of word co-occurrences as an intial step towards constructing a distributional representation of semantics. Describe the alternative neural network approach as implemented in approaches such as CBOW.\n",
        "\n",
        "(5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1_Qzkrt5WrF"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10IbUS6_BpRs"
      },
      "source": [
        "c)\n",
        "\n",
        "i) Sum over the counts in `word_feature_counts` to produce a total count for all words and feature and a dictionary of total counts for each word, $w$, and a dictionary of total counts for each feature, $f$.\n",
        "\n",
        "(5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Word Count (dict)\n",
        "* Feature Count (dict)\n",
        "* Grand Total Count (value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zRdEtAY3CRPT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "356"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = set([token for sentence in sentences for token in sentence])\n",
        "\n",
        "words = {}\n",
        "features = {}\n",
        "total = 0\n",
        "\n",
        "for word in tokens:\n",
        "    current_features = word_feature_counts.get(word,{})\n",
        "    words[word] = sum(current_features.values())\n",
        "\n",
        "    for feature in current_features:\n",
        "        features[feature] = features.get(feature,0)+current_features[feature]\n",
        "\n",
        "total += sum(words.values())\n",
        "total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbvcGUia5XMr"
      },
      "source": [
        "ii) Turn these counts into Positive PMI values.\n",
        "\n",
        "$$PPMI = \\max \\left( 0, \\log \\left( \\frac{freq(w,f) \\times freq_{tot}}{freq(w) \\times freq(f)} \\right) \\right)$$\n",
        "\n",
        "Here, $freq(w,f)$ represents the count in `word_feature_counts[w][f]` and $freq_{tot}$ is the sum of these counts for all words and features. $freq(w)$ is the total of the counts for word $w$ and $freq(f)$ is the total of the counts for feature $f$.\n",
        "\n",
        "(5 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "t0YaJ4pm5nrd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'A': {'semantic': 1, 'network': 2, 'neural': 1, 'connectome': 1, 'is': 1}, 'semantic': {'A': 1, 'network': 1, ',': 1, 'that': 1, 'represents': 1, 'connections': 1, 'between': 1}, 'network': {'A': 2, 'semantic': 1, ',': 1, 'or': 2, 'frame': 1, 'is': 1, 'a': 2, 'in': 1, '.': 1, 'neural': 1, 'consists': 1, 'of': 1}, ',': {'semantic': 1, 'network': 1, 'or': 1, 'frame': 1, 'computer': 1, 'science': 1, 'graph': 1, 'theory': 1, 'of': 1, 'graphs': 1, 'which': 2, 'are': 1, 'by': 1, 'edges': 1, 'model': 1, 'the': 1, 'brain': 1, 'and': 1, 'may': 1}, 'or': {'network': 2, ',': 1, 'frame': 1, 'connected': 1, 'units': 1, 'nodes': 1, 'called': 1}, 'frame': {',': 1, 'or': 1, 'network': 1, 'is': 1}, 'is': {'frame': 1, 'network': 1, 'a': 2, 'knowledge': 1, 'graph': 1, 'theory': 1, 'the': 1, 'study': 1, 'A': 1, 'connectome': 1, 'comprehensive': 1}, 'a': {'network': 2, 'is': 2, 'knowledge': 1, 'base': 1, 'concepts': 1, 'in': 1, '.': 1, 'connectome': 1, 'comprehensive': 1, 'map': 1}, 'knowledge': {'is': 1, 'a': 1, 'base': 1, 'that': 1}, 'base': {'a': 1, 'knowledge': 1, 'that': 1, 'represents': 1}, 'that': {'knowledge': 1, 'base': 1, 'represents': 1, 'semantic': 1}, 'represents': {'base': 1, 'that': 1, 'semantic': 1, 'connections': 1}, 'connections': {'represents': 1, 'semantic': 1, 'between': 1, 'concepts': 1, 'of': 1, 'neural': 1, 'in': 1, 'the': 1}, 'between': {'semantic': 1, 'connections': 1, 'concepts': 1, 'in': 1, 'pairwise': 1, 'relations': 1, 'objects': 1, '.': 1}, 'concepts': {'connections': 1, 'between': 1, 'in': 1, 'a': 1}, 'in': {'between': 1, 'concepts': 1, 'a': 1, 'network': 1, 'the': 3, 'synapses': 1, 'brain': 2, 'neural': 1, 'connections': 1}, '.': {'a': 1, 'network': 1, 'between': 1, 'objects': 1, 'the': 1, 'brain': 1, 'diagram': 1, \"'\": 1}, 'In': {'mathematics': 1, 'and': 1}, 'mathematics': {'In': 1, 'and': 1, 'computer': 1}, 'and': {'In': 1, 'mathematics': 1, 'computer': 1, 'science': 1, 'brain': 1, ',': 1, 'may': 1, 'be': 1}, 'computer': {'mathematics': 1, 'and': 1, 'science': 1, ',': 1}, 'science': {'and': 1, 'computer': 1, ',': 1, 'graph': 1}, 'graph': {'science': 1, ',': 1, 'theory': 1, 'is': 1}, 'theory': {',': 1, 'graph': 1, 'is': 1, 'the': 1}, 'the': {'theory': 1, 'is': 1, 'study': 1, 'of': 1, 'which': 1, 'model': 1, 'synapses': 2, 'in': 3, 'brain': 2, '.': 1, 'connections': 1, ',': 1}, 'study': {'is': 1, 'the': 1, 'of': 1, 'graphs': 1}, 'of': {'the': 1, 'study': 1, 'graphs': 1, ',': 1, 'network': 1, 'consists': 1, 'connected': 1, 'units': 1, 'comprehensive': 1, 'map': 1, 'neural': 1, 'connections': 1, 'be': 1, 'thought': 1, 'as': 1, 'its': 1}, 'graphs': {'study': 1, 'of': 1, ',': 1, 'which': 1}, 'which': {'graphs': 1, ',': 2, 'are': 1, 'mathematical': 1, 'edges': 1, 'model': 1, 'the': 1}, 'are': {',': 1, 'which': 1, 'mathematical': 1, 'structures': 1}, 'mathematical': {'which': 1, 'are': 1, 'structures': 1, 'used': 1}, 'structures': {'are': 1, 'mathematical': 1, 'used': 1, 'to': 1}, 'used': {'mathematical': 1, 'structures': 1, 'to': 1, 'model': 1}, 'to': {'structures': 1, 'used': 1, 'model': 1, 'pairwise': 1}, 'model': {'used': 1, 'to': 1, 'pairwise': 1, 'relations': 1, ',': 1, 'which': 1, 'the': 1, 'synapses': 1}, 'pairwise': {'to': 1, 'model': 1, 'relations': 1, 'between': 1}, 'relations': {'model': 1, 'pairwise': 1, 'between': 1, 'objects': 1}, 'objects': {'relations': 1, 'between': 1, '.': 1}, 'neural': {'A': 1, 'network': 1, 'consists': 1, 'map': 1, 'of': 1, 'connections': 1, 'in': 1}, 'consists': {'neural': 1, 'network': 1, 'of': 1, 'connected': 1}, 'connected': {'consists': 1, 'of': 1, 'units': 1, 'or': 1, 'artificial': 1, 'neurons': 1, 'by': 1, 'edges': 1}, 'units': {'of': 1, 'connected': 1, 'or': 1, 'nodes': 1}, 'nodes': {'units': 1, 'or': 1, 'called': 1, 'artificial': 1}, 'called': {'or': 1, 'nodes': 1, 'artificial': 1, 'neurons': 1}, 'artificial': {'nodes': 1, 'called': 1, 'neurons': 1, 'connected': 1}, 'neurons': {'called': 1, 'artificial': 1, 'connected': 1, 'by': 1}, 'by': {'neurons': 1, 'connected': 1, 'edges': 1, ',': 1}, 'edges': {'connected': 1, 'by': 1, ',': 1, 'which': 1}, 'synapses': {'model': 1, 'the': 2, 'in': 1}, 'brain': {'in': 2, 'the': 2, '.': 1, ',': 1, 'and': 1}, 'connectome': {'A': 1, 'is': 1, 'a': 1}, 'comprehensive': {'is': 1, 'a': 1, 'map': 1, 'of': 1}, 'map': {'a': 1, 'comprehensive': 1, 'of': 1, 'neural': 1}, 'may': {',': 1, 'and': 1, 'be': 1, 'thought': 1}, 'be': {'and': 1, 'may': 1, 'thought': 1, 'of': 1}, 'thought': {'may': 1, 'be': 1, 'of': 1, 'as': 1}, 'as': {'thought': 1, 'of': 1, 'its': 1, \"'wiring\": 1}, 'its': {'of': 1, 'as': 1, \"'wiring\": 1, 'diagram': 1}, \"'wiring\": {'as': 1, 'its': 1, 'diagram': 1, \"'\": 1}, 'diagram': {'its': 1, \"'wiring\": 1, \"'\": 1, '.': 1}, \"'\": {\"'wiring\": 1, 'diagram': 1, '.': 1}}\n",
            "{'A': {'semantic': 3.0834160081876374, 'network': 2.9838803346367233, 'neural': 3.0834160081876374, 'connectome': 4.3058084295240855, 'is': 2.3058084295240855}, 'semantic': {'A': 3.0834160081876374, 'network': 1.761487913300275, ',': 1.3464504140214313, 'that': 3.6683785089087935, 'represents': 3.6683785089087935, 'connections': 2.6683785089087935, 'between': 2.6683785089087935}, 'network': {'A': 2.9838803346367233, 'semantic': 1.761487913300275, ',': 0.24691474047051698, 'or': 2.5688428353578794, 'frame': 2.5688428353578794, 'is': 0.9838803346367231, 'a': 1.983880334636723, 'in': 0.9838803346367231, '.': 1.5688428353578794, 'neural': 1.761487913300275, 'consists': 2.5688428353578794, 'of': 0.5688428353578793}, ',': {'semantic': 1.3464504140214313, 'network': 0.24691474047051698, 'or': 1.1538053360790355, 'frame': 2.1538053360790355, 'computer': 2.1538053360790355, 'science': 2.1538053360790355, 'graph': 2.1538053360790355, 'theory': 2.1538053360790355, 'of': 0.15380533607903546, 'graphs': 2.1538053360790355, 'which': 2.1538053360790355, 'are': 2.1538053360790355, 'by': 2.1538053360790355, 'edges': 2.1538053360790355, 'model': 1.1538053360790355, 'the': 0.15380533607903546, 'brain': 1.3464504140214313, 'and': 1.1538053360790355, 'may': 2.1538053360790355}, 'or': {'network': 2.5688428353578794, ',': 1.1538053360790355, 'frame': 3.4757334309663976, 'connected': 2.4757334309663976, 'units': 3.4757334309663976, 'nodes': 3.4757334309663976, 'called': 3.4757334309663976}, 'frame': {',': 2.1538053360790355, 'or': 3.4757334309663976, 'network': 2.5688428353578794, 'is': 2.8907709302452416}, 'is': {'frame': 2.8907709302452416, 'network': 0.9838803346367231, 'a': 2.3058084295240855, 'knowledge': 2.8907709302452416, 'graph': 2.8907709302452416, 'theory': 2.8907709302452416, 'the': 0.8907709302452417, 'study': 2.8907709302452416, 'A': 2.3058084295240855, 'connectome': 3.3058084295240855, 'comprehensive': 2.8907709302452416}, 'a': {'network': 1.983880334636723, 'is': 2.3058084295240855, 'knowledge': 2.8907709302452416, 'base': 2.8907709302452416, 'concepts': 2.8907709302452416, 'in': 1.3058084295240855, '.': 1.8907709302452416, 'connectome': 3.3058084295240855, 'comprehensive': 2.8907709302452416, 'map': 2.8907709302452416}, 'knowledge': {'is': 2.8907709302452416, 'a': 2.8907709302452416, 'base': 4.475733430966398, 'that': 4.475733430966398}, 'base': {'a': 2.8907709302452416, 'knowledge': 4.475733430966398, 'that': 4.475733430966398, 'represents': 4.475733430966398}, 'that': {'knowledge': 4.475733430966398, 'base': 4.475733430966398, 'represents': 4.475733430966398, 'semantic': 3.6683785089087935}, 'represents': {'base': 4.475733430966398, 'that': 4.475733430966398, 'semantic': 3.6683785089087935, 'connections': 3.4757334309663976}, 'connections': {'represents': 3.4757334309663976, 'semantic': 2.6683785089087935, 'between': 2.4757334309663976, 'concepts': 3.4757334309663976, 'of': 1.4757334309663979, 'neural': 2.6683785089087935, 'in': 1.8907709302452416, 'the': 1.4757334309663979}, 'between': {'semantic': 2.6683785089087935, 'connections': 2.4757334309663976, 'concepts': 3.4757334309663976, 'in': 1.8907709302452416, 'pairwise': 3.4757334309663976, 'relations': 3.4757334309663976, 'objects': 3.8907709302452416, '.': 2.4757334309663976}, 'concepts': {'connections': 3.4757334309663976, 'between': 3.4757334309663976, 'in': 2.8907709302452416, 'a': 2.8907709302452416}, 'in': {'between': 1.8907709302452416, 'concepts': 2.8907709302452416, 'a': 1.3058084295240855, 'network': 0.9838803346367231, 'the': 2.4757334309663976, 'synapses': 2.8907709302452416, 'brain': 3.0834160081876374, 'neural': 2.0834160081876374, 'connections': 1.8907709302452416}, '.': {'a': 1.8907709302452416, 'network': 1.5688428353578794, 'between': 2.4757334309663976, 'objects': 3.8907709302452416, 'the': 1.4757334309663979, 'brain': 2.6683785089087935, 'diagram': 3.4757334309663976, \"'\": 3.8907709302452416}, 'In': {'mathematics': 5.890770930245242, 'and': 4.475733430966398}, 'mathematics': {'In': 5.890770930245242, 'and': 3.8907709302452416, 'computer': 4.890770930245242}, 'and': {'In': 4.475733430966398, 'mathematics': 3.8907709302452416, 'computer': 3.4757334309663976, 'science': 3.4757334309663976, 'brain': 2.6683785089087935, ',': 1.1538053360790355, 'may': 3.4757334309663976, 'be': 3.4757334309663976}, 'computer': {'mathematics': 4.890770930245242, 'and': 3.4757334309663976, 'science': 4.475733430966398, ',': 2.1538053360790355}, 'science': {'and': 3.4757334309663976, 'computer': 4.475733430966398, ',': 2.1538053360790355, 'graph': 4.475733430966398}, 'graph': {'science': 4.475733430966398, ',': 2.1538053360790355, 'theory': 4.475733430966398, 'is': 2.8907709302452416}, 'theory': {',': 2.1538053360790355, 'graph': 4.475733430966398, 'is': 2.8907709302452416, 'the': 2.4757334309663976}, 'the': {'theory': 2.4757334309663976, 'is': 0.8907709302452417, 'study': 2.4757334309663976, 'of': 0.47573343096639775, 'which': 1.4757334309663979, 'model': 1.4757334309663979, 'synapses': 3.4757334309663976, 'in': 2.4757334309663976, 'brain': 2.6683785089087935, '.': 1.4757334309663979, 'connections': 1.4757334309663979, ',': 0.15380533607903546}, 'study': {'is': 2.8907709302452416, 'the': 2.4757334309663976, 'of': 2.4757334309663976, 'graphs': 4.475733430966398}, 'of': {'the': 0.47573343096639775, 'study': 2.4757334309663976, 'graphs': 2.4757334309663976, ',': 0.15380533607903546, 'network': 0.5688428353578793, 'consists': 2.4757334309663976, 'connected': 1.4757334309663979, 'units': 2.4757334309663976, 'comprehensive': 2.4757334309663976, 'map': 2.4757334309663976, 'neural': 1.6683785089087935, 'connections': 1.4757334309663979, 'be': 2.4757334309663976, 'thought': 2.4757334309663976, 'as': 2.4757334309663976, 'its': 2.4757334309663976}, 'graphs': {'study': 4.475733430966398, 'of': 2.4757334309663976, ',': 2.1538053360790355, 'which': 3.4757334309663976}, 'which': {'graphs': 3.4757334309663976, ',': 2.1538053360790355, 'are': 3.4757334309663976, 'mathematical': 3.4757334309663976, 'edges': 3.4757334309663976, 'model': 2.4757334309663976, 'the': 1.4757334309663979}, 'are': {',': 2.1538053360790355, 'which': 3.4757334309663976, 'mathematical': 4.475733430966398, 'structures': 4.475733430966398}, 'mathematical': {'which': 3.4757334309663976, 'are': 4.475733430966398, 'structures': 4.475733430966398, 'used': 4.475733430966398}, 'structures': {'are': 4.475733430966398, 'mathematical': 4.475733430966398, 'used': 4.475733430966398, 'to': 4.475733430966398}, 'used': {'mathematical': 4.475733430966398, 'structures': 4.475733430966398, 'to': 4.475733430966398, 'model': 3.4757334309663976}, 'to': {'structures': 4.475733430966398, 'used': 4.475733430966398, 'model': 3.4757334309663976, 'pairwise': 4.475733430966398}, 'model': {'used': 3.4757334309663976, 'to': 3.4757334309663976, 'pairwise': 3.4757334309663976, 'relations': 3.4757334309663976, ',': 1.1538053360790355, 'which': 2.4757334309663976, 'the': 1.4757334309663979, 'synapses': 3.4757334309663976}, 'pairwise': {'to': 4.475733430966398, 'model': 3.4757334309663976, 'relations': 4.475733430966398, 'between': 3.4757334309663976}, 'relations': {'model': 3.4757334309663976, 'pairwise': 4.475733430966398, 'between': 3.4757334309663976, 'objects': 4.890770930245242}, 'objects': {'relations': 4.890770930245242, 'between': 3.8907709302452416, '.': 3.8907709302452416}, 'neural': {'A': 3.0834160081876374, 'network': 1.761487913300275, 'consists': 3.6683785089087935, 'map': 3.6683785089087935, 'of': 1.6683785089087935, 'connections': 2.6683785089087935, 'in': 2.0834160081876374}, 'consists': {'neural': 3.6683785089087935, 'network': 2.5688428353578794, 'of': 2.4757334309663976, 'connected': 3.4757334309663976}, 'connected': {'consists': 3.4757334309663976, 'of': 1.4757334309663979, 'units': 3.4757334309663976, 'or': 2.4757334309663976, 'artificial': 3.4757334309663976, 'neurons': 3.4757334309663976, 'by': 3.4757334309663976, 'edges': 3.4757334309663976}, 'units': {'of': 2.4757334309663976, 'connected': 3.4757334309663976, 'or': 3.4757334309663976, 'nodes': 4.475733430966398}, 'nodes': {'units': 4.475733430966398, 'or': 3.4757334309663976, 'called': 4.475733430966398, 'artificial': 4.475733430966398}, 'called': {'or': 3.4757334309663976, 'nodes': 4.475733430966398, 'artificial': 4.475733430966398, 'neurons': 4.475733430966398}, 'artificial': {'nodes': 4.475733430966398, 'called': 4.475733430966398, 'neurons': 4.475733430966398, 'connected': 3.4757334309663976}, 'neurons': {'called': 4.475733430966398, 'artificial': 4.475733430966398, 'connected': 3.4757334309663976, 'by': 4.475733430966398}, 'by': {'neurons': 4.475733430966398, 'connected': 3.4757334309663976, 'edges': 4.475733430966398, ',': 2.1538053360790355}, 'edges': {'connected': 3.4757334309663976, 'by': 4.475733430966398, ',': 2.1538053360790355, 'which': 3.4757334309663976}, 'synapses': {'model': 3.4757334309663976, 'the': 3.4757334309663976, 'in': 2.8907709302452416}, 'brain': {'in': 3.0834160081876374, 'the': 2.6683785089087935, '.': 2.6683785089087935, ',': 1.3464504140214313, 'and': 2.6683785089087935}, 'connectome': {'A': 4.3058084295240855, 'is': 3.3058084295240855, 'a': 3.3058084295240855}, 'comprehensive': {'is': 2.8907709302452416, 'a': 2.8907709302452416, 'map': 4.475733430966398, 'of': 2.4757334309663976}, 'map': {'a': 2.8907709302452416, 'comprehensive': 4.475733430966398, 'of': 2.4757334309663976, 'neural': 3.6683785089087935}, 'may': {',': 2.1538053360790355, 'and': 3.4757334309663976, 'be': 4.475733430966398, 'thought': 4.475733430966398}, 'be': {'and': 3.4757334309663976, 'may': 4.475733430966398, 'thought': 4.475733430966398, 'of': 2.4757334309663976}, 'thought': {'may': 4.475733430966398, 'be': 4.475733430966398, 'of': 2.4757334309663976, 'as': 4.475733430966398}, 'as': {'thought': 4.475733430966398, 'of': 2.4757334309663976, 'its': 4.475733430966398, \"'wiring\": 4.475733430966398}, 'its': {'of': 2.4757334309663976, 'as': 4.475733430966398, \"'wiring\": 4.475733430966398, 'diagram': 4.475733430966398}, \"'wiring\": {'as': 4.475733430966398, 'its': 4.475733430966398, 'diagram': 4.475733430966398, \"'\": 4.890770930245242}, 'diagram': {'its': 4.475733430966398, \"'wiring\": 4.475733430966398, \"'\": 4.890770930245242, '.': 3.4757334309663976}, \"'\": {\"'wiring\": 4.890770930245242, 'diagram': 4.890770930245242, '.': 3.8907709302452416}}\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import copy\n",
        "\n",
        "word_feature_ppmi = copy.deepcopy(word_feature_counts)\n",
        "print(word_feature_ppmi)\n",
        "\n",
        "for word in word_feature_counts.keys():\n",
        "    nested_dict = word_feature_ppmi.get(word,{})\n",
        "    for feature in nested_dict: \n",
        "        \n",
        "        wf = nested_dict[feature]\n",
        "        t = total\n",
        "\n",
        "        w_freq = words[word]\n",
        "        f_freq = features[feature]\n",
        "\n",
        "        ppmi = max(0,math.log2((wf*t)/(w_freq*f_freq)))\n",
        "\n",
        "        nested_dict[feature] = ppmi # direct reference update\n",
        "    \n",
        "print(word_feature_ppmi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rpFUxf_5rUD"
      },
      "source": [
        "iii) Define a function that will take two tokens and return their cosine similarity, based on the PPMI representation you have just constructed. Base your calculation on the expression below using dot products.\n",
        "\n",
        "$$cos \\left( w_1, w_2 \\right) = \\frac{w_1 \\cdot w_2}{\\sqrt{w_1 \\cdot w_1 \\times w_2 \\cdot w_2}}$$\n",
        "\n",
        "Apply this function to the tokens `\"semantic\"` and `\"neural\"`.\n",
        "\n",
        "(7 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Qyk0Vav8vVEh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3609308711807981"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def dot_product(v1, v2):\n",
        "    dot = 0\n",
        "    for feature, value in v1.items():\n",
        "        dot += value * v2.get(feature, 0)\n",
        "    return dot\n",
        "\n",
        "def cosine_sim(v1, v2):\n",
        "    numerator = dot_product(v1, v2)\n",
        "    denominator = math.sqrt(dot_product(v1, v1) * dot_product(v2, v2))\n",
        "    \n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    return numerator / denominator\n",
        "\n",
        "cosine_sim(\n",
        "    word_feature_ppmi.get(\"semantic\", {}),\n",
        "    word_feature_ppmi.get(\"neural\", {})\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGNfRJNfHg0m"
      },
      "source": [
        "d) Distributional representations are useful for identifying relations of semantic similarity between words. Name three other types of semantic relationship between words (such as might be found in WordNet) and give an example for each.\n",
        "\n",
        "(6 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWXF29l1H9pt"
      },
      "source": [
        "Distribution represnetations are an unsupervised approach to semantic similarity between words. There are no input output labels, the information is contained and extracted from the input itself. On the other hands, we have Lexical Semantics which is a human-crafted, supervised approach, like that of WordNet which is a hierarchical dictionary. The structure of WordNet is built from 3, though not only 3, lexical semantics known as Synonyms, Hypernyms and Hyponyms. \n",
        "\n",
        "A Synonym is a word that means the exact same as another. A true Synonym can be used in place of another and not change the meaning of the sentence, though there may be contexual clues about meaning hidden in usage of a particular Synonym. \n",
        "\n",
        "Hypernyms represent parent or root words. It is the broader, general category of a collection of Synonyms (synset in WordNet). An example would be Feline is the Hypernym of Cat, Pather, Lion, etc. \n",
        "\n",
        "Hyponym is the inverse of this. It is the most specific, finer grain category derived from the Hypernym. Hypernym and Hyponym are not synonymous as Feline is place of Lion would loose too much information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elOXn_crjgPX"
      },
      "source": [
        "e)\n",
        "\n",
        "i) Apply stop word removal to the original list of sentences, recompute token co-occurrence counts, convert these counts to PPMI values and recompute the similarity of `\"semantic\"` and `\"neural\"`.\n",
        "\n",
        "(5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "GIzi55b5kMQE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['semantic', 'network', ',', 'frame', 'network', 'knowledge', 'base', 'represents', 'semantic', 'connections', 'concepts', 'network', '.'], ['mathematics', 'computer', 'science', ',', 'graph', 'theory', 'study', 'graphs', ',', 'mathematical', 'structures', 'used', 'model', 'pairwise', 'relations', 'objects', '.'], ['neural', 'network', 'consists', 'connected', 'units', 'nodes', 'called', 'artificial', 'neurons', 'connected', 'edges', ',', 'model', 'synapses', 'brain', '.'], ['connectome', 'comprehensive', 'map', 'neural', 'connections', 'brain', ',', 'may', 'thought', \"'wiring\", 'diagram', \"'\", '.']]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.16441506233767836"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_sentences = [[w for w in sent if w.lower() not in stops] for sent in sentences]\n",
        "print(filtered_sentences)\n",
        "\n",
        "# CO-OCCURANCE\n",
        "word_feature_counts = co_occurance(filtered_sentences)\n",
        "\n",
        "# FREQS\n",
        "tokens = set([token for sentence in filtered_sentences for token in sentence])\n",
        "\n",
        "words = {}\n",
        "features = {}\n",
        "total = 0\n",
        "\n",
        "for word in tokens:\n",
        "    current_features = word_feature_counts.get(word,{})\n",
        "    words[word] = sum(current_features.values())\n",
        "\n",
        "    for feature in current_features:\n",
        "        features[feature] = features.get(feature,0)+current_features[feature]\n",
        "\n",
        "total += sum(words.values())\n",
        "\n",
        "#PPMI\n",
        "word_feature_ppmi = copy.deepcopy(word_feature_counts)\n",
        "\n",
        "for word in word_feature_counts.keys():\n",
        "    nested_dict = word_feature_ppmi.get(word,{})\n",
        "    for feature in nested_dict: \n",
        "        \n",
        "        wf = nested_dict[feature]\n",
        "        t = total\n",
        "\n",
        "        w_freq = words[word]\n",
        "        f_freq = features[feature]\n",
        "\n",
        "        ppmi = max(0,math.log2((wf*t)/(w_freq*f_freq)))\n",
        "\n",
        "        nested_dict[feature] = ppmi\n",
        "    \n",
        "cosine_sim(\n",
        "    word_feature_ppmi.get(\"semantic\", {}),\n",
        "    word_feature_ppmi.get(\"neural\", {})\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSeKA_HTmOnF"
      },
      "source": [
        "ii) Explain the difference between the two cosine similarity values, with and without stopword removal, and discuss which approach is preferable as a measure of semantic similarity.\n",
        "\n",
        "(5 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YkFh7XfnB0P"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_exam",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
