# [Week 10 - Attention](https://canvas.sussex.ac.uk/courses/31315/pages/week-10-attention?module_item_id=1445753)
This week, we will have a look at attention in the context of neural networks.  There will also be a guest lecture from Prof Daniel RoggenLinks to an external site. (Research Scientist at Google) who will share about his experience developing wearable technologies and AI in both academia and industry.

### Learning outcomes
By the end of this unit of teaching and learning, you will know the fundamentals of transformers, which is the state of art in attention mechanisms. You will also have some insight into work as an AI scientist/engineer.

---

## Content
- [x] [Slides](https://canvas.sussex.ac.uk/courses/31315/files/5344202?wrap=1)
- [x] [Guest Slides](https://canvas.sussex.ac.uk/courses/31315/files/5638722?wrap=1)
- [x] [Lecture](https://sussex.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=795b8ade-8bcb-4bc1-9002-b2b100e4c2a3)
- [x] [Lecture Notes](https://github.com/LukeBirkett/study-planner/blob/main/934G5_Machine_Learning/week_10/ML_W10_Attention.pdf)
 
## Readings
#### Additional
- [x] A Zhang, ZC Lipton, M Li, AJ Smola. Dive into Deep Learning. 2021. [Link](https://readinglists.sussex.ac.uk/leganto/nui/citation/20811019870002461?institute=44SUS_INST&auth=SAML) [Chapter 11] [Notes](https://github.com/LukeBirkett/study-planner/blob/main/934G5_Machine_Learning/week_10/zhang_attention.pdf)

#### Optional 
- [ ] [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [ ] [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044)
- [ ] [Neural Machine Translation by Jointly Learning to Align and Translate](https://readinglists.sussex.ac.uk/leganto/nui/citation/23771559560002461?institute=44SUS_INST&auth=SAML)
- [ ] [Attention is all you need](https://readinglists.sussex.ac.uk/leganto/nui/citation/23771559550002461?institute=44SUS_INST&auth=SAML)
- [ ] [Zero-Shot Text-to-Image Generation](https://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf)

#### Self
- [x] [Sequence-to-Sequence (seq2seq) Encoder-Decoder Neural Networks](https://www.youtube.com/watch?v=L8HKweZIOmg&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=20) [Notes](https://github.com/LukeBirkett/study-planner/blob/main/934G5_Machine_Learning/week_10/encoder-decoder.pdf)
- [x] [Attention for Neural Networks](https://www.youtube.com/watch?v=PSs6nxngL6k&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=21&t=9s) [Notes](https://github.com/LukeBirkett/study-planner/blob/main/934G5_Machine_Learning/week_10/attention.pdf)
- [x] [Transformer Neural Networks](https://www.youtube.com/watch?v=zxQyTK8quyY&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=22) [Notes](https://github.com/LukeBirkett/study-planner/blob/main/934G5_Machine_Learning/week_10/transformers.pdf)
- [x] [Decoder-Only Transformers](https://www.youtube.com/watch?v=bQ5BoolX9Ag&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=23) [Notes](https://github.com/LukeBirkett/study-planner/blob/main/934G5_Machine_Learning/week_10/decoder.pdf)
- [x] [Encoder-Only Transformers](https://www.youtube.com/watch?v=GDN649X_acE&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=24) [Notes](https://github.com/LukeBirkett/study-planner/blob/main/934G5_Machine_Learning/week_10/encoder.pdf)
