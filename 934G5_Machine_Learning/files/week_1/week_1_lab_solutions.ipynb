{"cells":[{"cell_type":"markdown","source":["# Learning outcomes"],"metadata":{"id":"fatNy6SSaIGp"}},{"cell_type":"markdown","source":["When you've worked through the tasks and exercises in this notebook, you'd have\n","\n","* built a machine learning model using a standard software library;\n","* run experiments to explore effects of regularization and data augmentation."],"metadata":{"id":"tfzIAcpBaNYt"}},{"cell_type":"markdown","metadata":{"id":"eFg7aL7BqPex"},"source":["# Objectives\n","\n"]},{"cell_type":"markdown","source":["\n","* To introduce you to 2 of the main (Python-based) software libraries we'll be using throughout the module:\n","> 1. scikit-learn (https://scikit-learn.org/stable/) - one of the well-used machine learning libraries.\n","> 2. numpy (https://numpy.org/) -  a very common library for mathematical functions.\n","\n",">**Note**: It is your responsibility as a machine learning scientist to read documentations for any library function you use and to thoroughly understand what it is doing, if it validly serves your purpose, and which of its parameters you need to consider.\n","\n","* To see some of the basic components of machine learning first hand - training data, test (i.e. unseen) data, and machine learning model (with weights and biases being the primary parameters that specify a model for most types of models):\n",">1. Data - today, we'll use the Iris dataset. You can read more about it here: https://scikit-learn.org/stable/datasets/toy_dataset.html.\n",">2. Model - we'll explore linear regression, regularization, and augmentation, which we covered in the Week 1 mini-videos\n"],"metadata":{"id":"g8f4eGPFVUaz"}},{"cell_type":"markdown","metadata":{"id":"hsv7Ks_9qPe1"},"source":["# Section 1 - Set up imports and random number generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bibkBao6qPe1"},"outputs":[],"source":["%matplotlib inline\n","\n","import numpy\n","from sklearn import linear_model\n","from sklearn.metrics import mean_squared_error\n","import copy\n","\n","\n","# Set up the random number generator\n","rng =  numpy.random.default_rng()"]},{"cell_type":"markdown","source":["# Section 2 - Load the Iris dataset"],"metadata":{"id":"VHYf0-SDbXFO"}},{"cell_type":"code","source":["from sklearn import datasets\n","\n","iris_data, iris_labels = datasets.load_iris(return_X_y=True, as_frame=False)\n","\n","print(\"The dimensions of the Iris feature matrix\", iris_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkpcYHZVbb_v","executionInfo":{"status":"ok","timestamp":1752851182133,"user_tz":-60,"elapsed":23,"user":{"displayName":"Temi Olugbade","userId":"13847897350210385961"}},"outputId":"0ec7b4b9-2369-494d-b8e3-d0248e8f5f13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The dimensions of the Iris feature matrix (150, 4)\n"]}]},{"cell_type":"markdown","source":["# Section 3 - Explore the Iris dataset"],"metadata":{"id":"FRn04pDMbl8D"}},{"cell_type":"markdown","source":["* Read about the Iris dataset here: https://scikit-learn.org/stable/datasets/toy_dataset.html\n","* What type of labels does it have (real continuous or categorical)? What kind of machine learning task is this type of label suited to, i.e. classification or regression?\n","* What is the feature dimensionality of the dataset, i.e. the number of features?\n","* How many data instances are there? What is the distribution of instances across classes?\n","\n","\n","\n","---\n","\n","\n","* Select one of the features. What association does the selected feature have with the iris classes, with respect to differentiating between them (Hint - use a search engine to read about Iris Setosa, Iris Versicolour, and Iris Virginica plant)?\n","* What factors do you think limited the number of data instances per class?\n","* How do you think the data was collected? What implication would this have for real world deployment of a model for automatic detection of iris classes based on this dataset?\n","* How do you think it was labelled? What kind of challenge might this pose for collection of more training data (and labels) for automatic detection of iris classes?"],"metadata":{"id":"dbv3Jmu3bqdK"}},{"cell_type":"markdown","source":["**Solution**"],"metadata":{"id":"Ot-aeQOAdgeN"}},{"cell_type":"markdown","source":["* The dataset has categorical labels, and so, classification is more appropriate for it, rather than regression.\n","* The Iris dataset has 4 features.\n","* There are 150 data instances, and an equal number of instances per class.\n","\n","The last 4 sets of questions are open questions. You can discuss your thoughts with your peers or with a TA.\n","\n","\n","> **Note** - In good practice, machine learning (ML) development does not start with thinking about the learning algorithm but must begin with thinking about the real world problem that ML aims to address, as well as the dataset associated with the problem. You can listen here to an overview of the data-centric AI movement that encourages the approach of exploring and 'working on' the data (rather than only focusing on crafting the ML architecture) at the centre of developing a model for a given problem: https://datacentricai.org/blog/opening-remarks/."],"metadata":{"id":"jNo05YiYdfT4"}},{"cell_type":"markdown","source":["# Section 4 - Split into training and test sets"],"metadata":{"id":"P4dgDkeob0fD"}},{"cell_type":"code","source":["\n","# Randomly split the data into 50:50 training:test sets\n","rand_inds = numpy.arange(iris_labels.shape[0],)\n","rng.shuffle(rand_inds)\n","split_point = int(0.5*iris_labels.shape[0])\n","\n","training_data_x = iris_data[rand_inds[0:split_point], :]\n","training_labels_y = iris_labels[rand_inds[0:split_point]]\n","test_data_x = iris_data[rand_inds[split_point:iris_labels.shape[0]], :]\n","test_labels_y = iris_labels[rand_inds[split_point:iris_labels.shape[0]]]\n","\n","print(\"Size of the training data:\", training_data_x.shape)\n","print(\"Size of the ttest data:\", test_data_x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJYJnYNyb1E_","executionInfo":{"status":"ok","timestamp":1752851184321,"user_tz":-60,"elapsed":31,"user":{"displayName":"Temi Olugbade","userId":"13847897350210385961"}},"outputId":"daa1d6de-b08f-4b54-b236-c6f78796895a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of the training data: (75, 4)\n","Size of the ttest data: (75, 4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"SS3p1FHFqPe4"},"source":["# Section 5 - Train linear regression model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VG1xu2MqPe5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752851186379,"user_tz":-60,"elapsed":23,"user":{"displayName":"Temi Olugbade","userId":"13847897350210385961"}},"outputId":"da41a44b-1148-4491-8185-9b7c95e551f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[5.7 3.  4.2 1.2]\n"," [7.9 3.8 6.4 2. ]\n"," [5.5 2.4 3.8 1.1]\n"," [4.4 3.  1.3 0.2]\n"," [4.8 3.  1.4 0.3]\n"," [6.7 3.  5.2 2.3]\n"," [5.7 2.9 4.2 1.3]\n"," [5.5 2.5 4.  1.3]\n"," [7.7 2.6 6.9 2.3]\n"," [4.9 3.  1.4 0.2]\n"," [5.1 3.5 1.4 0.2]\n"," [5.5 2.4 3.7 1. ]\n"," [6.7 3.1 4.4 1.4]\n"," [5.6 3.  4.5 1.5]\n"," [5.4 3.4 1.5 0.4]\n"," [4.8 3.  1.4 0.1]\n"," [4.8 3.4 1.9 0.2]\n"," [7.7 3.8 6.7 2.2]\n"," [6.4 3.2 5.3 2.3]\n"," [7.6 3.  6.6 2.1]\n"," [6.5 3.2 5.1 2. ]\n"," [5.2 3.4 1.4 0.2]\n"," [5.9 3.  5.1 1.8]\n"," [6.6 3.  4.4 1.4]\n"," [4.6 3.4 1.4 0.3]\n"," [4.4 2.9 1.4 0.2]\n"," [6.9 3.1 5.1 2.3]\n"," [4.7 3.2 1.3 0.2]\n"," [5.8 2.7 5.1 1.9]\n"," [4.9 2.4 3.3 1. ]\n"," [6.5 3.  5.2 2. ]\n"," [6.3 2.7 4.9 1.8]\n"," [6.7 3.3 5.7 2.5]\n"," [5.6 2.5 3.9 1.1]\n"," [6.1 3.  4.9 1.8]\n"," [4.9 3.6 1.4 0.1]\n"," [4.6 3.2 1.4 0.2]\n"," [7.7 3.  6.1 2.3]\n"," [5.5 4.2 1.4 0.2]\n"," [5.6 3.  4.1 1.3]\n"," [6.7 3.1 5.6 2.4]\n"," [5.7 2.8 4.5 1.3]\n"," [5.5 2.3 4.  1.3]\n"," [6.4 2.7 5.3 1.9]\n"," [4.4 3.2 1.3 0.2]\n"," [5.1 3.4 1.5 0.2]\n"," [5.1 2.5 3.  1.1]\n"," [5.4 3.9 1.3 0.4]\n"," [6.8 2.8 4.8 1.4]\n"," [6.4 2.8 5.6 2.1]\n"," [6.1 2.9 4.7 1.4]\n"," [4.6 3.6 1.  0.2]\n"," [5.1 3.7 1.5 0.4]\n"," [5.8 2.7 5.1 1.9]\n"," [5.6 2.9 3.6 1.3]\n"," [6.4 3.1 5.5 1.8]\n"," [6.5 3.  5.8 2.2]\n"," [6.9 3.1 4.9 1.5]\n"," [6.1 2.6 5.6 1.4]\n"," [7.3 2.9 6.3 1.8]\n"," [4.7 3.2 1.6 0.2]\n"," [6.  3.4 4.5 1.6]\n"," [5.  3.3 1.4 0.2]\n"," [4.9 2.5 4.5 1.7]\n"," [6.3 2.5 5.  1.9]\n"," [5.8 4.  1.2 0.2]\n"," [5.4 3.  4.5 1.5]\n"," [6.7 3.3 5.7 2.1]\n"," [6.9 3.1 5.4 2.1]\n"," [6.1 2.8 4.  1.3]\n"," [5.8 2.8 5.1 2.4]\n"," [5.4 3.7 1.5 0.2]\n"," [5.6 2.8 4.9 2. ]\n"," [5.1 3.8 1.9 0.4]\n"," [5.9 3.2 4.8 1.8]]\n","\n","The weight (w): [-0.09279415 -0.07641343  0.17415337  0.71879044]\n","The bias (b): 0.2774695677305642\n","\n","Mean squared error (error on training data): 0.04 \n"]}],"source":["\n","print(training_data_x)\n","\n","# Train a linear regression model\n","lr_model = linear_model.LinearRegression()\n","lr_model.fit(training_data_x, training_labels_y)\n","print(\"\\nThe weight (w):\",  lr_model.coef_)\n","print(\"The bias (b):\",  lr_model.intercept_)\n","\n","# Check the performance of the model on the data used to train it\n","training_pred_y = lr_model.predict(training_data_x)\n","print(\"\\nMean squared error (error on training data): %.2f \" % mean_squared_error(training_labels_y, training_pred_y))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x4cKOMKXqPe7"},"source":["# Section 6 - Explore reproducibility\n"]},{"cell_type":"markdown","source":["* Run Sections 4 and 5 code multiple times (e.g. 3 times) - each time, copy and paste your outputs (training data, weight, bias, mean squared error) somewhere so that you can compare outputs across the multiple runs. What do you notice? What is the implication, and how could you address it?"],"metadata":{"id":"KIPtsaBlfeDi"}},{"cell_type":"markdown","source":["**Solution**"],"metadata":{"id":"KVh7P-SAgL7d"}},{"cell_type":"markdown","source":["* As no random seed is set in Section 1, the random data generated in each run of Section 5 code is different, and this affects everything else in the section.\n","* To address this, you need to replace\n","\n",">`rng = numpy.random.default_rng(random_seed)`\n","\n","  in Section 1 with\n","\n",">```\n","> random_seed = 1\n","> rng = numpy.random.default_rng(random_seed)\n","\n","* You can set *random_seed* to any integer.\n","\n","* There is random number generation at several different levels within available software libraries for building and evaluating machine learning models. It is your responsibility as a machine learning scientist to take note of the points in the library you are using (or in your code) where randomness is introduced and address it so as to have reproducible models and model evaluation results.\n","\n","* Addressing it is usually as simple as applying a random seed (e.g. for both Python and the specific machine learning library functions being used). However, note that some machine learning libraries, e.g. Tensorflow 2.11 GPU, have in them randomness that cannot be completely eliminated by the library user. You should look for the section on reproducibility in the documentation for the machine learning library that you want to use to be sure how randomness may (or not) be addressed for the given library."],"metadata":{"id":"kcLunje7gOP5"}},{"cell_type":"markdown","source":[],"metadata":{"id":"h6GCeeJe4LHM"}},{"cell_type":"markdown","source":["# Section 7 - Explore the linear regression model"],"metadata":{"id":"XAmcTJuGTa7_"}},{"cell_type":"markdown","source":["\n","\n","*   Why are the 4 weights for the model?\n","*   Why is there one bias for the model?\n","\n"],"metadata":{"id":"qDMXHuw5TaFe"}},{"cell_type":"markdown","source":["**Solution**\n","\n","The number of weights matches the number of features, whereas there is only one bias per basic linear model."],"metadata":{"id":"fuW3bd6zVY40"}},{"cell_type":"markdown","source":["# Section 8 - Explore the effects of L1 and L2 regularization"],"metadata":{"id":"eXBbTaUsWp9x"}},{"cell_type":"markdown","source":["*   Train and evaluate a linear regression model\n","  * See above examples; also see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n","*   Train and evaluate a linear regression model with L2 regularization\n","  * Set alpha to 0.5\n","  * See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)\n","*   Train and evaluate a linear regression model with L1 regularization\n","  * Set alpha to 0.5\n","  * See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)\n","*   What are the effects of regularization that you notice?\n","  * See Week 1 mini-videos\n","  * Hint - Compare the weights (and bias) and the errors.\n","\n","\n"],"metadata":{"id":"Kgt_0IwhW2HZ"}},{"cell_type":"markdown","source":["**Solution**"],"metadata":{"id":"9irrZJxwXqgD"}},{"cell_type":"code","source":["\n","# Get the weights and bias of trained linear regression model from Section 5\n","print(\"\\nThe weights (w):\",  lr_model.coef_)\n","print(\"The bias (b):\",  lr_model.intercept_)\n","\n","alpha = 0.5\n","\n","# Train a new linear regression model with L2 regularization\n","# Get its weights and bias\n","lr_model_L2 = linear_model.Ridge(alpha=alpha)\n","lr_model_L2.fit(training_data_x, training_labels_y)\n","print(\"\\nThe weights (w) - L2 reg:\",  lr_model_L2.coef_)\n","print(\"The bias (b) - L2 reg:\",  lr_model_L2.intercept_)\n","\n","# Train a separate linear regression model with L1 regularization\n","# Get its weights and bias\n","lr_model_L1 = linear_model.Lasso(alpha=alpha)\n","lr_model_L1.fit(training_data_x, training_labels_y)\n","print(\"\\nThe weights (w) - L1 reg:\",  lr_model_L1.coef_)\n","print(\"The bias (b) - L1 reg:\",  lr_model_L1.intercept_)\n","print()\n","\n","\n","# Check the performance of the models on the data used to train them\n","training_pred_y = lr_model.predict(training_data_x)\n","training_pred_y_L2 = lr_model_L2.predict(training_data_x)\n","training_pred_y_L1 = lr_model_L1.predict(training_data_x)\n","print(\"\\nMean squared error (training error): %.2f \" % mean_squared_error(training_labels_y, training_pred_y))\n","print(\"Mean squared error (training error) - L2: %.2f \" % mean_squared_error(training_labels_y, training_pred_y_L2))\n","print(\"Mean squared error (training error) - L1: %.2f \" % mean_squared_error(training_labels_y, training_pred_y_L1))\n","\n","# Check the performance of the models on test data not seen by the models in training\n","test_pred_y = lr_model.predict(test_data_x)\n","test_pred_y_L2 = lr_model_L2.predict(test_data_x)\n","test_pred_y_L1 = lr_model_L1.predict(test_data_x)\n","print(\"\\nMean squared error (error on test data): %.2f \" % mean_squared_error(test_labels_y, test_pred_y))\n","print(\"Mean squared error (error on test data) - L2: %.2f \" % mean_squared_error(test_labels_y, test_pred_y_L2))\n","print(\"Mean squared error (error on test data) - L1: %.2f \" % mean_squared_error(test_labels_y, test_pred_y_L1))\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XJZ3hg7YpIO","executionInfo":{"status":"ok","timestamp":1752851189902,"user_tz":-60,"elapsed":101,"user":{"displayName":"Temi Olugbade","userId":"13847897350210385961"}},"outputId":"f5d02bae-cccd-439d-ccd9-4213898e1d08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The weights (w): [-0.09279415 -0.07641343  0.17415337  0.71879044]\n","The bias (b): 0.2774695677305642\n","\n","The weights (w) - L2 reg: [-0.09448983 -0.07012257  0.20941161  0.63727928]\n","The bias (b) - L2 reg: 0.23588894051204656\n","\n","The weights (w) - L1 reg: [ 0.         -0.          0.28847935  0.        ]\n","The bias (b) - L1 reg: -0.032247321880808366\n","\n","\n","Mean squared error (training error): 0.04 \n","Mean squared error (training error) - L2: 0.04 \n","Mean squared error (training error) - L1: 0.15 \n","\n","Mean squared error (error on test data): 0.05 \n","Mean squared error (error on test data) - L2: 0.05 \n","Mean squared error (error on test data) - L1: 0.13 \n","\n"]}]},{"cell_type":"markdown","source":["* With L2 regularization, you should see smaller weight values compared to no regularization.\n","* With L1 regularization, you should see more weights that are value zero, compared to L2 and no regularization.\n","* Remember from Week 1 lecture that regularization is a strategy for reducing overfitting to training data and making it more generalizable to unseen data (i.e. represented as the test data). See Week 1 lecture and Week 1 suggested readings for more on overfitting and regularization. As the experiments above are based on randomly generated data (for which there is likely no relationship between the features and labels that can be learnt by a model), you may not notice any effect of regularization on the mean squared errors."],"metadata":{"id":"808uEO98bdsW"}},{"cell_type":"markdown","source":["# Section 9 - Explore the effect of alpha on L1 and L2 regularization"],"metadata":{"id":"NS8-6nL6dzi3"}},{"cell_type":"markdown","source":["* Using your code in Section 8, compare the effect of multiple alpha values, e.g. alpha = 0.000000001, 0.0001, 0.1, on regularization."],"metadata":{"id":"avIg1Nmsd7hq"}},{"cell_type":"markdown","source":["**Solution**"],"metadata":{"id":"vcQV0blGjeSJ"}},{"cell_type":"markdown","source":["You will find that higher values of alpha increase the effect of regularization, and alpha value of zero corresponds to no regularization applied."],"metadata":{"id":"kgvg53Fjjhx8"}},{"cell_type":"markdown","source":["# Section 10 - Explore the effect of data augmentation"],"metadata":{"id":"IDlB-YXs4ryU"}},{"cell_type":"markdown","source":["*   Train and evaluate a linear regression model\n","  * See above examples; also see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n","*   Train and evaluate a linear regression model with data augmentation applied to the training data\n","  * See Week 1 mini-videos\n","  * You could try adding randomly setting feature values to '-1'\n","  * You could try multiple augmentation intensities, e.g. probability of 0.01, 0.1, and 0.5 of setting to '-1'\n","*   Train and evaluate a linear regression model with another data augmentation applied to the training data\n","  * See Week 1 mini-videos\n","  * You could try randomly adding Gaussian noise to the data\n","  * You could try multiple augmentation intensities, e.g. Gaussian noise of standard deviation = 0.01, 0.1, and 0.5\n","*   What are the effects of augmentation that you notice?\n"],"metadata":{"id":"rEkGrdoJ4-ol"}},{"cell_type":"markdown","source":["**Solution**\n","\n","The solution below tests data augmentation setting to -1 with probability of 0.01 as well as adding Gaussian noise of standard deviation of 0.01.\n","\n","With real world data, depending on the data and the specific data augmentation method and settings appropriate for it, you would expect a model trained with augmented data to be more generalizable (see Week 1 mini-videos for what this means)."],"metadata":{"id":"x_RhCvL96cQh"}},{"cell_type":"code","source":["\n","# Randomly select training data instances and dimensions to 'drop out'by setting to -1\n","# -- Generate random numbers between 0 and 1 for each dimension of each data instance\n","# -- Select dimensions (within data instances) with generated random number <= thresh\n","# -- And set their values to -1\n","thresh = 0.01\n","dropout_ind = rng.random(size=training_data_x.shape)\n","training_data_x_droppedout = copy.deepcopy(training_data_x)\n","numpy.place(training_data_x_droppedout, dropout_ind<=thresh, -1)\n","\n","\n","print(training_data_x_droppedout)\n","\n","# Train a new linear regression model with the data with the 'drop outs'\n","lr_model_droppedout = linear_model.LinearRegression()\n","lr_model_droppedout.fit(training_data_x_droppedout, training_labels_y)\n","print(\"\\nThe weight (w):\",  lr_model_droppedout.coef_)\n","print(\"The bias (b):\",  lr_model_droppedout.intercept_)\n","\n","\n","\n","\n","# Generate Gaussian noise of standard deviation std\n","# and of the same size as the training data\n","# then add the noise to the features for the training data\n","std = 0.01\n","noise = rng.normal(loc=0.0, scale=std, size=training_data_x.shape)\n","training_data_x_noised = training_data_x + noise\n","\n","\n","print(training_data_x_noised)\n","\n","# Train a new linear regression model with the noised data\n","lr_model_noised = linear_model.LinearRegression()\n","lr_model_noised.fit(training_data_x_noised, training_labels_y)\n","print(\"\\nThe weight (w):\",  lr_model_noised.coef_)\n","print(\"The bias (b):\",  lr_model_noised.intercept_)\n","\n","\n","\n","# Check the performance of the models on the data used to train them\n","training_pred_y_droppedout = lr_model_droppedout.predict(training_data_x)\n","training_pred_y_noised = lr_model_noised.predict(training_data_x_noised)\n","print(\"\\nMean squared error (error on training data): %.2f \" % mean_squared_error(training_labels_y, training_pred_y))\n","print(\"Mean squared error (error on training data) - droppedout: %.2f \" % mean_squared_error(training_labels_y, training_pred_y_droppedout))\n","print(\"Mean squared error (error on training data) - noised: %.2f \" % mean_squared_error(training_labels_y, training_pred_y_noised))\n","\n","# Check the performance of the models on test data not seen by the models in training\n","test_pred_y_droppedout = lr_model_droppedout.predict(test_data_x)\n","test_pred_y_noised = lr_model_noised.predict(test_data_x)\n","print(\"\\nMean squared error (error on test data): %.2f \" % mean_squared_error(test_labels_y, test_pred_y))\n","print(\"Mean squared error (error on test data) - droppedout: %.2f \" % mean_squared_error(test_labels_y, test_pred_y_droppedout))\n","print(\"Mean squared error (error on test data) - noised: %.2f \" % mean_squared_error(test_labels_y, test_pred_y_noised))\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bgcbdExi6_uV","executionInfo":{"status":"ok","timestamp":1752851192791,"user_tz":-60,"elapsed":103,"user":{"displayName":"Temi Olugbade","userId":"13847897350210385961"}},"outputId":"88cb3588-7d03-4c8d-cd2c-544e2451608a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 5.7  3.   4.2  1.2]\n"," [ 7.9  3.8  6.4  2. ]\n"," [ 5.5  2.4  3.8  1.1]\n"," [ 4.4  3.   1.3  0.2]\n"," [-1.   3.   1.4  0.3]\n"," [ 6.7  3.   5.2  2.3]\n"," [ 5.7  2.9  4.2  1.3]\n"," [ 5.5  2.5  4.   1.3]\n"," [ 7.7  2.6  6.9  2.3]\n"," [ 4.9  3.   1.4  0.2]\n"," [ 5.1  3.5  1.4  0.2]\n"," [ 5.5  2.4  3.7  1. ]\n"," [ 6.7  3.1  4.4  1.4]\n"," [ 5.6  3.   4.5  1.5]\n"," [ 5.4  3.4  1.5  0.4]\n"," [ 4.8  3.   1.4  0.1]\n"," [ 4.8  3.4  1.9  0.2]\n"," [ 7.7  3.8  6.7 -1. ]\n"," [ 6.4  3.2  5.3  2.3]\n"," [ 7.6  3.   6.6  2.1]\n"," [ 6.5  3.2  5.1  2. ]\n"," [ 5.2  3.4  1.4  0.2]\n"," [-1.   3.   5.1  1.8]\n"," [ 6.6  3.   4.4  1.4]\n"," [ 4.6  3.4  1.4  0.3]\n"," [ 4.4  2.9  1.4  0.2]\n"," [ 6.9  3.1  5.1  2.3]\n"," [ 4.7  3.2  1.3  0.2]\n"," [ 5.8  2.7  5.1  1.9]\n"," [ 4.9  2.4  3.3  1. ]\n"," [ 6.5  3.   5.2  2. ]\n"," [ 6.3  2.7  4.9  1.8]\n"," [ 6.7  3.3  5.7  2.5]\n"," [ 5.6  2.5  3.9  1.1]\n"," [ 6.1  3.   4.9  1.8]\n"," [ 4.9  3.6  1.4  0.1]\n"," [ 4.6  3.2  1.4  0.2]\n"," [ 7.7  3.   6.1  2.3]\n"," [ 5.5  4.2  1.4  0.2]\n"," [ 5.6  3.   4.1  1.3]\n"," [ 6.7  3.1  5.6  2.4]\n"," [ 5.7  2.8  4.5  1.3]\n"," [ 5.5  2.3  4.   1.3]\n"," [ 6.4  2.7  5.3  1.9]\n"," [ 4.4  3.2  1.3  0.2]\n"," [ 5.1  3.4  1.5  0.2]\n"," [ 5.1  2.5  3.   1.1]\n"," [ 5.4 -1.   1.3  0.4]\n"," [ 6.8  2.8  4.8  1.4]\n"," [ 6.4  2.8  5.6  2.1]\n"," [ 6.1  2.9  4.7  1.4]\n"," [ 4.6  3.6  1.   0.2]\n"," [ 5.1  3.7  1.5  0.4]\n"," [ 5.8  2.7  5.1  1.9]\n"," [ 5.6  2.9  3.6  1.3]\n"," [ 6.4  3.1  5.5  1.8]\n"," [ 6.5  3.   5.8  2.2]\n"," [ 6.9  3.1  4.9  1.5]\n"," [ 6.1  2.6  5.6  1.4]\n"," [ 7.3  2.9  6.3  1.8]\n"," [ 4.7  3.2  1.6  0.2]\n"," [ 6.   3.4  4.5  1.6]\n"," [ 5.   3.3  1.4  0.2]\n"," [ 4.9  2.5  4.5  1.7]\n"," [ 6.3  2.5  5.   1.9]\n"," [ 5.8  4.   1.2  0.2]\n"," [ 5.4  3.   4.5  1.5]\n"," [ 6.7  3.3  5.7  2.1]\n"," [ 6.9  3.1  5.4  2.1]\n"," [ 6.1  2.8  4.   1.3]\n"," [-1.   2.8  5.1  2.4]\n"," [ 5.4  3.7  1.5 -1. ]\n"," [ 5.6  2.8 -1.   2. ]\n"," [ 5.1  3.8  1.9  0.4]\n"," [ 5.9  3.2  4.8  1.8]]\n","\n","The weight (w): [ 0.00157272 -0.00180084  0.22269992  0.47701736]\n","The bias (b): -0.3373722032684363\n","[[5.6855899  3.00541691 4.19008357 1.19117929]\n"," [7.89011112 3.78912371 6.40704039 1.98670536]\n"," [5.49888688 2.38550884 3.79604049 1.10933743]\n"," [4.40134962 2.99578687 1.28218662 0.19596243]\n"," [4.81019283 2.98933335 1.40335307 0.28363885]\n"," [6.70159436 3.00492219 5.19479371 2.3036289 ]\n"," [5.69855023 2.89741546 4.19612828 1.28561951]\n"," [5.49257491 2.47553306 3.99430378 1.28702339]\n"," [7.70654282 2.57829167 6.90464115 2.30848689]\n"," [4.90598345 2.99813641 1.39490271 0.18124229]\n"," [5.09879557 3.50705841 1.40083692 0.19175841]\n"," [5.49151345 2.40231093 3.69679706 0.99525276]\n"," [6.68018482 3.08181016 4.39203637 1.39876671]\n"," [5.59425465 3.00641619 4.50061878 1.49215259]\n"," [5.39454924 3.41414514 1.51908642 0.39641112]\n"," [4.79661557 2.9886244  1.41514757 0.10190073]\n"," [4.79908742 3.41517728 1.90306805 0.20699602]\n"," [7.69849287 3.78465144 6.71202734 2.20389998]\n"," [6.40085152 3.20169703 5.3051694  2.31663941]\n"," [7.60399642 3.00392485 6.60165297 2.12339773]\n"," [6.50009281 3.21136766 5.10521695 1.99753558]\n"," [5.19931906 3.40752454 1.38450748 0.20319031]\n"," [5.88843437 3.00397132 5.09724747 1.79437582]\n"," [6.61182152 2.99808944 4.39632873 1.39877004]\n"," [4.59647941 3.39924737 1.39618301 0.30014254]\n"," [4.38933277 2.90199327 1.40451472 0.1883    ]\n"," [6.90809998 3.10197804 5.10795358 2.31057062]\n"," [4.68511873 3.19302265 1.28846725 0.1914911 ]\n"," [5.80210826 2.70186219 5.08750763 1.91365648]\n"," [4.90119356 2.41848774 3.29148623 1.00906012]\n"," [6.50704783 2.99462305 5.19990899 2.00376013]\n"," [6.29062905 2.69499892 4.89542944 1.79076733]\n"," [6.70022041 3.3083867  5.70397078 2.50112483]\n"," [5.59831372 2.50507687 3.90955024 1.08896725]\n"," [6.10438552 3.00257127 4.89815547 1.80034828]\n"," [4.89552506 3.62916785 1.40254395 0.09425194]\n"," [4.60351276 3.22086829 1.39662455 0.18537362]\n"," [7.719125   3.0154684  6.08304483 2.28585334]\n"," [5.51157064 4.20199089 1.39864915 0.21332429]\n"," [5.6086693  3.01528155 4.09282851 1.3048245 ]\n"," [6.71918902 3.08286319 5.61226691 2.39844865]\n"," [5.69992579 2.80272339 4.49820723 1.29428912]\n"," [5.4915746  2.30375552 3.98020112 1.28942839]\n"," [6.39441093 2.69472031 5.28735375 1.8954294 ]\n"," [4.40090255 3.19179828 1.29263955 0.18779058]\n"," [5.09374706 3.40298388 1.50251082 0.19624073]\n"," [5.11297737 2.4846007  3.01436577 1.10456521]\n"," [5.41848449 3.90370958 1.30621407 0.40605343]\n"," [6.78522359 2.79822966 4.79320434 1.3896416 ]\n"," [6.40112933 2.8060028  5.59676641 2.0906906 ]\n"," [6.11740152 2.9085648  4.69761395 1.40610347]\n"," [4.59504944 3.59056917 1.00390356 0.20235563]\n"," [5.1076863  3.69104665 1.5060841  0.39752299]\n"," [5.79311291 2.69734406 5.08536243 1.90599186]\n"," [5.59041923 2.88506477 3.61492587 1.28017388]\n"," [6.41026437 3.10063553 5.50076759 1.78937501]\n"," [6.50839093 3.01333236 5.80097273 2.19609865]\n"," [6.89218015 3.10763825 4.8851311  1.49723836]\n"," [6.10687309 2.59693199 5.59829874 1.4042812 ]\n"," [7.30007992 2.90178169 6.29870317 1.78744551]\n"," [4.6975152  3.20101139 1.58120565 0.19127882]\n"," [5.99773778 3.39311804 4.50631088 1.58417969]\n"," [4.98237143 3.30188241 1.40582908 0.21178586]\n"," [4.89808509 2.48401355 4.50974681 1.71463505]\n"," [6.31185409 2.48413749 4.99458777 1.90151057]\n"," [5.81152383 3.99215407 1.20035898 0.18779332]\n"," [5.40235424 3.01569357 4.49737009 1.48801632]\n"," [6.696161   3.32726449 5.69224297 2.09921004]\n"," [6.88679122 3.10050571 5.38850294 2.10218065]\n"," [6.09052105 2.78228151 4.0111651  1.30301172]\n"," [5.7873545  2.80875872 5.0944935  2.42297041]\n"," [5.39316249 3.72364214 1.50804978 0.18943652]\n"," [5.584101   2.81435156 4.88329422 2.00408997]\n"," [5.10053422 3.80452748 1.90617844 0.4005481 ]\n"," [5.91027222 3.18037501 4.80772326 1.80571148]]\n","\n","The weight (w): [-0.09151728 -0.07745275  0.17402351  0.71587071]\n","The bias (b): 0.27888453157738635\n","\n","Mean squared error (error on training data): 0.04 \n","Mean squared error (error on training data) - droppedout: 0.05 \n","Mean squared error (error on training data) - noised: 0.04 \n","\n","Mean squared error (error on test data): 0.05 \n","Mean squared error (error on test data) - droppedout: 0.06 \n","Mean squared error (error on test data) - noised: 0.05 \n","\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[],"collapsed_sections":["fatNy6SSaIGp","eFg7aL7BqPex","hsv7Ks_9qPe1","VHYf0-SDbXFO","FRn04pDMbl8D","P4dgDkeob0fD","SS3p1FHFqPe4","x4cKOMKXqPe7","XAmcTJuGTa7_","eXBbTaUsWp9x","NS8-6nL6dzi3","IDlB-YXs4ryU"]}},"nbformat":4,"nbformat_minor":0}