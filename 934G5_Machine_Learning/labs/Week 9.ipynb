{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhcT91UwAajV"
   },
   "source": [
    "# Objectives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYQHmdWBneSr"
   },
   "source": [
    "* To explore transfer learning from Week 9 lecture and apply it in pistachio image classification.\n",
    "> **Remember**: It is your responsibility as a machine learning scientist to read documentations for each library function in the code to thoroughly understand what it is doing, how it serves the purpose highlighted in the code comments, and other parameters that could be set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_joWMUBMn2VC"
   },
   "source": [
    "# Section 1 - Load Pistachio Image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtxWtuzWcP_d"
   },
   "source": [
    "\n",
    "1. <ins>Dataset information</ins>\n",
    "\n",
    "This week, we will use the *Pistachio Image dataset*, which contains raw images and their labels.\n",
    ">See the publication below for some info on the dataset:\n",
    "  * OZKAN IA., KOKLU M. and SARACOGLU R. (2021). Classification of Pistachio Species Using Improved K-NN Classifier. Progress in Nutrition, Vol. 23, N. 2. https://doi.org/10.23751/pn.v23i2.9686.\n",
    "\n",
    "\n",
    "2. <ins>Dataset download</ins>\n",
    "\n",
    "You need to download the data before you can get started. Download from https://www.muratkoklu.com/datasets/. The link for the *Pistachio_Image_Dataset* is in the first table on the page.\n",
    "\n",
    "3. <ins> Dataset upload</ins>\n",
    "\n",
    "You would then have downloaded a *Pistachio_Image_Dataset.zip* file. This file can be uploaded to your Colab directory using the File menu in Google Colab. Once upload is complete, you should be able to see the file on the listed contents of your Colab directory.\n",
    "\n",
    "4. <ins> Dataset folder structure</ins>\n",
    "\n",
    "The folder has three sets of data (and so, three subfolders):\n",
    "\n",
    "* Pistachio_Image_Dataset - containing image data as jpg files split between two folders named with the classification label of interest.\n",
    "\n",
    "* Pistachio_16_Features_Dataset - containing 16 hand-crafted (i.e. engineered) features extracted from the image data, represented as a feature vector and labels provided in different file formats including xls.\n",
    ">Read about the 16 features here: https://www.researchgate.net/profile/Ridvan-Saracoglu/publication/353121533_Classification_of_Pistachio_Species_Using_Improved_k-NN_Classifier/links/60e8213930e8e50c01f0e73f/Classification-of-Pistachio-Species-Using-Improved-k-NN-Classifier.pdf\n",
    "\n",
    "* Pistachio_28_Features_Dataset - containing 28 hand-crafted (i.e. engineered) features extracted from the image data, represented as a feature vector and labels provided in different file formats including xls. (It is not very clear what how the 28 features were extracted, but I expect that it includes the 16 features above.).\n",
    "\n",
    "5. <ins> Loading the dataset</ins>\n",
    "\n",
    "You can use the code below to unzip the folder (first code cell), load the hand-crafted features data, with labels (second code cell), and preload the raw image data with labels (third code cell).\n",
    ">Only the path addresses of the images are loaded here ('pre-load') as loading all the images into memory at once would not be efficient on the memory.\n",
    "\n",
    ">Note that the two data versions (hand-crafted features and image) are loaded such that they are matched for the same data instance. (At least there is an attempt to do so based on what is understood of the data arrangement.)\n",
    "\n",
    ">You may want to browse through the files yourself to see what they contain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7x9IysTwuZn4"
   },
   "source": [
    "**Unzipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yTM2KHoUG8V"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# specify the full path for the uploaded zipped dataset file\n",
    "data_folder_full_path = \"/content/Pistachio_Image_Dataset.zip\"\n",
    "\n",
    "\n",
    "# load the zip file and create a zip object\n",
    "with ZipFile(data_folder_full_path, 'r') as datasetFolderObject:\n",
    "\n",
    "    # and then extracting the contents to the main directory\n",
    "    datasetFolderObject.extractall(path=\"/content\")\n",
    "\n",
    "# lists the content of your main Colab directory\n",
    "!ls  /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hQvWSPSVDle"
   },
   "source": [
    "**Loading the *Pistachio_28_Features_Dataset* data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FipQwcNeDI0A"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "\n",
    "\n",
    "# specify the full path for the unzipped xls file for the 28_features version\n",
    "# of the dataset\n",
    "feat_data_file_full_path = \"/content/Pistachio_Image_Dataset/Pistachio_28_Features_Dataset/Pistachio_28_Features_Dataset.xls\"\n",
    "\n",
    "# load the data from the xls file\n",
    "feat_28_data_pandas = pandas.read_excel(feat_data_file_full_path)\n",
    "feat_28_data = feat_28_data_pandas.to_numpy()\n",
    "\n",
    "# output the shape of the loaded data to the screen\n",
    "print(\"\\n The dataset has shape: \"+str(feat_28_data.shape))\n",
    "\n",
    "\n",
    "# get the features and the labels from the loaded data\n",
    "# note that the label column is the last column of the file\n",
    "feat_col = numpy.arange(0, feat_28_data.shape[1]-1)\n",
    "label_col = feat_28_data.shape[1]-1\n",
    "\n",
    "feats_28 = feat_28_data[:, feat_col]\n",
    "labels = feat_28_data[:, label_col]\n",
    "\n",
    "print(\"\\n A peek at the 28-features dataset features: \\n\"+str(feats_28))\n",
    "print(\"\\n A peek at the 28-features dataset labels: \\n\"+str(labels))\n",
    "\n",
    "# recode the nominal labels as numeric\n",
    "numpy.place(labels, labels=='Kirmizi_Pistachio', '0')\n",
    "numpy.place(labels, labels=='Siirt_Pistachio', '1')\n",
    "print(\"\\n The recoded labels: \", str(numpy.unique(labels)))\n",
    "\n",
    "# convert from string type to integer type\n",
    "labels = labels.astype(int)\n",
    "print(\"\\n A peek at the recoded 28-features dataset labels: \\n\"+str(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKLBxPCKqyRX"
   },
   "source": [
    "**Pre-load the *Pistachio_Image_Dataset* data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exSriPCMq4HJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "# specify the full path for the folders containing\n",
    "# the image version of the dataset\n",
    "image_source_dir_kirmizi = \"/content/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Kirmizi_Pistachio\"\n",
    "image_source_dir_siirt = \"/content/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Siirt_Pistachio\"\n",
    "\n",
    "\n",
    "# create a method for getting the path addresses for the (image) contents\n",
    "# of the image source folders\n",
    "# as we also want to relate the image instances to the data instances (rows)\n",
    "# in the hand-crafted features version of the dataset, we also need to get\n",
    "# the instance id in the file names of the images.\n",
    "# (assuming that the row number corresponds to the matching instance id\n",
    "# for the hand-crafted features version of the dataset)\n",
    "#\n",
    "# NOTE that some filenames have the instance id repeated in bracket\n",
    "# for another instance, so we need to address that.\n",
    "def get_filepaths(image_source_dir, num_id_bracketed, limit=0):\n",
    "  image_files = []\n",
    "  instance_ids = []\n",
    "  count_instances = limit\n",
    "\n",
    "  # Loop through all the files in the given folder\n",
    "  # and get the full filepath for each file\n",
    "  # and also get the corresponding instance id from its filename.\n",
    "  #\n",
    "  # We need to account for the ids repeated in brackets\n",
    "  # (see NOTE above),\n",
    "  # in order to get unique ids. For the sake of this lab,\n",
    "  # we will assume that the instances with the bracketed ids\n",
    "  # are earlier instances than instances without brackets\n",
    "  # i.e. (1), (2) are earlier than 1, for example\n",
    "  for filename in os.listdir(image_source_dir):\n",
    "    image_files.append(os.path.join(image_source_dir, filename))\n",
    "\n",
    "    instance = filename.split(' ')\n",
    "    #print(instance)\n",
    "    instance = instance[1].split('.')\n",
    "    #print(instance)\n",
    "    instance = instance[0]\n",
    "    #print(instance)\n",
    "    if instance[0]=='(':\n",
    "      instance = instance[1:]\n",
    "      instance = instance[:len(instance)-1]\n",
    "      instance = int(instance)\n",
    "    else:\n",
    "      instance = int(instance)+num_id_bracketed\n",
    "    #print(instance)\n",
    "    instance_ids.append(instance-1+limit)\n",
    "\n",
    "    count_instances += 1\n",
    "\n",
    "  return image_files, instance_ids, count_instances\n",
    "\n",
    "\n",
    "# call the above method to get the path address for each image in the folders\n",
    "#\n",
    "# Since the Kirmizi instances appear as the first rows in the hand-crafted features\n",
    "# version of the dataset, we start the instance id ordering from the folder for this label.\n",
    "#\n",
    "# Remember that we want to be able to match instances in the hand-crafted features data\n",
    "# with instances in the raw image data.\n",
    "image_files_kirmizi, instance_ids_kirmizi, count_instances_kirmizi = get_filepaths(image_source_dir_kirmizi, num_id_bracketed=65)\n",
    "# Since the Siirt instances appear as the last rows in the hand-crafted features*,\n",
    "# version of the dataset, we continue the instance id ordering based on the count\n",
    "# of instances in the Kirmizi folder\n",
    "image_files_siirt, instance_ids_siirt, _ = get_filepaths(image_source_dir_siirt, num_id_bracketed=50, limit=count_instances_kirmizi)\n",
    "# parameter values 50 and 65 above correspond to the number of image files\n",
    "# with brackets in their instance ids\n",
    "# so in essence, the non-bracketed ids are shifted by those numbers\n",
    "# to make sure that ids are unique numbers (when brackets are removed)\n",
    "\n",
    "\n",
    "# combine the lists of files for both image source folders\n",
    "image_files = []\n",
    "image_files.extend(image_files_kirmizi)\n",
    "image_files.extend(image_files_siirt)\n",
    "\n",
    "\n",
    "# combine the list of instance ids for both image source folders\n",
    "# the ids are needed to match with the hand-crafted features data\n",
    "image_instance_ids = []\n",
    "image_instance_ids.extend(instance_ids_kirmizi)\n",
    "image_instance_ids.extend(instance_ids_siirt)\n",
    "\n",
    "print(\"\\n A peek at the image files: \\n\"+str(image_files))\n",
    "print(\"\\n A peek at the instance ids: \\n\"+str(image_instance_ids))\n",
    "\n",
    "\n",
    "# double-check that instance ids are indeed unique\n",
    "# if true, the number of unique instance ids should match\n",
    "# the number of instances in the hand-crafted features data\n",
    "print('\\n Check:')\n",
    "print('Number of images preloaded:', len(image_instance_ids))\n",
    "print('Number of unique instance ids for the images:', numpy.unique(numpy.array(image_instance_ids)).shape)\n",
    "print('Number of labels loaded:', labels.shape)\n",
    "\n",
    "\n",
    "# method to visualize sample images\n",
    "def plot_image(img_file, img_name):\n",
    "\n",
    "  print()\n",
    "  img = read_image(img_file)\n",
    "  print('Image shape:', img.size())\n",
    "  pil_img = to_pil_image(img)\n",
    "  plt.figure()\n",
    "  plt.imshow(numpy.asarray(pil_img))\n",
    "  plt.title(img_name)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# for a peek, randomly select one example from the kirmizi class to visualize\n",
    "rng =  numpy.random.default_rng(random_seed)\n",
    "kirmizi_id = rng.choice(instance_ids_kirmizi)\n",
    "# for a peek, randomly select one example from the siirt class to visualize\n",
    "siirt_id = rng.choice(instance_ids_siirt)\n",
    "\n",
    "\n",
    "plot_image(image_files[kirmizi_id], 'Kirmizi')\n",
    "plot_image(image_files[siirt_id], 'Siirt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqiOYPmMvnwT"
   },
   "source": [
    "# Section 2 - Split into training, validation, and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkPQK1S7LB-M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "all_ids = numpy.arange(0, feats_28.shape[0])\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "# First randomly split the data into 70:30 to get the training set\n",
    "# split to have similar distributions of the class labels.\n",
    "train_set_ids, rem_set_ids = train_test_split(all_ids, test_size=0.3, train_size=0.7,\n",
    "                                 random_state=random_seed, shuffle=True, stratify=labels)\n",
    "\n",
    "\n",
    "# Then further split the remaining data 50:50 into validation and test sets\n",
    "# split to have similar distributions of the class labels.\n",
    "val_set_ids, test_set_ids = train_test_split(rem_set_ids, test_size=0.5, train_size=0.5,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "\n",
    "\n",
    "# create a method for a histogram plot\n",
    "# that shows the distribution of each label class\n",
    "# (classes '0' and '1' in this case)\n",
    "# for a given set of label instances\n",
    "def plot_label_distr(labels, plot_title):\n",
    "  print()\n",
    "  plt.figure()\n",
    "  the_bin_centres = numpy.unique(labels)\n",
    "  plt.hist(labels, bins=the_bin_centres.shape[0], range=(the_bin_centres[0]-0.5, the_bin_centres[the_bin_centres.shape[0]-1]+0.5))\n",
    "  plt.xticks(the_bin_centres)\n",
    "  plt.title(plot_title)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# check the distribution of the labels in the training, validation, and test sets\n",
    "plot_label_distr(labels[train_set_ids], 'Class frequencies for training set')\n",
    "plot_label_distr(labels[val_set_ids], 'Class frequencies for validation set')\n",
    "plot_label_distr(labels[test_set_ids], 'Class frequencies for test set')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRB0Zg4_GXR9"
   },
   "source": [
    "# Section 3 - Scale (i.e. normalize) the hand-crafted features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sETTFPn0HBCL"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a function for normalizing the feature vector\n",
    "# using a standard scaling (results in mean=0 and standard deviation=1)\n",
    "def scale_feats(feat_vec):\n",
    "  # Scaling the features to the same range of values\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(feat_vec)\n",
    "  scaled_feat_vec = scaler.transform(feat_vec)\n",
    "  print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feat_vec))\n",
    "\n",
    "  return scaled_feat_vec\n",
    "\n",
    "# normalize the feature vector\n",
    "scaled_feats_28 = scale_feats(feats_28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxK8R5IbIjru"
   },
   "source": [
    "# Section 4 - Extract new (transfer learning) features using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHf5Jj-tWgKj"
   },
   "source": [
    "We will use a pre-trained *VGG-16* model. The VGG-16 is a 16-layer convolutional neural network (CNN).\n",
    ">Have a look at the basic details of the model here: https://pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16.\n",
    "\n",
    ">You can read more about the VGG-16 model architecture here: https://arxiv.org/abs/1409.1556.\n",
    "\n",
    ">From the documentation and publication, note:\n",
    "  * the number of parameters (weights) that the model has\n",
    "  * the dataset that it was trained on (You can read more about the dataset here: https://www.image-net.org/)\n",
    "  * the different kinds of objects etc that it was trained to differentiate, i.e. its classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tMuguTOUOGq"
   },
   "outputs": [],
   "source": [
    "from enum import auto\n",
    "from torchvision.io import read_image\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# load and view the VGG16 model\n",
    "model_vgg16 = models.vgg16()\n",
    "print(\"\\n 'View' the model architecture:\\n\")\n",
    "print(model_vgg16)\n",
    "\n",
    "\n",
    "# Extract features using the feature extraction layers of the pretrained model\n",
    "\n",
    "### Step 1: Initialize model with the best available weights\n",
    "model_vgg16_featlayers = models.vgg16(weights=\"IMAGENET1K_FEATURES\").features\n",
    "model_vgg16_featlayers.eval()\n",
    "\n",
    "\n",
    "### Step 2: Initialize the inference transforms\n",
    "# This does some preprocessing behind the scenes, including:\n",
    "# 1) Resizing the input to resize_size=[256];\n",
    "# 2) Followed by a central cropping of crop_size=[224];\n",
    "# 3) And finally the values are first rescaled to [0.0, 1.0]\n",
    "# and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "# The transform function expects either:\n",
    "# a batched (B, C, H, W) or single (C, H, W) image, as torch.Tensor objects.\n",
    "preprocess = models.VGG16_Weights.IMAGENET1K_FEATURES.transforms(antialias=True)\n",
    "\n",
    "\n",
    "\n",
    "# method to get features learnt in the pretrained VGG16\n",
    "# extracting these features for our own list of images\n",
    "def get_img_feats(img_path):\n",
    "\n",
    "\n",
    "  img = read_image(img_path)\n",
    "  #print(\"Fetching image...\")\n",
    "\n",
    "  # Step 3: Apply the preprocessing transforms\n",
    "  batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "  # Step 4: Use the feature extraction layers of the pretrained model\n",
    "  # to obtain the features\n",
    "  auto_feats = model_vgg16_featlayers(batch).squeeze(0)\n",
    "  auto_feats = auto_feats.detach().numpy()\n",
    "  auto_feats = numpy.mean(auto_feats, axis=1, keepdims=False)\n",
    "  auto_feats = numpy.mean(auto_feats, axis=1, keepdims=False)\n",
    "\n",
    "  return auto_feats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvmwanyJPZtN"
   },
   "source": [
    "# Section 5 - Train a MLP using the new (transfer learning) features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIU2sttg4_ti"
   },
   "source": [
    "* Train and evaluate a MLP for classifying pistachio images into the two classes based on the new features extracted using the VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2ul93QmQMTU"
   },
   "source": [
    "# Section 6 - Train a MLP using the hand-crafted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_ukJQLjW7uk"
   },
   "source": [
    "* Train and evaluate a MLP for classifying pistachio images into the two classes based on the original hand-crafted features that came with the dataset.\n",
    "\n",
    "* How does performance compare with that in Section 5?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNcCLN7avegbe9n7gztmG3t",
   "collapsed_sections": [
    "lhcT91UwAajV",
    "_joWMUBMn2VC",
    "dqiOYPmMvnwT",
    "GRB0Zg4_GXR9",
    "VxK8R5IbIjru",
    "vvmwanyJPZtN",
    "I2ul93QmQMTU"
   ],
   "provenance": [
    {
     "file_id": "1B-AbI9TnSXD5EynzY3el5dzsO-9xDGkC",
     "timestamp": 1676986531925
    },
    {
     "file_id": "1hugfZfRQ762fcbAXuSsxcWYBO-q16Ffq",
     "timestamp": 1676380655825
    },
    {
     "file_id": "1rIS3LLDgQ_aPDexPx3EQxFMrwUs7h2cZ",
     "timestamp": 1675782505060
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
