{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddc544d",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### 1. [Setup](#Setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94147ab",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117fcb6",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb0bc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b17a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import parquet engines\n",
    "try:\n",
    "    import pyarrow\n",
    "    PARQUET_ENGINE = 'pyarrow'\n",
    "except ImportError:\n",
    "    try:\n",
    "        import fastparquet\n",
    "        PARQUET_ENGINE = 'fastparquet'\n",
    "    except ImportError:\n",
    "        print(\"Warning: No parquet engine found. Installing pyarrow recommended.\")\n",
    "        print(\"Run: pip install pyarrow\")\n",
    "        PARQUET_ENGINE = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd37eb2",
   "metadata": {},
   "source": [
    "#### Mkdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedea4e8",
   "metadata": {},
   "source": [
    "#### Consts and Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6b80fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = [\n",
    "    'AvgSurfT_inst',\n",
    "    'CanopInt_inst',\n",
    "    'LWdown_f_tavg',\n",
    "    'Psurf_f_inst',\n",
    "    'Qair_f_inst',\n",
    "    'SnowDepth_inst',\n",
    "    'SWdown_f_tavg',\n",
    "    'Tair_f_inst',\n",
    "    'TVeg_tavg',\n",
    "    'Wind_f_inst',\n",
    "    'Rainf_tavg'\n",
    "]  \n",
    "\n",
    "AGGREGATION = {\n",
    "    'Rainf_tavg': 'sum',        # Rain accumulates over time\n",
    "    'SnowDepth_inst': 'sum',    # Snow accumulation over time  \n",
    "    'CanopInt_inst': 'sum',     # Water accumulation over time\n",
    "    'Tair_f_inst': 'mean',      # Daily average temperature\n",
    "    'AvgSurfT_inst': 'mean',    # Daily average surface temperature\n",
    "    'Psurf_f_inst': 'mean',     # Daily average pressure\n",
    "    'Qair_f_inst': 'mean',      # Daily average humidity\n",
    "    'Wind_f_inst': 'mean',      # Daily average wind speed\n",
    "    'LWdown_f_tavg': 'mean',    # Daily average longwave radiation\n",
    "    'SWdown_f_tavg': 'mean',    # Daily average shortwave radiation\n",
    "    'TVeg_tavg': 'mean'         # Daily average transpiration\n",
    "}\n",
    "\n",
    "DATES = [\n",
    "    '2024_March',\n",
    "    '2024_April',\n",
    "    '2024_May',\n",
    "    '2024_June',\n",
    "    '2024_July',\n",
    "    '2024_Aug',\n",
    "    '2024_Sept',\n",
    "    '2024_Oct',\n",
    "    '2024_Nov',\n",
    "    '2024_Dec',\n",
    "    '2025_Jan',\n",
    "    '2025_Feb',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a87ae",
   "metadata": {},
   "source": [
    "# ReUsable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f23f1",
   "metadata": {},
   "source": [
    "#### Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc85e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_csv(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV file and return a pandas DataFrame.\"\"\"\n",
    "    file_path = Path('data/raw') / filename\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d1913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_numpy(filename: str, subfolder: str) -> np.ndarray:\n",
    "    filename = f\"{filename}.csv\"\n",
    "    file_path = os.path.join('data', subfolder, filename)\n",
    "\n",
    "    data_as_list = []\n",
    "\n",
    "    with open(file_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        next(csv_reader)  # Skip header\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            data_as_list.append([float(val) for val in row])\n",
    "    \n",
    "    data = np.array(data_as_list)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddc8a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_parquet(arr, filename: str, subfolder: str):\n",
    "    filename = f\"{filename}.parquet\"\n",
    "    file_path = os.path.join('data', subfolder, filename)\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    df = pd.DataFrame(arr, columns=['longitude', 'latitude', 'year', 'month', 'day', 'value'])\n",
    "    df.to_parquet(file_path, index=False, engine=PARQUET_ENGINE)\n",
    "\n",
    "    print(f\"\\n‚úÖ Data saved successfully!\")\n",
    "    print(f\"üìÅ Location: {file_path}\")\n",
    "    print(f\"ÔøΩÔøΩ Shape: {arr.shape[0]:,} rows √ó {arr.shape[1]} columns\")\n",
    "    print(f\"üíæ File size: {os.path.getsize(file_path) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265acc2",
   "metadata": {},
   "source": [
    "#### Complex Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dbf1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_3hourly_to_daily_pandas(arr, parameter):\n",
    "    # Convert array to pandas DataFrame\n",
    "    df = pd.DataFrame(arr, columns=['year', 'month', 'day', 'hour', 'longitude', 'latitude', 'value'])\n",
    "    \n",
    "    # Get the aggregation method for this parameter\n",
    "    agg_method = AGGREGATION.get(parameter, 'mean')  # Default to 'mean' if not found\n",
    "    \n",
    "    # Aggregate to daily using the appropriate method\n",
    "    daily_df = df.groupby(['longitude', 'latitude', 'year', 'month', 'day'])['value'].agg(agg_method).reset_index()\n",
    "    \n",
    "    # Convert back to numpy array\n",
    "    daily_arr = daily_df.values\n",
    "    \n",
    "    return daily_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6480a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_basic_stats(arr):\n",
    "\n",
    "    print(f\"Total rows: {arr.shape[0]}\")\n",
    "    print(f\"Missing values in final column: {np.isnan(arr[:, -1]).sum()}\")\n",
    "    print(f\"Missing percentage: {np.isnan(arr[:, -1]).sum() / arr.shape[0] * 100:.2f}%\")\n",
    "\n",
    "    print(f\"Range: {arr[:, -1].min():.2f} to {arr[:, -1].max():.2f}\")\n",
    "    print(f\"Average: {arr[:, -1].mean():.2f}\")\n",
    "\n",
    "    print(f\"Longitude range: {arr[:, 4].min():.2f} to {arr[:, 4].max():.2f}\")\n",
    "    print(f\"Latitude range: {arr[:, 5].min():.2f} to {arr[:, 5].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6e59e",
   "metadata": {},
   "source": [
    "# Raw Data Proccess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b4bc4",
   "metadata": {},
   "source": [
    "- Loop through and read monthly parameter CSVs\n",
    "- Run basic checks: missing data\n",
    "- Select data types so can be held as numpy array\n",
    "- Aggregate to daily\n",
    "- Concat monthlys into a single array\n",
    "- Sort by long, lat, year, month, day\n",
    "- Save as parqets in aggregated folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc868e01",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c2440da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_raw_data(parameter_list: list, date_list: list):    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for parameter in parameter_list:\n",
    "        print(f'Parameter: {parameter}')\n",
    "\n",
    "        monthly = []\n",
    "\n",
    "        for date in date_list:\n",
    "            print(f\"\\n===\\n\")\n",
    "\n",
    "            # File to be processed:\n",
    "            filename = f\"{parameter}_data_{date}\"\n",
    "            print(f\"date: {date}\")\n",
    "            print(filename + '.csv')\n",
    "\n",
    "            # Load CSV to numpy array:\n",
    "            try:\n",
    "                arr = read_csv_to_numpy(filename, 'raw')\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            \n",
    "            # Basic stats/checks:\n",
    "            print_basic_stats(arr)\n",
    "\n",
    "            # Aggregation from 3hourly to daily:\n",
    "            daily_arr_numpy = aggregate_3hourly_to_daily_pandas(arr, parameter)\n",
    "            print(f\"Original 3-hourly data: {arr.shape}\")\n",
    "            print(f\"Aggregated daily data: {daily_arr_numpy.shape}\")\n",
    "\n",
    "            # Add to monthly list\n",
    "            monthly.append(daily_arr_numpy)\n",
    "            print(f\"Monthly list now contains {len(monthly)} datasets\")\n",
    "        \n",
    "        print(f\"\\n===\\n\")\n",
    "\n",
    "        if monthly:\n",
    "            print(f\"\\nStacking {len(monthly)} monthly datasets...\")\n",
    "        \n",
    "            # Stack all monthly arrays vertically (one on top of the other)\n",
    "            consolidated_arr = np.vstack(monthly)\n",
    "            \n",
    "            print(f\"Consolidated array shape: {consolidated_arr.shape}\")\n",
    "            print(f\"Total rows: {consolidated_arr.shape[0]:,}\")\n",
    "            print(f\"Total columns: {consolidated_arr.shape[1]}\")\n",
    "            print(f\"Memory usage: {consolidated_arr.nbytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "            print(consolidated_arr[0])\n",
    "\n",
    "            # Basic stats/checks:\n",
    "            print_basic_stats(consolidated_arr)\n",
    "            save_as_parquet(consolidated_arr, parameter, 'daily')\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe5ef0",
   "metadata": {},
   "source": [
    "# Reference Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16ff91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def references_tables(parameters: list):\n",
    "\n",
    "    references = pd.DataFrame()\n",
    "\n",
    "    for parameter in parameters:\n",
    "        df = pd.read_parquet(\n",
    "            f'data/daily/{parameter}.parquet', \n",
    "            columns=['longitude', 'latitude', 'year', 'month', 'day']\n",
    "        )\n",
    "        unique_combinations = df.drop_duplicates()\n",
    "\n",
    "        if parameter == 'Rainf_tavg':\n",
    "            label_combinations = unique_combinations.copy()\n",
    "\n",
    "        if references.empty:\n",
    "            references = unique_combinations.copy()\n",
    "        else:\n",
    "            references = pd.concat([references, unique_combinations], ignore_index=True)\n",
    "\n",
    "            references = references.drop_duplicates(subset=['longitude', 'latitude', 'year', 'month', 'day'])\n",
    "\n",
    "    save_as_parquet(references, \"total_references\", \"references\")\n",
    "    save_as_parquet(label_combinations, \"label_references\", \"references\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da14839",
   "metadata": {},
   "source": [
    "# Spitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68d52d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting dataset splitting process...\n",
      "============================================================\n",
      "\n",
      "üìä Processing AvgSurfT_inst...\n",
      "üîÑ Processing AvgSurfT_inst...\n",
      "   Loaded: 5,533,035 rows\n",
      "    year  month     day  longitude  latitude       value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  240.669425\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  254.237700\n",
      "2 -179.5   66.5  2024.0        3.0       3.0  251.013837\n",
      "3 -179.5   66.5  2024.0        3.0       4.0  252.279575\n",
      "4 -179.5   66.5  2024.0        3.0       5.0  250.055550\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing CanopInt_inst...\n",
      "üîÑ Processing CanopInt_inst...\n",
      "   Loaded: 5,533,035 rows\n",
      "    year  month     day  longitude  latitude    value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  0.08007\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  0.08007\n",
      "2 -179.5   66.5  2024.0        3.0       3.0  0.08007\n",
      "3 -179.5   66.5  2024.0        3.0       4.0  0.08007\n",
      "4 -179.5   66.5  2024.0        3.0       5.0  0.08007\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing LWdown_f_tavg...\n",
      "üîÑ Processing LWdown_f_tavg...\n",
      "   Loaded: 5,593,625 rows\n",
      "    year  month     day  longitude  latitude       value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  155.210125\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  223.623662\n",
      "2 -179.5   66.5  2024.0        3.0       3.0  199.869912\n",
      "3 -179.5   66.5  2024.0        3.0       4.0  184.439388\n",
      "4 -179.5   66.5  2024.0        3.0       5.0  178.061413\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing Psurf_f_inst...\n",
      "üîÑ Processing Psurf_f_inst...\n",
      "   Loaded: 5,593,625 rows\n",
      "    year  month     day  longitude  latitude         value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  101736.38750\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  100640.19125\n",
      "2 -179.5   66.5  2024.0        3.0       3.0   99019.14750\n",
      "3 -179.5   66.5  2024.0        3.0       4.0   98786.29625\n",
      "4 -179.5   66.5  2024.0        3.0       5.0   99197.87375\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing Qair_f_inst...\n",
      "üîÑ Processing Qair_f_inst...\n",
      "   Loaded: 5,593,625 rows\n",
      "    year  month     day  longitude  latitude     value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  0.000388\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  0.000858\n",
      "2 -179.5   66.5  2024.0        3.0       3.0  0.001019\n",
      "3 -179.5   66.5  2024.0        3.0       4.0  0.000804\n",
      "4 -179.5   66.5  2024.0        3.0       5.0  0.000653\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing SnowDepth_inst...\n",
      "üîÑ Processing SnowDepth_inst...\n",
      "   Loaded: 5,533,035 rows\n",
      "    year  month     day  longitude  latitude     value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  7.268184\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  7.882546\n",
      "2 -179.5   66.5  2024.0        3.0       3.0  8.156570\n",
      "3 -179.5   66.5  2024.0        3.0       4.0  8.122273\n",
      "4 -179.5   66.5  2024.0        3.0       5.0  8.086541\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing SWdown_f_tavg...\n",
      "üîÑ Processing SWdown_f_tavg...\n",
      "   Loaded: 5,593,625 rows\n",
      "    year  month     day  longitude  latitude      value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  52.416609\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  34.640720\n",
      "2 -179.5   66.5  2024.0        3.0       3.0  41.723025\n",
      "3 -179.5   66.5  2024.0        3.0       4.0  56.027321\n",
      "4 -179.5   66.5  2024.0        3.0       5.0  59.661755\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing Tair_f_inst...\n",
      "üîÑ Processing Tair_f_inst...\n",
      "   Loaded: 5,593,625 rows\n",
      "    year  month     day  longitude  latitude       value\n",
      "0 -179.5   66.5  2024.0        3.0       1.0  249.176437\n",
      "1 -179.5   66.5  2024.0        3.0       2.0  258.130650\n",
      "2 -179.5   66.5  2024.0        3.0       3.0  258.273962\n",
      "3 -179.5   66.5  2024.0        3.0       4.0  256.262662\n",
      "4 -179.5   66.5  2024.0        3.0       5.0  253.423462\n",
      "   Filtering to label reference combinations...\n",
      "   After filtering: 5,533,035 rows\n",
      "   Splitting into train/test/valid...\n",
      "   Train set: 5,533,035 rows\n",
      "   Test set: 0 rows\n",
      "   Valid set: 0 rows\n",
      "\n",
      "üìä Processing TVeg_tavg...\n",
      "üîÑ Processing TVeg_tavg...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parameter \u001b[38;5;129;01min\u001b[39;00m PARAMS:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparameter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43msplit_dataset_train_test_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m#     train_df, test_df, valid_df = split_dataset_train_test_valid(parameter, label_reference)\u001b[39;00m\n\u001b[32m     69\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[38;5;66;03m#     print(f\"   Total: {len(splits['train']) + len(splits['test']) + len(splits['valid']):,} rows\")\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m#     print()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36msplit_dataset_train_test_valid\u001b[39m\u001b[34m(parameter_name)\u001b[39m\n\u001b[32m      9\u001b[39m label_reference = pd.read_parquet(\u001b[33m'\u001b[39m\u001b[33mdata/references/label_references.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparameter_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/daily/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparameter_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/study-planner/934G5_Machine_Learning/assessment/venv/lib/python3.13/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/study-planner/934G5_Machine_Learning/assessment/venv/lib/python3.13/site-packages/pandas/io/parquet.py:273\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcatch_warnings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n\u001b[32m    279\u001b[39m         result = arrow_table_to_pandas(\n\u001b[32m    280\u001b[39m             pa_table,\n\u001b[32m    281\u001b[39m             dtype_backend=dtype_backend,\n\u001b[32m    282\u001b[39m             to_pandas_kwargs=to_pandas_kwargs,\n\u001b[32m    283\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/warnings.py:462\u001b[39m, in \u001b[36mcatch_warnings.__init__\u001b[39m\u001b[34m(self, record, module, action, category, lineno, append)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatch_warnings\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m    444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A context manager that copies and restores the warnings filter upon\u001b[39;00m\n\u001b[32m    445\u001b[39m \u001b[33;03m    exiting the context.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m    context.\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, record=\u001b[38;5;28;01mFalse\u001b[39;00m, module=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    463\u001b[39m                  action=\u001b[38;5;28;01mNone\u001b[39;00m, category=\u001b[38;5;167;01mWarning\u001b[39;00m, lineno=\u001b[32m0\u001b[39m, append=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    464\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"Specify whether to record warnings and if an alternative module\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m        should be used other than sys.modules['warnings'].\u001b[39;00m\n\u001b[32m    466\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m \n\u001b[32m    470\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28mself\u001b[39m._record = record\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def split_dataset_train_test_valid(parameter_name):\n",
    "    \"\"\"\n",
    "    Split a parameter dataset into train/test/valid sets.\n",
    "    - Test: February data\n",
    "    - Valid: January data  \n",
    "    - Train: All other months\n",
    "    \"\"\"\n",
    "\n",
    "    label_reference = pd.read_parquet('data/references/label_references.parquet')\n",
    "    \n",
    "    print(f\"üîÑ Processing {parameter_name}...\")\n",
    "    \n",
    "    df = pd.read_parquet(f'data/daily/{parameter_name}.parquet')\n",
    "    print(f\"   Loaded: {len(df):,} rows\")\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    # print(f\"Month distribution:\\n{df['month'].value_counts().sort_index().to_string()}\")\n",
    "    \n",
    "    print(f\"   Filtering to label reference combinations...\")\n",
    "    df_filtered = df.merge(label_reference, \n",
    "                          on=['longitude', 'latitude', 'year', 'month', 'day'], \n",
    "                          how='inner')\n",
    "    print(f\"   After filtering: {len(df_filtered):,} rows\")\n",
    "    \n",
    "    # Split by month\n",
    "    print(f\"   Splitting into train/test/valid...\")\n",
    "    \n",
    "    test_df = df_filtered[df_filtered['month'] == 2].copy()\n",
    "    valid_df = df_filtered[df_filtered['month'] == 1].copy()\n",
    "    train_df = df_filtered[~df_filtered['month'].isin([1, 2])].copy()\n",
    "    \n",
    "    # Print split statistics\n",
    "    print(f\"   Train set: {len(train_df):,} rows\")\n",
    "    print(f\"   Test set: {len(test_df):,} rows\") \n",
    "    print(f\"   Valid set: {len(valid_df):,} rows\")\n",
    "    \n",
    "    # Save the split datasets\n",
    "    # os.makedirs('data/splits', exist_ok=True)\n",
    "    \n",
    "    # Save train set\n",
    "    # train_path = f'data/splits/{parameter_name}_train.parquet'\n",
    "    # train_df.to_parquet(train_path, index=False)\n",
    "    \n",
    "    # Save test set\n",
    "    # test_path = f'data/splits/{parameter_name}_test.parquet'\n",
    "    # test_df.to_parquet(test_path, index=False)\n",
    "    \n",
    "    # Save validation set\n",
    "    # valid_path = f'data/splits/{parameter_name}_valid.parquet'\n",
    "    # valid_df.to_parquet(valid_path, index=False)\n",
    "    \n",
    "    # print(f\"   ‚úÖ Saved to data/splits/\")\n",
    "    \n",
    "    # return train_df, test_df, valid_df\n",
    "\n",
    "# Process all parameters\n",
    "print(\"üöÄ Starting dataset splitting process...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "split_results = {}\n",
    "\n",
    "for parameter in PARAMS:\n",
    "    print(f\"\\nüìä Processing {parameter}...\")\n",
    "    split_dataset_train_test_valid(parameter)\n",
    "    \n",
    "    # try:\n",
    "    #     train_df, test_df, valid_df = split_dataset_train_test_valid(parameter, label_reference)\n",
    "        \n",
    "    #     # Store results\n",
    "    #     split_results[parameter] = {\n",
    "    #         'train': train_df,\n",
    "    #         'test': test_df, \n",
    "    #         'valid': valid_df\n",
    "    #     }\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     print(f\"   ‚ùå Error processing {parameter}: {e}\")\n",
    "    #     continue\n",
    "\n",
    "# # Final summary\n",
    "# print(f\"\\nÔøΩÔøΩ SPLITTING COMPLETE!\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# for param, splits in split_results.items():\n",
    "#     print(f\"{param}:\")\n",
    "#     print(f\"   Train: {len(splits['train']):,} rows\")\n",
    "#     print(f\"   Test: {len(splits['test']):,} rows\")\n",
    "#     print(f\"   Valid: {len(splits['valid']):,} rows\")\n",
    "#     print(f\"   Total: {len(splits['train']) + len(splits['test']) + len(splits['valid']):,} rows\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4d96d",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b7fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: AvgSurfT_inst\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_March\n",
      "AvgSurfT_inst_data_2024_March.csv\n",
      "Total rows: 3759432\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 206.82 to 349.63\n",
      "Average: 277.67\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3759432, 7)\n",
      "Aggregated daily data: (469929, 6)\n",
      "Monthly list now contains 1 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_April\n",
      "AvgSurfT_inst_data_2024_April.csv\n",
      "Total rows: 3638160\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 215.17 to 346.24\n",
      "Average: 283.46\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3638160, 7)\n",
      "Aggregated daily data: (454770, 6)\n",
      "Monthly list now contains 2 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_May\n",
      "AvgSurfT_inst_data_2024_May.csv\n",
      "Total rows: 3759432\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 223.41 to 345.40\n",
      "Average: 287.95\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3759432, 7)\n",
      "Aggregated daily data: (469929, 6)\n",
      "Monthly list now contains 3 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_June\n",
      "AvgSurfT_inst_data_2024_June.csv\n",
      "Total rows: 3638160\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 233.83 to 343.98\n",
      "Average: 292.42\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3638160, 7)\n",
      "Aggregated daily data: (454770, 6)\n",
      "Monthly list now contains 4 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_July\n",
      "AvgSurfT_inst_data_2024_July.csv\n",
      "Total rows: 3759432\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 248.17 to 345.72\n",
      "Average: 294.05\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3759432, 7)\n",
      "Aggregated daily data: (469929, 6)\n",
      "Monthly list now contains 5 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_Aug\n",
      "AvgSurfT_inst_data_2024_Aug.csv\n",
      "Total rows: 3759432\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 237.14 to 344.69\n",
      "Average: 293.05\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3759432, 7)\n",
      "Aggregated daily data: (469929, 6)\n",
      "Monthly list now contains 6 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_Sept\n",
      "AvgSurfT_inst_data_2024_Sept.csv\n",
      "Total rows: 3638160\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 223.88 to 347.08\n",
      "Average: 289.87\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3638160, 7)\n",
      "Aggregated daily data: (454770, 6)\n",
      "Monthly list now contains 7 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_Oct\n",
      "AvgSurfT_inst_data_2024_Oct.csv\n",
      "Total rows: 3759432\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 213.78 to 347.86\n",
      "Average: 284.38\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3759432, 7)\n",
      "Aggregated daily data: (469929, 6)\n",
      "Monthly list now contains 8 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_Nov\n",
      "AvgSurfT_inst_data_2024_Nov.csv\n",
      "Total rows: 3638160\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 206.80 to 347.52\n",
      "Average: 278.43\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3638160, 7)\n",
      "Aggregated daily data: (454770, 6)\n",
      "Monthly list now contains 9 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2024_Dec\n",
      "AvgSurfT_inst_data_2024_Dec.csv\n",
      "Total rows: 3759432\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 209.30 to 350.53\n",
      "Average: 274.20\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3759432, 7)\n",
      "Aggregated daily data: (469929, 6)\n",
      "Monthly list now contains 10 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2025_Jan\n",
      "AvgSurfT_inst_data_2025_Jan.csv\n",
      "Total rows: 3759432\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 205.20 to 349.71\n",
      "Average: 272.93\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3759432, 7)\n",
      "Aggregated daily data: (469929, 6)\n",
      "Monthly list now contains 11 datasets\n",
      "\n",
      "===\n",
      "\n",
      "date: 2025_Feb\n",
      "AvgSurfT_inst_data_2025_Feb.csv\n",
      "Total rows: 3395616\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 203.79 to 348.51\n",
      "Average: 273.96\n",
      "Longitude range: -179.50 to 179.50\n",
      "Latitude range: -54.50 to 83.50\n",
      "Original 3-hourly data: (3395616, 7)\n",
      "Aggregated daily data: (424452, 6)\n",
      "Monthly list now contains 12 datasets\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "Stacking 12 monthly datasets...\n",
      "Consolidated array shape: (5533035, 6)\n",
      "Total rows: 5,533,035\n",
      "Total columns: 6\n",
      "Memory usage: 253.28 MB\n",
      "[-1.79500000e+02  6.65000000e+01  2.02400000e+03  3.00000000e+00\n",
      "  1.00000000e+00  2.40669425e+02]\n",
      "Total rows: 5533035\n",
      "Missing values in final column: 0\n",
      "Missing percentage: 0.00%\n",
      "Range: 207.60 to 324.24\n",
      "Average: 283.58\n",
      "Longitude range: 1.00 to 31.00\n",
      "Latitude range: 207.60 to 324.24\n",
      "Time taken: 73.20 seconds\n"
     ]
    }
   ],
   "source": [
    "consolidate_raw_data(PARAMS, DATES)\n",
    "references_tables(PARAMS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
