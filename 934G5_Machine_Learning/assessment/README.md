# Data Processing Pipeline - ML Coursework 2025A3

**Rain Prediction Data Processing Pipeline**  
*Transform 132 NASA CSV files into ML-ready datasets*

## ğŸ¯ Project Overview

This pipeline processes NASA weather data to create train/test/validation datasets for rain prediction models. It handles temporal aggregation, data transformation, and split management while optimizing for M1 Pro MacBook performance.

## ğŸ“ Project Structure

```
assessment/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ data_loader.py     # CSV loading and validation
â”‚   â”œâ”€â”€ id_system.py       # Location-date ID management
â”‚   â”œâ”€â”€ split_strategy.py  # Train/test/validation assignment
â”‚   â”œâ”€â”€ aggregation.py     # Temporal aggregation logic
â”‚   â”œâ”€â”€ transformation.py  # Data format conversion
â”‚   â”œâ”€â”€ chunking.py        # Memory-efficient processing
â”‚   â”œâ”€â”€ optimization.py    # M1 Pro optimizations
â”‚   â”œâ”€â”€ error_handling.py  # Error management
â”‚   â”œâ”€â”€ validation.py      # Data quality checks
â”‚   â””â”€â”€ pipeline.py        # Main orchestration
â”œâ”€â”€ tests/                 # Unit and integration tests
â”œâ”€â”€ data/                  # Input/output data
â”‚   â”œâ”€â”€ raw/              # 132 NASA CSV files
â”‚   â”œâ”€â”€ interim/          # Intermediate processing
â”‚   â””â”€â”€ processed/        # Final ML datasets
â”œâ”€â”€ logs/                  # Processing logs
â”œâ”€â”€ config.py              # Configuration parameters
â”œâ”€â”€ main.py                # Entry point
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On macOS/Linux
# or
venv\Scripts\activate     # On Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Pipeline
```bash
python main.py
```

## ğŸ“Š Data Processing Strategy

### **Split Strategy**: Interleaved Monthly
- **Training**: March-December 2024 (10 months)
- **Testing**: January 2025 (1 month)
- **Validation**: February 2025 (1 month)

### **Data Flow**:
1. **Raw CSV Files** â†’ Load and validate
2. **Temporal Aggregation** â†’ 3-hourly to daily
3. **Data Transformation** â†’ 2D to 3D format
4. **Split Assignment** â†’ Train/test/validation
5. **Output** â†’ ML-ready datasets

## ğŸ› ï¸ Key Features

- **Memory Optimization**: Designed for 16GB M1 Pro
- **Split Integrity**: No data leakage across splits
- **Error Handling**: Robust processing with graceful degradation
- **Progress Tracking**: Real-time monitoring and ETA
- **Data Lineage**: Complete processing audit trail

## ğŸ“ Configuration

Edit `config.py` to adjust:
- Chunk sizes and memory limits
- Split ratios and strategies
- Data type optimizations
- Validation thresholds

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test file
pytest tests/test_data_loader.py
```

## ğŸ“ˆ Performance

- **Target Memory**: < 12GB (leaving 4GB for system)
- **Processing Time**: Optimized for M1 Pro performance
- **Parallel Processing**: Multi-core utilization
- **Chunking**: Memory-efficient batch processing

## ğŸš¨ Limitations

- **Dataset Size**: Only 12 months of data available
- **Seasonal Coverage**: Limited winter training data
- **Geographic Resolution**: 1.0 degree grid spacing
- **Temporal Resolution**: 3-hourly to daily aggregation

## ğŸ“š Documentation

- **Implementation Roadmap**: `docs/implementation_roadmap.md`
- **Data Processing Notes**: `docs/data_processing_notes.md`
- **Code Documentation**: Inline docstrings and comments

## ğŸ¤ Contributing

This is a university coursework project. The pipeline is designed to be:
- **Educational**: Clear, well-documented code
- **Robust**: Production-quality error handling
- **Efficient**: Optimized for the target hardware
- **Reproducible**: Complete data lineage tracking

## ğŸ“„ License

University coursework project - not for commercial use.

---

**Status**: Ready for Implementation  
**Next Step**: Follow the Implementation Roadmap in `docs/implementation_roadmap.md`
