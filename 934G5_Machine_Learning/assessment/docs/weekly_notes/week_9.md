# Week 9 - Beyond Supervised Learning

Contents: 
- Learning outcomes
- Lecture slides structure
- Lecture Notes
- Lab Notes
- Reading Notes

# Learning Outcomes
- representation learning and transfer learning;
- variational autoencoders (VAEs) and generative adversarial networks (GANs), types of generative algorithms.

# Slides Structure
- Transfer learning
- Generative models

# Lecture Notes
## Transfer Learning
Deep learning is a NN with >3 layers

First set of layers act as feature extractors, learning low-level and general representations from the raw data

Second set of layers learn high-level information that is specific to the particular task

The final layer is the target task and is generally supervised learning

The previous layers can be seen as an intermediate task to learn low level representations and features

This is known as representation learning

Pretext task means an intermediate task (supervised/self-supervised) with the particular aim of automatically learning features from data

Representations are also called features, embeddings, encodings or latent variables
Representation learning can be based on supervised learning, for example an encoder NN into a classifier NN

Or RL learning can be based on self-supervised learning, i.e. autoencoders

The (self-)supervised learning task of an autoencoders is reconstruction of the input data

Regularized Autoencoders

Sparse autoencoders a regularization term is added to the reconstruction loss function to encourage sparsity

Denoising autoencoders the input data is transformed (using data augmentation methods) to force the model to learn meaningful features

Transfer learning involves reuse of a trained encoder in a target task

Transfer Learning: Pre-training

Pretraining is a representation learning task

Transfer, i.e. (re)use the pretrained encoder on the target task

Can be used as:
- An offline feature extractor 
- Encoder can be embedded in the target task
    - Pre-train weights may be frozen
    - Some or all weights may be finetuned to the target task

The data used for pre-training vs target tasks may differ

Additionally, the “task” for pre-training and target may be different too. It about transferring the relevant learnt information

This is very prevalent and common for image models. A model can learn to see important aspects regardless of the task, i.e. dogs have similar features to cats

VGG-16 was trained as a encoder-decoder and the encoder is often used for its transfer learning and fine-tuned

Summary:
- The state of the art in feature extraction is representation learning, which involves deep neural networks. 
- A special application of representation learning is transfer learning, involving (re)use of pretrained weights for extracting features.

## Generative models
### Variational Autoencoders
Recall, an autoencoder is representation learning based on self-supervised learning

Standard autoencoders have latent variables (representations) which are deterministic

That is to say, a given input will always output the same thing

Variational Autoencoders are probabilistic 

They represent a probability distribution 

This means an unseen output can be generated by sampling from the probability distribution

### Generative Adversarial Network (GAN)

Skipped as won’t be using GANs in the assessment

# Lab Notes

To explore transfer learning from Week 9 lecture and apply it in pistachio image classification

The dataset used in this notebook is the Pistachio Image dataset from Ozkan IA., Koklu M. and Saracogli R. (2021). Classification of Pistachio Species Using Improved K-NN Classifier. Progress in Nutrition, Vol. 23, N. 2.

The dataload comes as a folder with 3 subfolders

Pistachio_Image_Dataset folder has two subfolders named Siirt_Pistachio and Kirmizi_Pistachio

Folder name represents the labels and the files within are jpg images

Pistachio_16_Features_Dataset contains 16 handcrafted features represented as a feature vector and labels (An array of length 16 + label)

Pistachio_28_Features_Dataset has 28 handcrafted features which is assumed to include the 16 above

Data is downloaded as a zip and it is the zipped file that is read through the notebook

Once the folder is unzipped we are loading in the 28 feature dataset

The dataset, which is held as rows of vectors, is split into features and labels

Labels are just the corresponding final entry of each vector

Labels are recoded at 0 or 1

Next the images from Pistachio_Image_Dataset are read in

In this setup, lots of information (labels, instance id) is held in the folder and files names so this needs to be extracted

The goal is to match the images with the handcrafted vectors to which they are derived

The order of images is matched with the order of vectors in each original dataset

Data is once again split in to test, train, validate

A plot is set up to visualize the class frequencies for each set

The handcrafted vector features are then scaled/normalized

Section 4 focuses on extracting new, learned features. This is the transfer learning component

A pre-trained VGG-16 model is used which is a 16 Layer CNN

The mode architecture is loaded in initialized with a particular available set of weights

A transforms function is used which does some pre-processing to ready the data for the model

Rescaling, image resize and image center cropping

The pertained model has some layers which are called the ‘feature extraction’ layers

These layers specifically refer to the Convolution and MaxPooling layers

Or in other words, it excludes the fully connected layer

The features learned in this section are highly generalizable and are what represent the transfer learning component

The learned features are then passed onto the fully connected layer to execute the task at hand. This part is less transferable

A very simple looping function is set up to pass the images into the layers.

A class is then set up to handle the MLP classification. This does not include anything specific to the CNN code. It is just MLP stuff from previous weeks

Feedforward, evaluation metrics, SGD optimization, training, epochs,

The 3 hyperparams set up are epochs, learning rate and batch size

A class called AutoFeatsDataset is set up. This holds the pre-trained model w/ weights, takes in an image and outputs a learned featured vector of 512 params.

Note, it does not do any training. The model is already trained, we are using transfer learning to create a condensed, high-level representation of the input image, capturing the most important visual features that VGG-16 learned during its training on ImageNet.

This vector is the passed into the MLP, along with the label, and training takes places on the MLP so it can execute the task

The MLP class has a train_model function which has 5 inputs: - The init version of the model to be trained - Hyperparams: (Learning rate and Epochs) - Training set auto features (CNN outputs) - Validation set auto features (CNN) outputs)

The batch size hyperparam is used in the auto feature stage

Remember the validation set is used for evaluation

Training takes place on train set, validation evaluates the output, this metric is used to decide the best model

Section 6 works with the hand-crafted features data

Given these are already a vector, there is no CNN needed and they plug straight into the MLP

Section 7 fine-tuned a pre-trained VGG. Previously we just used the ‘offline’ weights as they were

The pre-trained VGG and the MLP will be connected as one model and trained today, in turn updating the VGG weights

In the feedforward code, self.pretrained = models.vgg16(weights="IMAGENET1K_FEATURES").features is added as a layer

# Reading Notes

Start with:

I Goodfellow, Y Bengio, A Courville. Deep Learning. 2016. [Sections 14, 15, 20.10.3, 20.10.4]

C Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908. 2016.

A Zhang, ZC Lipton, M Li, AJ Smola. Dive into Deep Learning. 2021. [Chapter 20]

You might also find useful
DP Kingma, M Welling. Auto-encoding variational Bayes. arXiv:1312.6114. 2013. 

I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio. 

Generative adversarial nets. Advances in neural information processing systems 27. 2014.

M Mirza, S Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784. 2014. 

J Zhu, T Park, P Isola, AA Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision. 2017. 

M Arjovsky, S Chintala, L Bottou. Wasserstein generative adversarial networks. In International conference on machine learning. 2017. 

## I Goodfellow, Y Bengio, A Courville. Deep Learning. 2016. 

### Autoencoders

An autoencoder is an NN that is trained to attempt to copy its input to its output

Internally, a hidden layer h describes a code/variable used to represent the input (latent)

Autoencoder has two parts: encoders and decoder

The ability to merely copy is not useful, so autoencoders are designed to be unable to learn copy perfectly

Trained to resemble training data

** Under Complete Autoencoder **

An undercomplete autoecoder is forced to make the hidden variable small than the input and then regenerate its size

This size reduction force the encoder to learn salient features

Too much capacity in an undercomplete autoencoder will cause it to learn to copy the training data

The architecture of an undercomplete autoencoder is the use of a dimension reducing bottleneck. But this too is its downfall as not all information can be captured through the bottle neck

Thus additional tools are required to make autoencoding useful 

An overcomplete autoencoder occurs when the dimensions of the hidden variable are bigger than the input 

Ideally you would train an autoencoder by choosing code dimensions and capacity based on the complexity of the distribution to be modelled

**Regularized Autoencoders**

Regularized autoencoders provide this ability

Rather than limiting the capacity of the the EN/DE by keeping it shallow, regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy

The loss function includes a regularizer

Regularized autoencoders will learning something about the data even if overcomplete

The regularizer allows the model to be deeper and learn more complex relationships about the data without just copying

A bottleneck is a defining feature of an undercomplete autoencoder, but it's not the only way to achieve a good representation. It's not a matter of "best practice" to avoid a bottleneck; rather, modern autoencoder architectures have evolved to use more sophisticated regularization techniques.

Regularized autoencoders, such as sparse, denoising, or contractive autoencoders, address this by allowing the hidden layer to be the same size as or even larger than the input layer (an "overcomplete" autoencoder). These models use a regularization term in the loss function to constrain the model, preventing it from simply learning the identity function without a bottleneck.

When to Use a Bottleneck vs. Regularization
- Undercomplete (Bottleneck): Use this when you are specifically interested in dimensionality reduction. It's a simple and effective way to ensure a compressed representation.
Regularized (No Bottleneck): Use this when you want to learn a more robust, less sensitive, or more interpretable representation. These methods can often lead to better performance on downstream tasks because they allow the model to learn a richer set of features. For example, a denoising autoencoder is great for learning representations that are robust to noise, which can be useful in many real-world applications.
- An overcomplete autoencoder can learn a context vector which is larger than the input dimensions. This can allow it to present information and relationships which are more complex than the original dimensions allow for

**Sparse Autoencoders**

This is an autoencoder that includes a sparsity penalty on the code/hidden layer

Spare encoders learn features to then be applied to another task

Must respond to unique statistical features rather than becoming an identity function (repeater)

Regularized Autoencoders are not bayesian as the regularization depends on the data

A sparsity penalty is not to be thought of as regularizer for the copying task but instead think of the entire sparse autoencoder as approximating the max likelihood of training a generative model that has latent variables 

**Representational Power, Layer SIze Depth**

Autoencoders are generally single encoder and single decoder. But that is not a requirement and deep encoders and decodes may have many advantages

Autoencoders are feedforward. As are decoders and decoders. Hence all benefit from depth

Universal Approx Theorem guarantees that a FF network with at least one hidden layer can represent an approx of any function to an arbitrary degree of accuracy provided enough hidden units

Arbitrary here means any, i.e. there exists a number of hidden units to achieve the selected accuracy

If an encoder is too shallow it cannot achieve the arbitrary constraints but a deep encoder can

A deep network can reduce computation costs as:
- Requires less weights and biases
- Layers build up features
- A shallow network needs many units to achieve this hence increases the wights and bias
- Layers can be seen a modular and reusable 

**Stochastic Encoders and Decoders**

Autoencoders are just feedforward networks

In traditional FF, x is the input and y is the output to measure against the class label

In an autoencoder, x is the input but also the class label

X > h >  r

P_encoder(h|x)

P_decoder(x|h)

Both encoder and decoder are not simple functions and involve some noise injections

Thus output can be seen as being samples from a distribution 

Any latent variable model defines and stochastic encode/decoder

**Denoising Autoencoders**

A DAE receives a corrupted data point as its input and is trained to predict the original, uncorrected data point

This is a very specific type of model and can learn a good internal representation 

Goal is to create a high capacity encoder that avoids the identify function

**Applications of Autoencoders**

Autoencoders have been successfully applied to dimension reduction and information retrieval tasks

Dimension reduction was one of the early goals of autoencoders 

Lower dimensions improve on performance using less memory and runtime

These methods place semantically related examples near each other in the dimension space

The hints provided by the mapping to lower dimensions aid generalization

### Representation Learning

A good representation makes subsequent learning tasks easier

Supervised feed forwards are a type of representation learning that are passed into the last layer to conduct a task

The entire start/middle of the network learns a representation to pass to the task layer

Each hidden layer learns its down representation

In principle, the last layer does not need to be a linear classifier model. Could be a nearest neighbour model. 

The useful of final layer does impact the previous/penultimate layer as the backdrop will dictation different learning

Unsupervised algos learn representations as a side effect which we can pick up and use for other tasks

Representation Learning provides a way to perform unsupervised and semi-suprvised learning

Although representation learning also be provided through supervised learning

Often we have very large amounts of unlabeled data and less labelled data 

Training on just this small amount of labelled data tends to learn to overfitting

Semi-supervised offers a way to utilize the large amounts of unlabelled data and create strong representations

These representations can then be used to solve an end task through supervised

**Greedy Layer-Wise Unsupervised Pretraining**

Greedy learning proc could be used to find a good initialization point of weights and bias

Instead of init randomly

Learned representation can be transferred and learned from

It’s called greedy because it optimises each piece of the solution independently

Instead of jointly optimizing through backprop

Layer wise = independently pieces are the layers

Trains Rth layer whilst keeping previous fixed

Lower layers are frozen, have no knowledge of later layers and are not re-trained once introduced

Unsupervised because layer is trained w unsupervised representation learning

Called pre-training because it is only supposed to be the first 

Later, a joint training algo is applied to fine-tune all layers together

Pre-training in supervised learning acts as a regularizer by decreasing the test error without necessarily decreasing the training error because it helps the model generalize better. The pre-training phase helps the model learn robust, general features from a large, often more diverse, dataset.

Pre-training also acts a form of parameter initialization

Fine-tuning can be simple or complex

Simple = pretrain models and appended to a classifier and the classifer is trained

Complex = pertain models appended to a classifier and whole network retrained

**When and Why Does Pretraining Work?**

Greedy layer-wise unsupervised pretraining often leads to substantial improvement in test error in classification 

But it can sometimes be harmful so important to understand when and why it works

Unsupervised pre-training combines two concepts:
- Initialization to improve regularization
- Learning input distributions helps learn mapping from input to output

**Init**

Initialization of models is not a very well understood topic

Originally people thought init points may allow certain mimumas to be accessed

Today we know that NN don’t arrive at a singular critical point 

It is possible that certain inits allow the model to access areas that otherwise would be impossible, i.e. surrounded by areas where the gradients are too messy for the cost function to arrive at

**Sharing Learned Information**

The concept of an algo picking up info learned in an unsupervised setting is better understood

E.g. a model for cars has some representation about wheels

**Pretraining as representation**

Pertaining is more effective when the initial representation is poor

I.e. words need to be represented as numbers for models to understand. One hot encoding is not good because any word is the same distance from each other

Learned work embeddings encode similarity between words, as well as, putting them in numerical form

It is very useful for words

**Regularizer**

Pre-training is most useful as a regularizer when the labeled examples are very small
Other Factors 

Unsupervised pertaining is most likely to be useful when the function to be leaned is extremely complicated 

Regularizers such as weight decay may bias towards learning a simple function

If the underlying functions are complicated then unsupervised pretraining may be an appropriate regularizer 

Unsupervised pretraining is most known to improve classification and reduce test set error

Erhanetal (2010) suggests pretraining takes parameters into a region that would otherwise be inaccessible

Recall that NNs are non-deterministic and converse to a different function every time it is run

Networks will halt anywhere where the gradient becomes too small. This is to prevent overfitting

Networks that receive unsupervised pre-training consistently halt in same region 

The region where pretrained networks arrive at tend to be smaller

This suggests that it reduces the variance of the estimation process. This in turn reduces the risk of overfitting

Pre-training initializes the parameters into a region that they don’t escape from 

The results are more consistent and less likely to be very bad

Today, pre-training has largely been abandoned in the field of NLP

But it is often done for extremely large and unlabeled datasets

**Transfer Learning and Domain Application**

Learned in one setting P_1 and applied to another P_2

This is generally operated as unsupervised into a supervised task

The assumption behind transfer learning is that factors captured in P_1 also explain variations in P_2

Transfer learning is best achieved via representation when there exists features that are useful for different settings or tasks corresponding to underlying factors that appear in more than one setting

**Semi-Supervised Disentangling of Causal Factors**

What makes representation good?

One hypothesis:
Features learned in the representation correspond to the underlying causes of the observed data
With distinct features learned corresponding to different causes

That is to say, the representation disentables the causes

**Providing Clue to discover underlying causes**

Coming back round to, what makes a representation good?

An ideal representational is one that disentangles the underlying causal factors of variation that generates the data

Most strategies for deep learning are based on introducing clues that help find these underlying factors of variation 

Clues help learners separate observed factors from others

The strongest clues available is labels 

But unabled data is more common and vast so representation learning makes use of more subtle clues about the underlying factors

These hints take the form of implicit prior beliefs that we the algo designers impose to guide the learned

**How is regularization a prior and clue?**

L1 regularization implicitly expresses a prior belief that many features are irrelevant

L2 is a prior that functions are smooth and large weights could be sensitivities 
Dropout is a belief that model should not reply a single feature or neuron too heavily (robust)

Sometimes architectures have implicit beliefs 

CNNs = translation invariables, feature detector regardless of location and locality that pixels close together are related

