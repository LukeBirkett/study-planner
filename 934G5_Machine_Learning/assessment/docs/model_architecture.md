# Intro
The project as hand is a time series, classification task. In my pipeline I want to utilise 3 different times of networks. A fully connected, MLP layer for the output task - which will also likely integrate the use of Mixture of Experts. RNNs, in the form of LSTMs or GRUs, this due to the time-series and sequential nature of weather data. CNN in the form of 1D convolutions in order to exraction spatial features from the various parameters and to reduce dimenions of the input data. In addition to these direct architecture decisions there will be heavy use of conceptual decision such as transfer learning, pretraining, unsupervised training, self-supervised training and autoencoders.

In the follow bullet points I will outline the chronology of the pipeline to be built:

# Step 1: Foundational Model
The initial phase of the pipeline will be a model trained that proxies the concept of a large, general, foundation model. In this class, alot of the learning content was around image data and popular foundational models such as VGG-16 were introduced. The task given for the assesment is not image based instead it is a time-series based. I am not confident in locating a popular, pre-trained model for this domain but I want to take advantage and demonstate that I understand the value in training a foundational model on large, general corpus of unlablled data in order to learn low-level representations for a wide task that can later be passed onto a more specific task. As mentioned before, the task is time series. For this foundational step, I would like to build up a large time-series dataset but with varying lengths, sampling from the raw output and applying augmentations to create a large courpus of data to train and encoder-decoder model. From the trained model we will extract the encoder portion to continue using later in the pipeline.

# Step 2: Pre-trained Specalized Representations
The second portion will be a step towards specalizing the general respresentations learned by the foundational model. An important distinction of this step is that the input data, whilst also unlabelled, will be in the same format as the final model. This step will explicitly utilized self-supervised learning as well as CNNs. The pipeline will start with a CNN, it will ingest data that is structured with the time series component on the column axis and the parameters on the depth axis from 1D convolutions will be applied. This will be in order to learn spatail relationships between the parameters/days and reduce the time-series dimensions. The output of the CNN will into an RNN Regularized, Overcomplete Encoder where the RNN will be initalized as the pretained RNN from step 1 and be picked up by a decoder to try and recreate the input. Once trained the decoder layers will be dropped and the Convo-RNN layers carried over to the final pipeline. 

# Step 3: Final Classification Layer
This the final and main form of the model being built. It will extract and build upon the respresentations learnt in the previous pre-training steps. A fully connected layer will be appended on to the pre-trained Convo-RNN. The fully connected layer will result in n-number of shallow MLPs to execute a mixture of experts structure. This should be an important step because our dataset includes locations throughout the globe. It is the hope that different experts will specalised in different weather extremes to form a better prediction overall. 