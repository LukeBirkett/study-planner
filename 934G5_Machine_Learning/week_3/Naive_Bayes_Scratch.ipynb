{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b667655c-c13f-4fd1-9a02-dc537903f30d",
   "metadata": {},
   "source": [
    " # Naive Bayes Classifier \n",
    "\n",
    "Bayes classifiers aim to determine the most probable class for a data point based on its features, using Bayes' theorem. They calculate the probability of each class given the observed features, considering prior knowledge and the likelihood of those features. The class with the highest probability is then assigned as the prediction.  This approach allows for probabilistic predictions, incorporating uncertainty and prior beliefs into the classification process.\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $\n",
    "\n",
    "Gaussian Naive Bayes assumes continuous features within each class follow a normal distribution, using the provided PDF to calculate likelihoods. It estimates the mean and variance for each feature per class from the training data. Then, it applies the Naive Bayes assumption of feature independence to compute the probability of a class given the features. Finally, it often uses log probabilities for numerical stability.\n",
    "\n",
    "$ P(X | C_k) = P(x_1 | C_k) \\cdot P(x_2 | C_k) \\cdot ... \\cdot P(x_n | C_k) = \\prod_{i=1}^{n} P(x_i | C_k) $\n",
    "\n",
    "Here X is the instances vector, $C_k$ is the class and $x_n$ is a feature within the data instance\n",
    "\n",
    "To prevent numerical underflow, especially when dealing with many small probabilities, we convert the Naive Bayes equation into logarithms and sums. Multiplying numerous tiny probabilities can result in values too small for computers to represent accurately. By taking the logarithm, products become sums, which are computationally more stable and efficient. This conversion maintains the relative ordering of probabilities, ensuring that the class with the highest probability remains the highest after the logarithmic transformation.\n",
    "\n",
    "$ \\log P(C_k | X) = \\log(P(C_k)) + \\sum_{i=1}^{n} \\log(P(x_i | C_k)) - \\log(P(X)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986fa77-88fb-443c-aa39-ab907b1091a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Algorithm Steps\n",
    "\n",
    "The goal is to select the class with the highest posterior probability: $ argmax P(C_k|X) $\n",
    "\n",
    "That is to say, as we have obversed a data point X, what is the most likely class it belongs to\n",
    "\n",
    "1. **Get rid of the demoninator of Bayes Theorem $ P(X) $**. <br>\n",
    "   This is the probability of observing X the data point we are focusing on. It is the same regardless of what class we are calculating, hence it is constant and its not needed to identify the highest number, i.e. the most likely class. Note, removing this term means we are calculating the proportional posterior, not the exact probabilty hence the output is only useful for classification itself, not probabilitic modelling. It makes calculations quicker and easier\n",
    "\n",
    "2. **Calculate Prior** <br>\n",
    "    Prior = $P(y)$ = Frequency of each class in the whole data <br>\n",
    "    shape[0] of class subset/ shapre[0] total data\n",
    "\n",
    "3. **Class Conditionals** <br>\n",
    "    Class Condition Probabilities for each feature of a data instance = $P(x_i|y)$ Modelled using Probability Density Function (PDF) <br>\n",
    "\n",
    "    ---\n",
    "\n",
    "   \n",
    "    **When features are continuous, we often assume they follow a normal (Gaussian) distribution within each class.** <br>\n",
    "    The probability density function (PDF) for a normal distribution is:<br>\n",
    "    $$P(x_i | C_k) = N(x | \\mu, \\sigma^2) = f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "   \n",
    "    To use the normal distribution, you need to estimate $μ$ and $σ_2$ for each feature and each class from the training data:\n",
    "\n",
    "    $$\\mu_{ik} = \\frac{1}{N_{ck}} \\sum_{x_i \\in C_k} x_i$$\n",
    "\n",
    "    $$\\sigma_{ik}^2 = \\frac{1}{N_{ck} - 1} \\sum_{x_i \\in C_k} (x_i - \\mu_{ik})^2$$\n",
    "\n",
    "    Plug the observed value of x_i into the normal distribution PDF using the estimated $μ_ik$ and $σ^2_ik$\n",
    "\n",
    "    Once each features conditional probability has been calculated you mutliply these all together:\n",
    "\n",
    "    $$P(X | C_k) = \\prod_{i=1}^{n} P(x_i | C_k) = P(x_1 | C_k) \\cdot P(x_2 | C_k) \\cdot ... \\cdot P(x_n | C_k)$$\n",
    "\n",
    "    ---\n",
    "    **For categorical features, you calculate the probability by counting the frequency of each category within each class.**\n",
    "\n",
    "   $$P(x_i = v | C_k) = \\frac{\\text{Count of feature } x_i = v \\text{ in class } C_k}{\\text{Count of data points in class } C_k}$$\n",
    "\n",
    "   Where $v$ is a specific category of feature $x_i$.\n",
    "\n",
    "\n",
    "   To avoid probabilities of zero when a category is not observed in a class, Laplace smoothing is often used:\n",
    "\n",
    "    $$P(x_i = v | C_k) = \\frac{\\text{Count of feature } x_i = v \\text{ in class } C_k + 1}{\\text{Count of data points in class } C_k + \\text{Number of categories in } x_i} $$\n",
    "\n",
    "   ---\n",
    "\n",
    "5. **Convert equation to logarthims and sums** <br>\n",
    "    $$\\log P(C_k | X) = \\log(P(C_k)) + \\sum_{i=1}^{n} \\log(P(x_i | C_k)) - \\log(P(X))$$\n",
    "\n",
    "\n",
    "6. **Execution Steps** <br>\n",
    "   Training: Calculate means, variances and priors <br>\n",
    "\n",
    "   Preduction: Calculate Posterior for each class <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8453a43-09e6-4436-bd28-d78aa4ed75aa",
   "metadata": {},
   "source": [
    "# Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6fb46e-4bb0-458a-9240-3f6569d3e9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of Y: (150,)\n",
      "Data type of X: float64\n",
      "Data type of Y: int64\n",
      "\n",
      "First 5 rows of X:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "First 5 values of Y:\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract the data (features) into X\n",
    "X = iris.data\n",
    "\n",
    "# Extract the labels (target) into Y\n",
    "Y = iris.target\n",
    "\n",
    "# Optional: Verify the shapes and data types\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "print(\"Data type of X:\", X.dtype)\n",
    "print(\"Data type of Y:\", Y.dtype)\n",
    "\n",
    "# Optional: Print the first few rows of X and Y\n",
    "print(\"\\nFirst 5 rows of X:\")\n",
    "print(X[:5])\n",
    "print(\"\\nFirst 5 values of Y:\")\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faabba6-fa88-4ea7-b693-d0b5d5693b8b",
   "metadata": {},
   "source": [
    "# Naive Bayes Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a401b3fc-2c80-44d2-b89e-44c556af6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        print(\"x\")\n",
    "        # identify the classes and features\n",
    "        print(data.shape)\n",
    "\n",
    "        n_instances, n_features = data.shape\n",
    "        self._classes = np.unique(labels)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        print(n_instances, n_features, self._classes, n_classes)\n",
    "\n",
    "        # set up zeros for mean, var and priors for each class rows classes, columns features\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._prior =np.zeros((n_classes, 1), dtype=np.float64)\n",
    "\n",
    "        for idx, class_label in enumerate(self._classes):\n",
    "            print(f\"Processing Class {idx}\")\n",
    "            class_subset = data[labels == class_label]\n",
    "\n",
    "            self._mean[idx, :] =  class_subset.mean(axis=0)\n",
    "            self._var[idx, :] =  class_subset.var(axis=0)\n",
    "            self._prior[idx, :] =  class_subset.shape[0] / data.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, test_set):\n",
    "        y_pred = [self._predict(x) for x in test_set]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def predict(self, instance):\n",
    "        print(instance)\n",
    "        # each \n",
    "         # for idx, class_label in enumerate(self._classes):\n",
    "             \n",
    "\n",
    "        # per class calculate the posterior of Ck gives our X point\n",
    "        # within a class and for each feature use the pdf to calculate the conditionals\n",
    "        # mutliply all the conditionals together\n",
    "        # multiply the conditonal of the class by the prior of the class \n",
    "        # store value\n",
    "        # repeat for each class and select highest\n",
    "        \n",
    "\n",
    "    def pdf(self, x, mean, var):\n",
    "        # x is the feature of a data instance\n",
    "        \n",
    "        numerator = (x - mean)^2\n",
    "        denominator = (2 * var)\n",
    "    \n",
    "        return 1 / sqrt(2 * pi * var) * exp^(-(numerator/denominator)) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098d1c6-2194-4a45-b93e-8b3086abc010",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33bc8b5e-7a8c-4145-8c2e-39fb820f7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "(150, 4)\n",
      "150 4 [0 1 2] 3\n",
      "Processing Class 0\n",
      "[[5.006 3.428 1.462 0.246]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]]\n",
      "[[0.121764 0.140816 0.029556 0.010884]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.       0.       0.       0.      ]]\n",
      "[[0.33333333]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "Processing Class 1\n",
      "[[5.006 3.428 1.462 0.246]\n",
      " [5.936 2.77  4.26  1.326]\n",
      " [0.    0.    0.    0.   ]]\n",
      "[[0.121764 0.140816 0.029556 0.010884]\n",
      " [0.261104 0.0965   0.2164   0.038324]\n",
      " [0.       0.       0.       0.      ]]\n",
      "[[0.33333333]\n",
      " [0.33333333]\n",
      " [0.        ]]\n",
      "Processing Class 2\n",
      "[[5.006 3.428 1.462 0.246]\n",
      " [5.936 2.77  4.26  1.326]\n",
      " [6.588 2.974 5.552 2.026]]\n",
      "[[0.121764 0.140816 0.029556 0.010884]\n",
      " [0.261104 0.0965   0.2164   0.038324]\n",
      " [0.396256 0.101924 0.298496 0.073924]]\n",
      "[[0.33333333]\n",
      " [0.33333333]\n",
      " [0.33333333]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NaiveBayes.predict() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m nb = NaiveBayes()\n\u001b[32m      3\u001b[39m nb.fit(X, Y)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: NaiveBayes.predict() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "\n",
    "nb.fit(X, Y)\n",
    "\n",
    "nb.predict()\n",
    "\n",
    "# Not finished\n",
    "# https://www.youtube.com/watch?v=TLInuAorxqE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w3env",
   "language": "python",
   "name": "w3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
